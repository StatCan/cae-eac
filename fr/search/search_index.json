{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Environnement d'analyse collaborative (EAC) L'Environnement d'Analyse Collaborative (EAC) fournit des services infonuagiques pour l'ingestion, la transformation, la pr\u00e9paration, l'exploration et le traitement des donn\u00e9es. Il comprend des outils d'analyse collaborative, des environnements d'apprentissage machine et des fonctions de visualisation des donn\u00e9es. Les environnements de bloc-notes et les machines virtuelles offrent des capacit\u00e9s d'analyse par l'entremise de divers logiciels statistiques comme R, Python et SAS. L'EAC exploite la plateforme Microsoft Azure comme service (PaaS) et les offres de logiciel-service (SaaS). Aper\u00e7u de l'environnement Nous mettons actuellement \u00e0 l'essai diff\u00e9rents cas d'utilisation de la plateforme. Chaque cas d'utilisation peut \u00eatre int\u00e9gr\u00e9 \u00e0 l'environnement principal . Sinon, il est possible de cr\u00e9er un nouvel environnement priv\u00e9 . Environnement principal (partag\u00e9) L'environnement est partag\u00e9 avec les utilisateurs de plusieurs cas d'utilisation. Lorsqu'ils ont acc\u00e8s \u00e0 cet environnement, les utilisateurs peuvent visualiser et partager les donn\u00e9es de ces cas d'utilisation. Environnement priv\u00e9 Un environnement priv\u00e9 configur\u00e9 sur demande qui permet l'acc\u00e8s aux fichiers de l'espace de travail seulement aux utilisateurs d\u00e9sign\u00e9s. Ingestion de donn\u00e9es Les donn\u00e9es int\u00e8grent la plateforme au moyen d'un compte de stockage externe. Une fois dans la plateforme, elles sont stock\u00e9es dans un compte de stockage interne (Data Lake). Les sources de donn\u00e9es accessibles au public peuvent \u00eatre ing\u00e9r\u00e9es directement \u00e0 l'aide de l'un des outils de la plateforme. Compte de stockage externe Les utilisateurs pourront acc\u00e9der au compte de stockage externe \u00e0 partir d'Internet et s'en servir pour t\u00e9l\u00e9verser et t\u00e9l\u00e9charger des donn\u00e9es \u00e0 partir ou vers l'environnement. Dans certains environnements priv\u00e9s, des restrictions ou des processus de contr\u00f4le suppl\u00e9mentaires peuvent \u00eatre mis en \u0153uvre pour le t\u00e9l\u00e9versement ou le t\u00e9l\u00e9chargement de donn\u00e9es. Compte de stockage interne (Data Lake) Les fichiers qui sont t\u00e9l\u00e9vers\u00e9s dans le compte de stockage externe sont automatiquement d\u00e9plac\u00e9s vers un compte de stockage interne. Ce compte de stockage interne se trouve dans un r\u00e9seau virtuel s\u00e9curis\u00e9 et est uniquement accessible \u00e0 partir des services des machines virtuelles de la plateforme.","title":"En avant!"},{"location":"#environnement-danalyse-collaborative-eac","text":"L'Environnement d'Analyse Collaborative (EAC) fournit des services infonuagiques pour l'ingestion, la transformation, la pr\u00e9paration, l'exploration et le traitement des donn\u00e9es. Il comprend des outils d'analyse collaborative, des environnements d'apprentissage machine et des fonctions de visualisation des donn\u00e9es. Les environnements de bloc-notes et les machines virtuelles offrent des capacit\u00e9s d'analyse par l'entremise de divers logiciels statistiques comme R, Python et SAS. L'EAC exploite la plateforme Microsoft Azure comme service (PaaS) et les offres de logiciel-service (SaaS).","title":"Environnement d'analyse collaborative (EAC)"},{"location":"#apercu-de-lenvironnement","text":"Nous mettons actuellement \u00e0 l'essai diff\u00e9rents cas d'utilisation de la plateforme. Chaque cas d'utilisation peut \u00eatre int\u00e9gr\u00e9 \u00e0 l'environnement principal . Sinon, il est possible de cr\u00e9er un nouvel environnement priv\u00e9 .","title":"Aper\u00e7u de l'environnement"},{"location":"#environnement-principal-partage","text":"L'environnement est partag\u00e9 avec les utilisateurs de plusieurs cas d'utilisation. Lorsqu'ils ont acc\u00e8s \u00e0 cet environnement, les utilisateurs peuvent visualiser et partager les donn\u00e9es de ces cas d'utilisation.","title":"Environnement principal (partag\u00e9)"},{"location":"#environnement-prive","text":"Un environnement priv\u00e9 configur\u00e9 sur demande qui permet l'acc\u00e8s aux fichiers de l'espace de travail seulement aux utilisateurs d\u00e9sign\u00e9s.","title":"Environnement priv\u00e9"},{"location":"#ingestion-de-donnees","text":"Les donn\u00e9es int\u00e8grent la plateforme au moyen d'un compte de stockage externe. Une fois dans la plateforme, elles sont stock\u00e9es dans un compte de stockage interne (Data Lake). Les sources de donn\u00e9es accessibles au public peuvent \u00eatre ing\u00e9r\u00e9es directement \u00e0 l'aide de l'un des outils de la plateforme.","title":"Ingestion de donn\u00e9es"},{"location":"#compte-de-stockage-externe","text":"Les utilisateurs pourront acc\u00e9der au compte de stockage externe \u00e0 partir d'Internet et s'en servir pour t\u00e9l\u00e9verser et t\u00e9l\u00e9charger des donn\u00e9es \u00e0 partir ou vers l'environnement. Dans certains environnements priv\u00e9s, des restrictions ou des processus de contr\u00f4le suppl\u00e9mentaires peuvent \u00eatre mis en \u0153uvre pour le t\u00e9l\u00e9versement ou le t\u00e9l\u00e9chargement de donn\u00e9es.","title":"Compte de stockage externe"},{"location":"#compte-de-stockage-interne-data-lake","text":"Les fichiers qui sont t\u00e9l\u00e9vers\u00e9s dans le compte de stockage externe sont automatiquement d\u00e9plac\u00e9s vers un compte de stockage interne. Ce compte de stockage interne se trouve dans un r\u00e9seau virtuel s\u00e9curis\u00e9 et est uniquement accessible \u00e0 partir des services des machines virtuelles de la plateforme.","title":"Compte de stockage interne (Data\u00a0Lake)"},{"location":"AzureML/","text":"Nouveau: Veuillez acc\u00e9der \u00e0 Azure ML \u00e0 partir de votre machine virtuelle de l'EAC. Acc\u00e8s \u00e0 Azure Machine Learning Tableau de bord Consultez la section Tableau de bord de cette documentation pour obtenir de plus amples renseignements.. Cliquez sur le menu Tableau de bord du portail Azure. L\u2019affichage par d\u00e9faut pourrait d\u00e9j\u00e0 correspondre au tableau de bord. Sous Machine Learning (apprentissage automatique), s\u00e9lectionnez l\u2019espace de travail Machine Learning qui a \u00e9t\u00e9 cr\u00e9\u00e9 pour vous. Si l\u2019espace de travail que vous souhaitez ouvrir n\u2019est pas r\u00e9pertori\u00e9, cliquez sur Plus\u2026 pour acc\u00e9der \u00e0 la liste compl\u00e8te. Portail Azure Dans la bo\u00eete de recherche du portail Azure, cherchez Machine Learning . Vous devriez voir une liste des espaces de travail Machine Learning auxquels vous avez \u00e9t\u00e9 autoris\u00e9 \u00e0 acc\u00e9der. S\u00e9lectionnez l\u2019 espace de travail Machine Learning auquel vous souhaitez acc\u00e9der Adresse de Machine Learning Allez sur Machine Learning , connectez-vous avec les justificatifs d\u2019identit\u00e9 de votre compte d\u2019infonuagique, puis s\u00e9lectionnez l\u2019abonnement vdl et l\u2019 espace de travail Machine Learning qui a \u00e9t\u00e9 cr\u00e9\u00e9 pour vous. Pour commencer \u00c0 la page Vue d\u2019ensemble de l\u2019apprentissage automatique, cliquez sur Lancer le studio . Utilisez le menu d\u00e9roulant pour s\u00e9lectionner l\u2019abonnement vdl et l\u2019 espace de travail Machine Learning auquel vous souhaitez acc\u00e9der, puis cliquez sur D\u00e9marrer. Une fois dans votre espace de travail Machine Learning, vous pouvez former, d\u00e9ployer et g\u00e9rer des mod\u00e8les d\u2019apprentissage automatique, utiliser AutoML et utiliser des pipelines. Voir le Guide de d\u00e9marrage rapide pour obtenir de plus amples renseignements. Utilisation de la fonction ind\u00e9pendante notebook d\u2019Azure Machine Learning Exigences Une instance de calcul dans Azure Machine Learning est n\u00e9cessaire. Vous devriez la voir sous Calculer --> Instances de calcul . Note : Si une instance de calcul n\u2019a pas \u00e9t\u00e9 cr\u00e9\u00e9e pour vous, veuillez communiquer avec l\u2019\u00e9quipe de soutien au moyen de Slack . \u00c9tapes Sous Notebooks , cr\u00e9ez un nouveau notebook dans votre r\u00e9pertoire d\u2019utilisateur. Vous pouvez ensuite saisir le code \u00e0 ex\u00e9cuter. S\u00e9lectionnez l\u2019 instance de calcul qui vous est attribu\u00e9e. Cliquez sur le bouton D\u00e9marrer le calcul et ex\u00e9cuter toutes les cellules pour ex\u00e9cuter votre code. Utilisation de Databricks Connect comme ordinateur \u00e0 distance Avertissement : Veuillez noter que la configuration de Databricks Connect pr\u00e9sent\u00e9e ci-dessous est en cours de r\u00e9vision et qu\u2019elle changera probablement dans un avenir proche. Exigences Une instance de calcul dans Azure Machine Learning est n\u00e9cessaire. Vous devriez la voir sous Calculer --> Instances de calcul . Note : Si une instance de calcul n\u2019a pas \u00e9t\u00e9 cr\u00e9\u00e9e pour vous, veuillez communiquer avec l\u2019\u00e9quipe de soutien au moyen de Slack . \u00c9tapes Sous Notebooks , ouvrez Terminal . S\u00e9lectionnez votre instance de calcul dans le menu d\u00e9roulant \u00e0 c\u00f4t\u00e9 de Calculer . Ex\u00e9cutez le code \u00e0 partir de la page Databricks Connect Setup dans le terminal tout en suivant les directives pour continuer selon les besoins. Ce code installe Python 3.7 et met en place un nouveau noyau pour les notebooks d\u2019Azure Machine Learning. Lorsque vous y \u00eates invit\u00e9, entrez les valeurs suivantes pour configurer Databricks Connect : H\u00f4te : l\u2019URL de la page Vue d\u2019ensemble pour votre espace de travail Databricks Jeton : le jeton d\u2019acc\u00e8s personnel g\u00e9n\u00e9r\u00e9 dans les param\u00e8tres utilisateur de votre espace de travail Databricks ID de grappe : la valeur indiqu\u00e9e sous Instance de calcul --> Options avanc\u00e9es--> \u00c9tiquettes dans votre espace de travail Databricks ID de l\u2019organisation : la partie de l\u2019URL de Databricks qui se trouve apr\u00e8s .net/?o= Port : conserver la valeur existante Ex\u00e9cutez le code suivant dans le terminal pour tester la connexion avec Azure Databricks. databricks-connect test (databricks - test de connexion) Cr\u00e9ez un nouveau notebook avec l\u2019espace de travail Azure Machine Learning et s\u00e9lectionnez noyau Python 3. La version Python 3.7.9 devrait maintenant s\u2019afficher. Databricks Connect devrait maintenant \u00eatre configur\u00e9! Essayez le code \u00e9chantillon Databricks Connect dans le bloc-notes, en rempla\u00e7ant public-data/incoming/1test.txt par le chemin d\u2019acc\u00e8s \u00e0 un fichier dans votre conteneur de lac de donn\u00e9es. Demander un calcul Veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal slack pour demander un calcul dans Azure Machine Learning. Vous recevrez l'erreur suivante si vous proc\u00e9dez \u00e0 la cr\u00e9ation par vous-m\u00eame:","title":"Azure Machine Learning"},{"location":"AzureML/#acces-a-azure-machine-learning","text":"","title":"Acc\u00e8s \u00e0 Azure Machine Learning"},{"location":"AzureML/#tableau-de-bord","text":"Consultez la section Tableau de bord de cette documentation pour obtenir de plus amples renseignements.. Cliquez sur le menu Tableau de bord du portail Azure. L\u2019affichage par d\u00e9faut pourrait d\u00e9j\u00e0 correspondre au tableau de bord. Sous Machine Learning (apprentissage automatique), s\u00e9lectionnez l\u2019espace de travail Machine Learning qui a \u00e9t\u00e9 cr\u00e9\u00e9 pour vous. Si l\u2019espace de travail que vous souhaitez ouvrir n\u2019est pas r\u00e9pertori\u00e9, cliquez sur Plus\u2026 pour acc\u00e9der \u00e0 la liste compl\u00e8te.","title":"Tableau de bord"},{"location":"AzureML/#portail-azure","text":"Dans la bo\u00eete de recherche du portail Azure, cherchez Machine Learning . Vous devriez voir une liste des espaces de travail Machine Learning auxquels vous avez \u00e9t\u00e9 autoris\u00e9 \u00e0 acc\u00e9der. S\u00e9lectionnez l\u2019 espace de travail Machine Learning auquel vous souhaitez acc\u00e9der","title":"Portail Azure"},{"location":"AzureML/#adresse-de-machine-learning","text":"Allez sur Machine Learning , connectez-vous avec les justificatifs d\u2019identit\u00e9 de votre compte d\u2019infonuagique, puis s\u00e9lectionnez l\u2019abonnement vdl et l\u2019 espace de travail Machine Learning qui a \u00e9t\u00e9 cr\u00e9\u00e9 pour vous.","title":"Adresse de Machine Learning"},{"location":"AzureML/#pour-commencer","text":"\u00c0 la page Vue d\u2019ensemble de l\u2019apprentissage automatique, cliquez sur Lancer le studio . Utilisez le menu d\u00e9roulant pour s\u00e9lectionner l\u2019abonnement vdl et l\u2019 espace de travail Machine Learning auquel vous souhaitez acc\u00e9der, puis cliquez sur D\u00e9marrer. Une fois dans votre espace de travail Machine Learning, vous pouvez former, d\u00e9ployer et g\u00e9rer des mod\u00e8les d\u2019apprentissage automatique, utiliser AutoML et utiliser des pipelines. Voir le Guide de d\u00e9marrage rapide pour obtenir de plus amples renseignements.","title":"Pour commencer"},{"location":"AzureML/#utilisation-de-la-fonction-independante-notebook-dazure-machine-learning","text":"","title":"Utilisation de la fonction ind\u00e9pendante notebook d\u2019Azure Machine Learning"},{"location":"AzureML/#exigences","text":"Une instance de calcul dans Azure Machine Learning est n\u00e9cessaire. Vous devriez la voir sous Calculer --> Instances de calcul . Note : Si une instance de calcul n\u2019a pas \u00e9t\u00e9 cr\u00e9\u00e9e pour vous, veuillez communiquer avec l\u2019\u00e9quipe de soutien au moyen de Slack .","title":"Exigences"},{"location":"AzureML/#etapes","text":"Sous Notebooks , cr\u00e9ez un nouveau notebook dans votre r\u00e9pertoire d\u2019utilisateur. Vous pouvez ensuite saisir le code \u00e0 ex\u00e9cuter. S\u00e9lectionnez l\u2019 instance de calcul qui vous est attribu\u00e9e. Cliquez sur le bouton D\u00e9marrer le calcul et ex\u00e9cuter toutes les cellules pour ex\u00e9cuter votre code.","title":"\u00c9tapes"},{"location":"AzureML/#utilisation-de-databricks-connect-comme-ordinateur-a-distance","text":"Avertissement : Veuillez noter que la configuration de Databricks Connect pr\u00e9sent\u00e9e ci-dessous est en cours de r\u00e9vision et qu\u2019elle changera probablement dans un avenir proche.","title":"Utilisation de Databricks Connect comme ordinateur \u00e0 distance"},{"location":"AzureML/#exigences_1","text":"Une instance de calcul dans Azure Machine Learning est n\u00e9cessaire. Vous devriez la voir sous Calculer --> Instances de calcul . Note : Si une instance de calcul n\u2019a pas \u00e9t\u00e9 cr\u00e9\u00e9e pour vous, veuillez communiquer avec l\u2019\u00e9quipe de soutien au moyen de Slack .","title":"Exigences"},{"location":"AzureML/#etapes_1","text":"Sous Notebooks , ouvrez Terminal . S\u00e9lectionnez votre instance de calcul dans le menu d\u00e9roulant \u00e0 c\u00f4t\u00e9 de Calculer . Ex\u00e9cutez le code \u00e0 partir de la page Databricks Connect Setup dans le terminal tout en suivant les directives pour continuer selon les besoins. Ce code installe Python 3.7 et met en place un nouveau noyau pour les notebooks d\u2019Azure Machine Learning. Lorsque vous y \u00eates invit\u00e9, entrez les valeurs suivantes pour configurer Databricks Connect : H\u00f4te : l\u2019URL de la page Vue d\u2019ensemble pour votre espace de travail Databricks Jeton : le jeton d\u2019acc\u00e8s personnel g\u00e9n\u00e9r\u00e9 dans les param\u00e8tres utilisateur de votre espace de travail Databricks ID de grappe : la valeur indiqu\u00e9e sous Instance de calcul --> Options avanc\u00e9es--> \u00c9tiquettes dans votre espace de travail Databricks ID de l\u2019organisation : la partie de l\u2019URL de Databricks qui se trouve apr\u00e8s .net/?o= Port : conserver la valeur existante Ex\u00e9cutez le code suivant dans le terminal pour tester la connexion avec Azure Databricks. databricks-connect test (databricks - test de connexion) Cr\u00e9ez un nouveau notebook avec l\u2019espace de travail Azure Machine Learning et s\u00e9lectionnez noyau Python 3. La version Python 3.7.9 devrait maintenant s\u2019afficher. Databricks Connect devrait maintenant \u00eatre configur\u00e9! Essayez le code \u00e9chantillon Databricks Connect dans le bloc-notes, en rempla\u00e7ant public-data/incoming/1test.txt par le chemin d\u2019acc\u00e8s \u00e0 un fichier dans votre conteneur de lac de donn\u00e9es.","title":"\u00c9tapes"},{"location":"AzureML/#demander-un-calcul","text":"Veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal slack pour demander un calcul dans Azure Machine Learning. Vous recevrez l'erreur suivante si vous proc\u00e9dez \u00e0 la cr\u00e9ation par vous-m\u00eame:","title":"Demander un calcul"},{"location":"AzureSQL/","text":"Une base de donn\u00e9es Azure SQL peut \u00eatre cr\u00e9\u00e9e \u00e0 l'avance si vous en avez besoin pour votre projet. Rappel : Les bases de donn\u00e9es Azure SQL de l'Environnement d'Analyse Collaborative (EAC) sont seulement disponibles \u00e0 partir de l'environnement infonuagique de l'EAC. Vous ne pouvez y acc\u00e9der \u00e0 partir des centres de donn\u00e9es du gouvernement du Canada. Acc\u00e8s \u00e0 une base de donn\u00e9es Azure SQL Azure Data Factory On peut cr\u00e9er un service li\u00e9 dans Azure Data Factory. Configurez le service li\u00e9 pour se connecter via le runtime d'int\u00e9gration auto-h\u00e9berg\u00e9 et utilisez Identit\u00e9 manag\u00e9e comme M\u00e9thode d'authentification . Veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack si vous avez besoin d'aide. Databricks On peut configurer un carnet Databricks pour ouvrir une connexion \u00e0 la base de donn\u00e9es. Puisque ceci exige une configuration additionnelle, veuillez faire une demande aupr\u00e8s de l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack . Machine virtuelle Vous pouvez vous connecter \u00e0 une base de donn\u00e9es Azure SQL \u00e0 partir de votre machine virtuelle en infonuagique, en utilisant une application telle que: - SQL Server Management Studio - Power BI Desktop - Azure Data Studio - Visual Studio ou Visual Studio Code Pr\u00e9requis Une machine virtuelle dans l'EAC. Voir la page Machines Virtuelle pour plus d'informations. SQL Server Management Studio ou un autre outil tel que Power BI Desktop . Ces outils sont offerts par d\u00e9faut dans les images de machine virtuelle Data Science Virtual Machine . \u00c9tapes Connectez-vous \u00e0 votre machine virtuelle dans l'EAC. Lancez un outil tel que SQL Server Management Studio . Choisissez Azure Active Directory - Auth.universelle avec MFA comme type d'Authentication. Entrez le nom de votre serveur SQL puis entrez votre compte Cloud comme nom d'utilisateur. Cliquez sur le bouton Options . Sous l'onglet Propri\u00e9t\u00e9s de Connexion , selectionnez le nom de la base de donn\u00e9es en face du libell\u00e9 Se connecter \u00e0 la base de donn\u00e9es , puis cliquez sur le bouton de Connexion . Effectuer l'authentication avec les param\u00e8tres d'identification de votre compte Cloud.","title":"Base de donn\u00e9es Azure SQL"},{"location":"AzureSQL/#acces-a-une-base-de-donnees-azure-sql","text":"","title":"Acc\u00e8s \u00e0 une base de donn\u00e9es Azure\u00a0SQL"},{"location":"AzureSQL/#azure-data-factory","text":"On peut cr\u00e9er un service li\u00e9 dans Azure Data Factory. Configurez le service li\u00e9 pour se connecter via le runtime d'int\u00e9gration auto-h\u00e9berg\u00e9 et utilisez Identit\u00e9 manag\u00e9e comme M\u00e9thode d'authentification . Veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack si vous avez besoin d'aide.","title":"Azure Data Factory"},{"location":"AzureSQL/#databricks","text":"On peut configurer un carnet Databricks pour ouvrir une connexion \u00e0 la base de donn\u00e9es. Puisque ceci exige une configuration additionnelle, veuillez faire une demande aupr\u00e8s de l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack .","title":"Databricks"},{"location":"AzureSQL/#machine-virtuelle","text":"Vous pouvez vous connecter \u00e0 une base de donn\u00e9es Azure SQL \u00e0 partir de votre machine virtuelle en infonuagique, en utilisant une application telle que: - SQL Server Management Studio - Power BI Desktop - Azure Data Studio - Visual Studio ou Visual Studio Code","title":"Machine virtuelle"},{"location":"AzureSQL/#prerequis","text":"Une machine virtuelle dans l'EAC. Voir la page Machines Virtuelle pour plus d'informations. SQL Server Management Studio ou un autre outil tel que Power BI Desktop . Ces outils sont offerts par d\u00e9faut dans les images de machine virtuelle Data Science Virtual Machine .","title":"Pr\u00e9requis"},{"location":"AzureSQL/#etapes","text":"Connectez-vous \u00e0 votre machine virtuelle dans l'EAC. Lancez un outil tel que SQL Server Management Studio . Choisissez Azure Active Directory - Auth.universelle avec MFA comme type d'Authentication. Entrez le nom de votre serveur SQL puis entrez votre compte Cloud comme nom d'utilisateur. Cliquez sur le bouton Options . Sous l'onglet Propri\u00e9t\u00e9s de Connexion , selectionnez le nom de la base de donn\u00e9es en face du libell\u00e9 Se connecter \u00e0 la base de donn\u00e9es , puis cliquez sur le bouton de Connexion . Effectuer l'authentication avec les param\u00e8tres d'identification de votre compte Cloud.","title":"\u00c9tapes"},{"location":"AzureStorage/","text":"Les donn\u00e9es peuvent \u00eatre t\u00e9l\u00e9vers\u00e9es sur la plateforme par l'entremise du portail Azure ou de l'application Explorateur de stockage Azure. Une fois qu'elles auront \u00e9t\u00e9 t\u00e9l\u00e9vers\u00e9es dans un compte de stockage externe Blob Azure , elles seront automatiquement ing\u00e9r\u00e9es dans un compte de stockage interne Azure Data Lake Storage (ADLS) . Lorsque les donn\u00e9es seront dans Data Lake, les utilisateurs pourront choisir des outils de transformation et d'int\u00e9gration. Ils pourront utiliser des outils Web, comme Databricks et Data Factory, pour effectuer des transformations ou des outils de bureau sur une machine virtuelle pour transformer et analyser les donn\u00e9es. Les donn\u00e9es nettoy\u00e9es et transform\u00e9es pourront \u00eatre plac\u00e9es dans diff\u00e9rents dossiers (contenant des ensembles de donn\u00e9es trait\u00e9es ou de meilleure qualit\u00e9) ou t\u00e9l\u00e9vers\u00e9es dans une base de donn\u00e9es. Les utilisateurs seront de nouveau en mesure de se connecter \u00e0 ces donn\u00e9es \u00e0 l'aide des outils qu'ils souhaiteront utiliser, et ce, \u00e0 partir de leur machine virtuelle ou d'autres services offerts sur la plateforme, tels que Databricks et Data Factory. Rappel : Les comptes de stockage internes ne sont accessibles qu'\u00e0 partir d'une machine virtuelle dans l'Environnement d'analyse collaboratif (EAC); voir la FAQ . Explorateur de stockage -- Portail Azure Acc\u00e9dez au compte de stockage (aper\u00e7u) Storage Account (Preview) \u00e0 partir du portail Azure. S\u00e9lectionnez votre type d'abonnement, puis naviguez dans votre compte de stockage. Explorateur de stockage -- Poste de travail personnel ou machine virtuelle sur nuage T\u00e9l\u00e9chargez l'application Explorateur de stockage Azure et installez-la sur votre poste de travail ou votre machine virtuelle. Lancez l'Explorateur de stockage Azure \u00e0 partir du menu D\u00e9marrer. Ouvrez une session avec votre compte Azure. Entrez vos justificatifs d'identit\u00e9 du nuage . Explorateur de stockage -- VDI du r\u00e9seau B Cette section s'adresse aux employ\u00e9s de Statistique Canada qui doivent t\u00e9l\u00e9verser des donn\u00e9es du r\u00e9seau B. T\u00e9l\u00e9chargez l'application Explorateur de stockage Azure et installez-la sur votre VDI du r\u00e9seau B. Lancez l'Explorateur de stockage Azure \u00e0 partir du menu D\u00e9marrer. Sur un VDI du r\u00e9seau B, la seule fa\u00e7on d'acc\u00e9der \u00e0 votre compte de stockage est d'utiliser un jeton SAP temporaire. Pour en faire la demande, veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack . Note : Pour obtenir des renseignements sur la configuration des param\u00e8tres du proxy du r\u00e9seau B, consultez la FAQ . Documentation Microsoft T\u00e9l\u00e9chargez l'Explorateur de stockage Azure D\u00e9marrage rapide : Charger, t\u00e9l\u00e9charger et lister les objets blob avec le portail Azure","title":"Stockage Azure"},{"location":"AzureStorage/#explorateur-de-stockage-portail-azure","text":"Acc\u00e9dez au compte de stockage (aper\u00e7u) Storage Account (Preview) \u00e0 partir du portail Azure. S\u00e9lectionnez votre type d'abonnement, puis naviguez dans votre compte de stockage.","title":"Explorateur de stockage\u00a0-- Portail\u00a0Azure"},{"location":"AzureStorage/#explorateur-de-stockage-poste-de-travail-personnel-ou-machine-virtuelle-sur-nuage","text":"T\u00e9l\u00e9chargez l'application Explorateur de stockage Azure et installez-la sur votre poste de travail ou votre machine virtuelle. Lancez l'Explorateur de stockage Azure \u00e0 partir du menu D\u00e9marrer. Ouvrez une session avec votre compte Azure. Entrez vos justificatifs d'identit\u00e9 du nuage .","title":"Explorateur de stockage\u00a0-- Poste de travail personnel ou machine virtuelle sur nuage"},{"location":"AzureStorage/#explorateur-de-stockage-vdi-du-reseau-b","text":"Cette section s'adresse aux employ\u00e9s de Statistique Canada qui doivent t\u00e9l\u00e9verser des donn\u00e9es du r\u00e9seau B. T\u00e9l\u00e9chargez l'application Explorateur de stockage Azure et installez-la sur votre VDI du r\u00e9seau B. Lancez l'Explorateur de stockage Azure \u00e0 partir du menu D\u00e9marrer. Sur un VDI du r\u00e9seau B, la seule fa\u00e7on d'acc\u00e9der \u00e0 votre compte de stockage est d'utiliser un jeton SAP temporaire. Pour en faire la demande, veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack . Note : Pour obtenir des renseignements sur la configuration des param\u00e8tres du proxy du r\u00e9seau B, consultez la FAQ .","title":"Explorateur de stockage\u00a0-- VDI du r\u00e9seau\u00a0B"},{"location":"AzureStorage/#documentation-microsoft","text":"T\u00e9l\u00e9chargez l'Explorateur de stockage Azure D\u00e9marrage rapide : Charger, t\u00e9l\u00e9charger et lister les objets blob avec le portail Azure","title":"Documentation Microsoft"},{"location":"AzureSynapse/","text":"Getting Started Access Azure Synapse Make sure you are in your cloud VM in order to access Azure Synapse. See Virtual Machines for information on how to create one if needed. Inside your virtual machine, open a web browser and navigate to the Azure Portal . Sign in with your cloud account credentials. a . Click on the ** Azure Synapse Analytics ** icon under ** Azure services ** . If you do not see this icon , follow step 3 b instead . b. Start typing \"synapse\" into the search bar to find Azure Synapse Analytics . Find your Synapse workspace in the list and click on it. Then click Open Synapse Studio . Note: You can also acccess Synapse workspaces from the Collaborative Analytics Environment dashboard. Start and Stop Dedicated SQL Pool Click on the Integrate tab. Under Pipelines , click on either Start Dedicated SQL Pool or Pause Dedicated SQL Pool . Then click the trigger button to open a menu, and select Trigger now . On the next screen, click OK . Home The Home tab is where you start when you first open Azure Synapse Studio. From here, you can access shortcuts for common tasks such as creating SQL scripts or notebooks by clicking the New dropdown menu button. Recently opened resources are also displayed. Data The Data tab is where you can explore everything in your database and linked datasets. Under the Workspace tab, you can explore the dedicated SQL pool database and any Spark databases. Under the Linked tab, you can explore external objects (e.g. Data Lake accounts) and explore/create any integration datasets from external linked data (e.g. Data Lake, Blob Storage, web service, etc) to be used in pipelines. How to Bring in Data from Linked Services (Note: This example shows how to get data from the Data Lake, although there are many source types available.) Click the plus button the add a new resource, then click Integration Dataset . Select Azure Data Lake Storage Gen2 (you may need to search for this), then click Continue . Select the format type, then click Continue . Enter a name, then click the drop-down menu under Linked service and select your data lake. Under Connect via integration runtime , ensure that interactive authoring is enabled. If it is not, click the edit button to enable it, then click Apply . Set additional properties as appropriate, then click OK . How to Explore Data in the Data Lake Browse to find your dataset file (CSV, Parquet, JSON, Avro, etc) and right click on it. A menu will open with options to preview the data, or create resources such as SQL scripts and notebooks. How to Explore the Dedicated SQL Pool Under the Workspace tab, you can explore databases similarly to SQL Server Management Studio (SSMS). Right click on any table, highlight New SQL script , and click Select TOP 100 rows to create a new query. You can then view the results as either a table or a chart. Importing Data to the Dedicated SQL Pool To import data to the dedicated SQL pool, you can either: - create a pipeline with a Copy data activity (most efficient for large datasets); or - use the Bulk Load Wizard . Develop From here, you can create and save resources such as SQL scripts, notebooks, and Power BI reports. To add a new resource, click the plus button. A drop-down menu will open. To make your changes visible to others, you need to click the Publish button. SQL Scripts Be sure to connect to your dedicated SQL pool in order to run SQL scripts. Notebooks In order to run notebook cells, you first need to select your Apache Spark pool. To change languages for a single cell, you can use magic commands: %%pyspark, %%spark, %%csharp, %%sql. You can also change the default language using the Language dropdown menu. Dataflows To add a source to a dataflow, under Source Settings , click the plus button, then select Azure Data Lake Storage Gen2 (you may need to search for this). Click Continue , select the data format, then on the next page, select your Linked Service. Power BI Reports You can view and create Power BI reports directly in Azure Synapse. Please contact the CAE support team to validate that a linked service is setup. Integrate This is where you can create pipelines for ingesting, preparing and transforming all of your data, like in Azure Data Factory . Example: Copy Data from External Blob to Data Lake Click the plus button to add a new resource, then click on Pipeline . Under Move & transform , drag and drop Copy data into the window. Click on the Source tab, then click New to add the source dataset (where you want to copy the data from). Select Azure Blob Storage , then select the format type (CSV, Parquet, JSON, etc). Set any additional properties if relevant, then click OK . Click on the Sink , then click New to set the sink dataset (where you want the data to be copied to). Choose Azure Data Lake Storage Gen2 , then select the format type. Under Linked service , choose your data lake and ensure that interactive authoring is enabled (see How to Bring in Data from Linked Services under Data for more info). Debugging and Running Pipelines To run a pipeline in debug mode, click the Debug button at the top of the pipeline window. Results will appear in the Output tab. To run a pipeline without debugging, click the Add trigger button, then Trigger now . When you are ready to publish your pipelines, click the Validate all button, then click the Publish all button. Note that this will publish for all users to see everything that you currently have open (pipelines, SQL scripts, notebooks, etc). Monitor From the Monitor tab, you can monitor live pipeline runs (inputs/outputs of each activity and any errors) and view historical pipeline runs, trigger runs, SQL requests, etc. Manage This is where you can: - Add new SQL or Apache Spark pools - Add new linked services - Grant others access to the workspace - Set up git integration Microsoft Documentation Azure Synapse Analytics What is Azure Synapse Analytics? Analyse Data with Dedicated SQL Pools Integrate with Pipelines Visualize Data with Power BI Monitor Your Synapse Workspace","title":"Azure Synapse"},{"location":"AzureSynapse/#getting-started","text":"","title":"Getting Started"},{"location":"AzureSynapse/#access-azure-synapse","text":"Make sure you are in your cloud VM in order to access Azure Synapse. See Virtual Machines for information on how to create one if needed. Inside your virtual machine, open a web browser and navigate to the Azure Portal . Sign in with your cloud account credentials. a . Click on the ** Azure Synapse Analytics ** icon under ** Azure services ** . If you do not see this icon , follow step 3 b instead . b. Start typing \"synapse\" into the search bar to find Azure Synapse Analytics . Find your Synapse workspace in the list and click on it. Then click Open Synapse Studio . Note: You can also acccess Synapse workspaces from the Collaborative Analytics Environment dashboard.","title":"Access Azure Synapse"},{"location":"AzureSynapse/#start-and-stop-dedicated-sql-pool","text":"Click on the Integrate tab. Under Pipelines , click on either Start Dedicated SQL Pool or Pause Dedicated SQL Pool . Then click the trigger button to open a menu, and select Trigger now . On the next screen, click OK .","title":"Start and Stop Dedicated SQL Pool"},{"location":"AzureSynapse/#home","text":"The Home tab is where you start when you first open Azure Synapse Studio. From here, you can access shortcuts for common tasks such as creating SQL scripts or notebooks by clicking the New dropdown menu button. Recently opened resources are also displayed.","title":"Home"},{"location":"AzureSynapse/#data","text":"The Data tab is where you can explore everything in your database and linked datasets. Under the Workspace tab, you can explore the dedicated SQL pool database and any Spark databases. Under the Linked tab, you can explore external objects (e.g. Data Lake accounts) and explore/create any integration datasets from external linked data (e.g. Data Lake, Blob Storage, web service, etc) to be used in pipelines.","title":"Data"},{"location":"AzureSynapse/#how-to-bring-in-data-from-linked-services","text":"(Note: This example shows how to get data from the Data Lake, although there are many source types available.) Click the plus button the add a new resource, then click Integration Dataset . Select Azure Data Lake Storage Gen2 (you may need to search for this), then click Continue . Select the format type, then click Continue . Enter a name, then click the drop-down menu under Linked service and select your data lake. Under Connect via integration runtime , ensure that interactive authoring is enabled. If it is not, click the edit button to enable it, then click Apply . Set additional properties as appropriate, then click OK .","title":"How to Bring in Data from Linked Services"},{"location":"AzureSynapse/#how-to-explore-data-in-the-data-lake","text":"Browse to find your dataset file (CSV, Parquet, JSON, Avro, etc) and right click on it. A menu will open with options to preview the data, or create resources such as SQL scripts and notebooks.","title":"How to Explore Data in the Data Lake"},{"location":"AzureSynapse/#how-to-explore-the-dedicated-sql-pool","text":"Under the Workspace tab, you can explore databases similarly to SQL Server Management Studio (SSMS). Right click on any table, highlight New SQL script , and click Select TOP 100 rows to create a new query. You can then view the results as either a table or a chart.","title":"How to Explore the Dedicated SQL Pool"},{"location":"AzureSynapse/#importing-data-to-the-dedicated-sql-pool","text":"To import data to the dedicated SQL pool, you can either: - create a pipeline with a Copy data activity (most efficient for large datasets); or - use the Bulk Load Wizard .","title":"Importing Data to the Dedicated SQL Pool"},{"location":"AzureSynapse/#develop","text":"From here, you can create and save resources such as SQL scripts, notebooks, and Power BI reports. To add a new resource, click the plus button. A drop-down menu will open. To make your changes visible to others, you need to click the Publish button.","title":"Develop"},{"location":"AzureSynapse/#sql-scripts","text":"Be sure to connect to your dedicated SQL pool in order to run SQL scripts.","title":"SQL Scripts"},{"location":"AzureSynapse/#notebooks","text":"In order to run notebook cells, you first need to select your Apache Spark pool. To change languages for a single cell, you can use magic commands: %%pyspark, %%spark, %%csharp, %%sql. You can also change the default language using the Language dropdown menu.","title":"Notebooks"},{"location":"AzureSynapse/#dataflows","text":"To add a source to a dataflow, under Source Settings , click the plus button, then select Azure Data Lake Storage Gen2 (you may need to search for this). Click Continue , select the data format, then on the next page, select your Linked Service.","title":"Dataflows"},{"location":"AzureSynapse/#power-bi-reports","text":"You can view and create Power BI reports directly in Azure Synapse. Please contact the CAE support team to validate that a linked service is setup.","title":"Power BI Reports"},{"location":"AzureSynapse/#integrate","text":"This is where you can create pipelines for ingesting, preparing and transforming all of your data, like in Azure Data Factory .","title":"Integrate"},{"location":"AzureSynapse/#example-copy-data-from-external-blob-to-data-lake","text":"Click the plus button to add a new resource, then click on Pipeline . Under Move & transform , drag and drop Copy data into the window. Click on the Source tab, then click New to add the source dataset (where you want to copy the data from). Select Azure Blob Storage , then select the format type (CSV, Parquet, JSON, etc). Set any additional properties if relevant, then click OK . Click on the Sink , then click New to set the sink dataset (where you want the data to be copied to). Choose Azure Data Lake Storage Gen2 , then select the format type. Under Linked service , choose your data lake and ensure that interactive authoring is enabled (see How to Bring in Data from Linked Services under Data for more info).","title":"Example: Copy Data from External Blob to Data Lake"},{"location":"AzureSynapse/#debugging-and-running-pipelines","text":"To run a pipeline in debug mode, click the Debug button at the top of the pipeline window. Results will appear in the Output tab. To run a pipeline without debugging, click the Add trigger button, then Trigger now . When you are ready to publish your pipelines, click the Validate all button, then click the Publish all button. Note that this will publish for all users to see everything that you currently have open (pipelines, SQL scripts, notebooks, etc).","title":"Debugging and Running Pipelines"},{"location":"AzureSynapse/#monitor","text":"From the Monitor tab, you can monitor live pipeline runs (inputs/outputs of each activity and any errors) and view historical pipeline runs, trigger runs, SQL requests, etc.","title":"Monitor"},{"location":"AzureSynapse/#manage","text":"This is where you can: - Add new SQL or Apache Spark pools - Add new linked services - Grant others access to the workspace - Set up git integration","title":"Manage"},{"location":"AzureSynapse/#microsoft-documentation","text":"Azure Synapse Analytics What is Azure Synapse Analytics? Analyse Data with Dedicated SQL Pools Integrate with Pipelines Visualize Data with Power BI Monitor Your Synapse Workspace","title":"Microsoft Documentation"},{"location":"BonnesPratiques/","text":"Quel est le meilleur format de fichier \u00e0 utiliser pour les fichiers de donn\u00e9es volumineux? Il est recommand\u00e9 d'utiliser un format plus r\u00e9cent comme Parquet, car il enregistre des ensembles de dates plus volumineux dans un fichier plus petit par rapport \u00e0 un fichier CSV. Si vous n'acc\u00e9dez qu'\u00e0 certaines sections de l'ensemble de donn\u00e9es, il est \u00e9galement plus rapide d'utiliser Parquet car il utilise un format de stockage en colonnes. Ai-je besoin d'une base de donn\u00e9es SQL? Dans de nombreux cas, une base de donn\u00e9es SQL n'est pas n\u00e9cessaire, les donn\u00e9es peuvent \u00eatre enregistr\u00e9es dans des fichiers sur le datalake. Ai-je besoin d'une base de donn\u00e9es SQL lorsque j'utilise Power BI? Il n'est pas n\u00e9cessaire d'avoir une base de donn\u00e9es SQL lors de l'utilisation de Power BI. Vous pouvez lire des fichiers \u00e0 partir du stockage Azure. Une base de donn\u00e9es n'est n\u00e9cessaire que lorsque vous utilisez un syst\u00e8me similaire \u00e0 un sch\u00e9ma en \u00e9toile plus complexe. Pour vous connecter au lac de donn\u00e9es interne avec Power BI Desktop, veuillez vous r\u00e9f\u00e9rer \u00e0 ce lien : Comment puis je me connecter au compte de stockage interne data lake avec power bi desktop Comment devons-nous structurer le conteneur de lac de donn\u00e9es de nos projets? Il y a 4 parties dans lesquelles structurer votre conteneur de lac de donn\u00e9es : Zone Bronze/Brute Cette zone stocke le format d'origine de tous les fichiers ou fichiers/donn\u00e9es qui sont immuables. Les donn\u00e9es contenues dans cette zone sont g\u00e9n\u00e9ralement verrouill\u00e9es et ne sont accessibles qu'\u00e0 certains membres ou sont en lecture seule. Cette zone est \u00e9galement organis\u00e9e en diff\u00e9rents dossiers par syst\u00e8me source, chaque processus d'ingestion n'ayant un acc\u00e8s en \u00e9criture qu'\u00e0 son dossier associ\u00e9. Zone Argent/Nettoy\u00e9e Cette zone est l'endroit o\u00f9 des parties de donn\u00e9es suppriment les colonnes inutiles des donn\u00e9es, valide, standardise et harmonise ces donn\u00e9es au sein de cette zone. Cette zone est principalement un dossier par projet. Toutes les donn\u00e9es auxquelles il faut acc\u00e9der dans cette zone b\u00e9n\u00e9ficient g\u00e9n\u00e9ralement d'un acc\u00e8s en lecture seule. Zone Or/Cur\u00e9 Cette zone est principalement destin\u00e9e \u00e0 l'analyse plut\u00f4t qu'\u00e0 l'ingestion ou au traitement de donn\u00e9es. Les donn\u00e9es de la zone organis\u00e9e sont stock\u00e9es dans des sch\u00e9mas en \u00e9toile. La mod\u00e9lisation dimensionnelle est g\u00e9n\u00e9ralement effectu\u00e9e \u00e0 l'aide de Spark ou de Data Factory au lieu de l'int\u00e9rieur du moteur de base de donn\u00e9es. Mais si la mod\u00e9lisation dimensionnelle est effectu\u00e9e \u00e0 l'ext\u00e9rieur du lac, il est pr\u00e9f\u00e9rable de publier le mod\u00e8le dans le lac. Cette zone est la mieux adapt\u00e9e pour ex\u00e9cuter des requ\u00eates et des analyses \u00e0 grande \u00e9chelle qui n'ont pas de besoins stricts en mati\u00e8re de rapports sensibles au temps. Zone Laboratoire Cette zone est principalement destin\u00e9e \u00e0 l'exp\u00e9rimentation et \u00e0 l'exploration. Il est utilis\u00e9 pour le prototype et l'innovation en m\u00e9langeant \u00e0 la fois vos propres ensembles de donn\u00e9es avec des ensembles de donn\u00e9es de production. Cette zone ne remplace pas un lac de donn\u00e9es de d\u00e9veloppement ou de test requis pour un d\u00e9veloppement plus minutieux. Chaque projet de lac de donn\u00e9es wil aurait sa propre zone de laboratoire via un dossier. Les autorisations dans cette zone sont g\u00e9n\u00e9ralement en lecture et en \u00e9criture pour chaque utilisateur/projet. Pour plus d'informations sur la structuration du conteneur de lac de donn\u00e9es de vos projets : Construire votre Data Lake sur Azure Data Lake Storage gen2 Concevoir Azure Data Lake Storage Je re\u00e7ois une exception de m\u00e9moire insuffisante dans Databricks? Option 1: Le moyen le plus rapide et le plus co\u00fbteux de r\u00e9soudre ce probl\u00e8me consiste \u00e0 augmenter la taille de votre cluster. Pour augmenter la taille du cluster, veuillez contacter l'\u00e9quipe d'assistance CAE pour augmenter la taille du cluster. Option 2: Pour une r\u00e9ponse plus programmatique, si vous utilisez des pandas, il est \u00e9galement sugg\u00e9r\u00e9 de basculer et d'utiliser pySpark ou koalas. PySpark et les koalas peuvent s'ex\u00e9cuter plus rapidement que les pandas, ils ont de meilleurs avantages \u00e0 utiliser des pipelines d'ingestion de donn\u00e9es et fonctionnent \u00e9galement efficacement car ils s'ex\u00e9cutent en parall\u00e8le sur diff\u00e9rents n\u0153uds d'un cluster. Op\u00e9rations Spark Dataframe Option 3: Pensez \u00e0 utiliser un sous-ensemble de vos donn\u00e9es lorsque vous effectuez des requ\u00eates si possible. Si vous ne travaillez qu'avec une certaine section de l'ensemble de donn\u00e9es mais que vous l'interrogez sur l'ensemble, il est possible d'utiliser uniquement le sous-ensemble. Option 4: Envisagez de changer le format de fichier en quelque chose comme Parquet ou Avro qui utilise moins d'espace qu'un fichier CSV traditionnel. Conversion de CSV en parquet: % python testConvert = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/mnt/public-data/incoming/titanic.csv' ) testConvert . write . parquet ( \"/mnt/public-data/incoming/testingFile\" ) Conversion de CSV en Avro: % python diamonds = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/mnt/public-data/incoming/titanic.csv' ) diamonds . write . format ( \"avro\" ) . save ( \"/mnt/public-data/incoming/testingFile\" ) Comment puis-je facilement convertir le code SAS en Python ou R? Il n'est pas possible de convertir facilement le code SAS en Python ou R automatiquement, le seul moyen connu de convertir est de faire manuellement la conversion. Comment puis-je valider que je d\u00e9veloppe mon application de la mani\u00e8re la plus rentable dans le cloud \u00e0 l'aide des technologies Microsoft (CAE)? Il existe de nombreuses fa\u00e7ons de valider que votre d\u00e9veloppement est le plus rentable possible: Profitez de Spark dans les databricks. a. Spark est un excellent ajout aux databricks qui s'ex\u00e9cutent plus rapidement et mieux, en particulier pour les grands ensembles de donn\u00e9es. Utiliser Spark co\u00fbterait moins cher car cela prend moins de temps pour faire sa t\u00e2che. Assurez-vous que votre cluster est en cours d'ex\u00e9cution pendant une dur\u00e9e minimale. a. Si le cluster n'est plus n\u00e9cessaire ou n'est pas utilis\u00e9, assurez-vous qu'il n'est pas en cours d'ex\u00e9cution et qu'il ne s'ex\u00e9cute que lorsqu'il est n\u00e9cessaire. Assurez-vous que votre cluster de briques de donn\u00e9es est correctement dimensionn\u00e9. a. Assurez-vous que vous devez corriger le nombre de travailleurs dans votre cluster, un trop grand nombre de clusters entra\u00eene un co\u00fbt plus \u00e9lev\u00e9. Supprimez les fichiers de donn\u00e9es que vous n'utilisez pas. a. Assurez-vous que tous les fichiers qui ne sont plus n\u00e9cessaires ou qui ne sont plus utilis\u00e9s sont supprim\u00e9s du conteneur. Essayez de ne pas effectuer de traitement sur une machine virtuelle cloud. Demandez un examen de votre architecture. Revue de code. Si vous utilisez des Pandas, c'est une bonne id\u00e9e de passer aux Koalas. Utilisez un format de fichier optimal pour votre charge de travail (par exemple, Parquet, Avro) Comment les donn\u00e9es doivent-elles \u00eatre structur\u00e9es si nous pr\u00e9voyons d'utiliser Power BI? Les donn\u00e9es doivent \u00eatre structur\u00e9es \u00e0 l'aide du sch\u00e9ma en \u00e9toile. Pour plus de d\u00e9tails sur l'utilisation de Star Schema, cliquez sur le lien ci-dessous pour en savoir plus sur l'utilisation de Star Schema et les avantages avec Power BI: Documentation du Star Schema Comment lire dans un fichier Excel depuis Databricks? Voici un exemple de lecture d'un fichier Excel \u00e0 l'aide de Python: % python import pandas as pd pd . read_excel ( \"/dbfs/mnt/ccei-ccie-ext/Daily charts.xlsx\" , engine = 'openpyxl' ) Quels types de fichiers sont les meilleurs \u00e0 utiliser quand? Parquet Il est bon \u00e0 utiliser pour de tr\u00e8s grands ensembles de donn\u00e9es. Il est \u00e9galement bon de l'utiliser si seule une section de l'ensemble de donn\u00e9es est n\u00e9cessaire pour lire les donn\u00e9es \u00e0 un rythme plus rapide. Lire: % python data = spark . read . parquet ( \"/tmp/testParquet\" ) display ( data ) \u00c9crivez: % python // Supposition qu 'un cadre de donn\u00e9es a d\u00e9j\u00e0 \u00e9t\u00e9 cr\u00e9\u00e9 data . write . parquet ( \"/tmp/tempFile\" ) Avro Tout comme Parquet, il est id\u00e9al pour les tr\u00e8s grands ensembles de donn\u00e9es. Pour comparer, il est pr\u00e9f\u00e9rable de l'utiliser pour \u00e9diter/\u00e9crire dans un ensemble de donn\u00e9es et pour interroger toutes les colonnes de l'ensemble de donn\u00e9es. Lire: data = spark . read . format ( \"avro\" ) . load ( \"/tmp/test_dataset\" ) display ( data ) \u00c9crivez: % scala val ex = Seq (( 132 , \"baseball\" ), ( 148 , \"softball\" ), ( 172 , \"slow pitch\" )). toDF ( \"players\" , \"sport\" ) ex . write . format ( \"avro\" ). save ( \"/tmp/testExample\" ) CSV Il convient de l'utiliser avec des ensembles de donn\u00e9es l\u00e9g\u00e8rement plus petits, car les fichiers CSV ne se chargent pas bien lorsque la taille du fichier est tr\u00e8s volumineuse. Mais avec des ensembles de donn\u00e9es plus petits, c'est simple et lisible par l'homme. Pour \u00e9crire dans un fichier CSV, il est \u00e9galement bon de noter que vous pouvez modifier le fichier avec Office. Lire: % python data = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/tmp/test_dataCSV.csv' ) display ( data ) Excel Veuillez voir ci-dessus comment utiliser Excel. Les autres formats ci-dessus sont pr\u00e9f\u00e9rables \u00e0 Excel. Comment convertir des fichiers (CSV, Text, JSON) en parquet \u00e0 l'aide de databricks? La r\u00e8gle de base lors de la conversion d'un fichier en parquet est de d'abord lire le fichier, puis d'\u00e9crire un nouveau fichier dans le parquet. CSV vers Parquet: % python testConvert = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/mnt/public-data/incoming/titanic.csv' ) testConvert . write . parquet ( \"/mnt/public-data/incoming/testingFile\" ) JSON vers Parquet: % python testConvert = spark . read . json ( 'tmp/test.json' ) testConvert . write . parquet ( 'tmp/testingJson' ) Text vers Parquet: % python testConvert = spark . read . text ( \"/mnt/public-data/incoming/testing.txt\" ) testConvert . write . parquet ( \"/mnt/public-data/incoming/testingFile\" ) Puis-je lire un document Word dans Databricks? Il est pr\u00e9f\u00e9rable de lire les documents Word via Office \u00e0 la place. Quand devons-nous utiliser ADF ou Databricks pour l'ingestion de donn\u00e9es? Databricks est capable de diffuser en temps r\u00e9el via l'API Apache Spark qui peut g\u00e9rer les charges de travail d'analyse de streaming. Databricks n'a pas besoin que vous encapsulez le code python dans des fonctions ou des modules ex\u00e9cutables, tout le code peut fonctionner tel quel. Databricks prend \u00e9galement en charge le Machine Learning, ce qui facilite \u00e9galement l'ingestion de donn\u00e9es. Pour tout code qui se trouve d\u00e9j\u00e0 dans une fonction Azure ou qui est facilement traduit en un ex\u00e9cutable, l'utilisation de la fabrique de donn\u00e9es est utilisable. L'usine de donn\u00e9es est \u00e9galement bonne \u00e0 utiliser s'il s'agit d'un algorithme lourd qui n'est pas utilisable dans Databricks. Quelle est la diff\u00e9rence entre les tables temporelles de la base de donn\u00e9es SQL et Delta Lake? Les tables temporelles SQL sont sp\u00e9cifiques \u00e0 SQL 2018 et ne sont actuellement pas disponibles dans Azure Synapse. D'autre part, Delta Lake est disponible \u00e0 la fois dans Azure Synapse et dans Databricks. Une autre diff\u00e9rence est que les tables temporelles SQL ne sont disponibles qu'avec des requ\u00eates SQL, tandis que le voyage dans le temps du lac Delta est disponible en Scala, Python et SQL. Quand utiliser Power BI ou R-Shiny? Il est recommand\u00e9 d'utiliser Power BI sur R-shiny car moins de codage est requis lors de l'utilisation de Power BI. L'utilisation de Power BI pr\u00e9sente de nombreux avantages, notamment la quantit\u00e9 suppl\u00e9mentaire de types de graphiques disponibles, la visualisation des donn\u00e9es dans des graphiques est plus facile \u00e0 utiliser dans Power BI par rapport \u00e0 R-Shiny, la cr\u00e9ation d'un tableau de bord est plus rapide dans PowerBI, et la facilit\u00e9 de connectivit\u00e9 avec d'autres applications au sein d'Azure. Quel est le bon moment pour utiliser Azure Synapse vs ADF et Databricks? Azure Synapse est utile pour effectuer des requ\u00eates et des analyses de donn\u00e9es via le lac de donn\u00e9es, effectuer des analyses SQL et un entreposage de donn\u00e9es, et utiliser des services suppl\u00e9mentaires comme Power BI. Il est facile d'interroger les donn\u00e9es du lac de donn\u00e9es \u00e0 l'aide d'Azure Snapse et vous n'avez pas besoin de monter le lac de donn\u00e9es sur l'espace de travail. En ce qui concerne les analyses de donn\u00e9es et l'entreposage de donn\u00e9es, synapse est pr\u00e9f\u00e9r\u00e9 car il permet des mod\u00e8les de donn\u00e9es r\u00e9els complets, fournit toutes les fonctionnalit\u00e9s SQL et utilise \u00e9galement Delta Lake. Synapse inclut \u00e9galement des services directs avec Power BI pour une facilit\u00e9 d'utilisation. D'autre part, Databricks est pr\u00e9f\u00e9r\u00e9 lors du d\u00e9veloppement de l'apprentissage automatique et des transformations en temps r\u00e9el. Databricks inclut son propre d\u00e9veloppement d'apprentissage automatique qui inclut des biblioth\u00e8ques populaires telles que PyTorch, g\u00e8re la version de MLflow. Databricks est \u00e9galement pr\u00e9f\u00e9r\u00e9 pour les transformations en temps r\u00e9el car il utilise le streaming structur\u00e9 Spark et il vous donne la possibilit\u00e9 de visualiser les modifications des autres utilisateurs en temps r\u00e9el. Quand devrions-nous utiliser un entrep\u00f4t de donn\u00e9es de base de donn\u00e9es SQL par rapport \u00e0 Delta Lake? La meilleure pratique serait d'utiliser Delta Lake sur le serveur SQL car il n'utilise pas de ressources de calcul SQL suppl\u00e9mentaires et r\u00e9duira les co\u00fbts globaux du cloud. Comment puis-je facilement convertir des fichiers SAS dans un autre format? Les utilisateurs de Statcan peuvent utiliser SAS sur le r\u00e9seau interne stats-can pour le convertir en un format de fichier pris en charge. Vous pouvez convertir un fichier SAS en CSV ou JSON avec cette m\u00e9thode: Ouvrez d'abord les databricks et installez le convertisseur sas7bdat dans votre ordinateur portable. python %pip install sas7bdat-converter \u00c0 l'aide de python et de l'\u00e9diteur de code de votre choix, saisissez ce code avec le r\u00e9pertoire du fichier dans lequel se trouve le fichier et le r\u00e9pertoire dans lequel vous souhaitez que le fichier de sortie se trouve. ```python %python import sas7bdat_converter file_dicts = [{ 'sas7bdat_file': '/dbfs/mnt/public-data/ToNetA/sas7bdat/tablea_1_10k.sas7bdat', 'export_file': '/dbfs/mnt/public-data/testFolder/testingConvert.csv', }] sas7bdat_converter.batch_to_csv(file_dicts) ``` Vous obtiendrez alors le fichier de sortie dans le r\u00e9pertoire que vous avez sp\u00e9cifi\u00e9. Pour plus d'informations sur le convertisseur, veuillez vous r\u00e9f\u00e9rer \u00e0 ce lien : Documentation du sas7bdat Puis-je convertir un document Word en bloc-notes? Il n'y a pas de moyen facile de convertir un document Word en bloc-notes. Une solution manuelle pour convertir un document Word en bloc-notes consiste \u00e0 copier le code contenu dans le document Word dans un bloc-notes. Quelle taille de table dataframe/spark peut-on stocker dans l'espace de travail? Les tables Spark sont stock\u00e9es en tant que fichiers parquet et sont stock\u00e9es dans le compte de stockage interne li\u00e9 \u00e0 l'espace de travail Databricks, mais il est recommand\u00e9 de supprimer la table si elle n'est plus utilis\u00e9e. Quelle est la meilleure fa\u00e7on d'obtenir des fichiers de donn\u00e9es dans Azure ML? Le meilleur moyen serait de t\u00e9l\u00e9charger vos fichiers sur le lac de donn\u00e9es. Si vous devez ajouter un nouveau compte de stockage cloud, contactez l'\u00e9quipe CAE pour ajouter le compte de stockage au studio Azure ML. Quelle est la diff\u00e9rence avec le Machine Learning dans Databricks ou dans Azure ML? La principale diff\u00e9rence entre Azure ML et Databricks r\u00e9side dans le langage utilis\u00e9 par chaque application. Azure ML utilise des biblioth\u00e8ques bas\u00e9es sur python ou R tandis que Databricks utilise la plate-forme Apache Spark et MLFlow. Azure ML contient \u00e9galement un syst\u00e8me de suivi capable de suivre les ex\u00e9cutions individuelles de l'exp\u00e9rience et d'inclure les m\u00e9triques sp\u00e9cifiques de ce qui doit \u00eatre vu. Databricks inclut MLflow qui permet \u00e9galement le suivi mais n'offre pas autant de fonctionnalit\u00e9s qu'Azure ML. \u00c0 titre de recommandation, il est recommand\u00e9 d'utiliser Databricks pour la pr\u00e9paration des donn\u00e9es et pour les grands ensembles de donn\u00e9es, mais d'utiliser Azure ML pour leur syst\u00e8me de suivi, l'apprentissage automatique sur les ensembles de donn\u00e9es normaux, l'apprentissage profond sur les GPU et l'op\u00e9rationnalisation. Comment cr\u00e9er une table dans Databricks? Option 1 : Utiliser la fonction Cr\u00e9er une table Dans Databricks, s\u00e9lectionnez Donn\u00e9es et dans la base de donn\u00e9es que vous avez s\u00e9lectionn\u00e9e, cliquez sur Cr\u00e9er une table. Pour plus d'informations sur cette option, veuillez consulter ce lien : Databricks cr\u00e9er un tableau Option 2 : Cr\u00e9er une table \u00e0 partir d'une table Dataframe Python: df . write . saveAsTable ( \"Table-Name\" ) SQL: CREATE TABLE IF NOT EXISTS Table - Name AS SELECT * FROM df Option 3: Create Table Programatically SQL: CREATE TABLE example (id INT, name STRING, age INT) USING CSV; Quand utiliser Spark Dataframe ou Spark Table? Il n'y a vraiment aucune diff\u00e9rence entre l'utilisation d'un Spark Dataframe ou Spark Table. Actuellement avec Databricks, la meilleure pratique en ce moment serait de stocker les tables en tant que tables delta car elles sont enregistr\u00e9es au format parquet et donnent les capacit\u00e9s de suivi. Que dois-je faire si la taille de la table diffus\u00e9e d\u00e9passe de loin les estimations et d\u00e9passe la limite de spark.driver.maxResultSize = _____? Modifiez la configuration Spark \"spark.driver.maxResultSize\" en \"0\" (signifie pas de limite) ou quelque chose de plus grand que vos besoins. Que dois-je faire si je ne peux pas diffuser la table dont la taille est sup\u00e9rieure \u00e0 8GB? Cela se produit uniquement avec BroadcastHashJoin. Il y a 2 possibilit\u00e9s : Remplacez la configuration Spark \"spark.sql.autoBroadcastJoinThreshold\" par \"-1\". Cela force Databricks \u00e0 effectuer un SortMergeJoin. Remarque sur la modification de la configuration Spark Avertissement : La modification des configurations Spark peut entra\u00eener des erreurs de m\u00e9moire insuffisante Approche normale: - spark.conf.set(\"configuration\", \"value\") Si vous n'\u00eates pas autoris\u00e9 \u00e0 modifier certaines configurations, cela semble \u00eatre une solution: - conf = spark.sparkContext._cibf,setAkk([(\"configuration\", \"value\"), (\"configuration\", \"value\")]) Comment obtenir la configuration Spark : - spark.conf.get(\"configuration\") \u00c9tapes pour \u00e9viter de modifier les configurations: a. Partitionner le DataFrame A en plusieurs parties. b. Effectuez des jointures avec chaque partition de DataFrame A avec DataFrame B (c'est simultan\u00e9ment le moyen le plus rapide mais peut n\u00e9cessiter l'\u00e9criture de Dataframes dans un fichier pour la lecture \u00e0 l'\u00e9tape suivante). c. Effectuez une union sur tous les DataFrames joints.","title":"Bonnes pratiques"},{"location":"BonnesPratiques/#quel-est-le-meilleur-format-de-fichier-a-utiliser-pour-les-fichiers-de-donnees-volumineux","text":"Il est recommand\u00e9 d'utiliser un format plus r\u00e9cent comme Parquet, car il enregistre des ensembles de dates plus volumineux dans un fichier plus petit par rapport \u00e0 un fichier CSV. Si vous n'acc\u00e9dez qu'\u00e0 certaines sections de l'ensemble de donn\u00e9es, il est \u00e9galement plus rapide d'utiliser Parquet car il utilise un format de stockage en colonnes.","title":"Quel est le meilleur format de fichier \u00e0 utiliser pour les fichiers de donn\u00e9es volumineux?"},{"location":"BonnesPratiques/#ai-je-besoin-dune-base-de-donnees-sql","text":"Dans de nombreux cas, une base de donn\u00e9es SQL n'est pas n\u00e9cessaire, les donn\u00e9es peuvent \u00eatre enregistr\u00e9es dans des fichiers sur le datalake.","title":"Ai-je besoin d'une base de donn\u00e9es SQL?"},{"location":"BonnesPratiques/#ai-je-besoin-dune-base-de-donnees-sql-lorsque-jutilise-power-bi","text":"Il n'est pas n\u00e9cessaire d'avoir une base de donn\u00e9es SQL lors de l'utilisation de Power BI. Vous pouvez lire des fichiers \u00e0 partir du stockage Azure. Une base de donn\u00e9es n'est n\u00e9cessaire que lorsque vous utilisez un syst\u00e8me similaire \u00e0 un sch\u00e9ma en \u00e9toile plus complexe. Pour vous connecter au lac de donn\u00e9es interne avec Power BI Desktop, veuillez vous r\u00e9f\u00e9rer \u00e0 ce lien : Comment puis je me connecter au compte de stockage interne data lake avec power bi desktop","title":"Ai-je besoin d'une base de donn\u00e9es SQL lorsque j'utilise Power BI?"},{"location":"BonnesPratiques/#comment-devons-nous-structurer-le-conteneur-de-lac-de-donnees-de-nos-projets","text":"Il y a 4 parties dans lesquelles structurer votre conteneur de lac de donn\u00e9es :","title":"Comment devons-nous structurer le conteneur de lac de donn\u00e9es de nos projets?"},{"location":"BonnesPratiques/#zone-bronzebrute","text":"Cette zone stocke le format d'origine de tous les fichiers ou fichiers/donn\u00e9es qui sont immuables. Les donn\u00e9es contenues dans cette zone sont g\u00e9n\u00e9ralement verrouill\u00e9es et ne sont accessibles qu'\u00e0 certains membres ou sont en lecture seule. Cette zone est \u00e9galement organis\u00e9e en diff\u00e9rents dossiers par syst\u00e8me source, chaque processus d'ingestion n'ayant un acc\u00e8s en \u00e9criture qu'\u00e0 son dossier associ\u00e9.","title":"Zone Bronze/Brute"},{"location":"BonnesPratiques/#zone-argentnettoyee","text":"Cette zone est l'endroit o\u00f9 des parties de donn\u00e9es suppriment les colonnes inutiles des donn\u00e9es, valide, standardise et harmonise ces donn\u00e9es au sein de cette zone. Cette zone est principalement un dossier par projet. Toutes les donn\u00e9es auxquelles il faut acc\u00e9der dans cette zone b\u00e9n\u00e9ficient g\u00e9n\u00e9ralement d'un acc\u00e8s en lecture seule.","title":"Zone Argent/Nettoy\u00e9e"},{"location":"BonnesPratiques/#zone-orcure","text":"Cette zone est principalement destin\u00e9e \u00e0 l'analyse plut\u00f4t qu'\u00e0 l'ingestion ou au traitement de donn\u00e9es. Les donn\u00e9es de la zone organis\u00e9e sont stock\u00e9es dans des sch\u00e9mas en \u00e9toile. La mod\u00e9lisation dimensionnelle est g\u00e9n\u00e9ralement effectu\u00e9e \u00e0 l'aide de Spark ou de Data Factory au lieu de l'int\u00e9rieur du moteur de base de donn\u00e9es. Mais si la mod\u00e9lisation dimensionnelle est effectu\u00e9e \u00e0 l'ext\u00e9rieur du lac, il est pr\u00e9f\u00e9rable de publier le mod\u00e8le dans le lac. Cette zone est la mieux adapt\u00e9e pour ex\u00e9cuter des requ\u00eates et des analyses \u00e0 grande \u00e9chelle qui n'ont pas de besoins stricts en mati\u00e8re de rapports sensibles au temps.","title":"Zone Or/Cur\u00e9"},{"location":"BonnesPratiques/#zone-laboratoire","text":"Cette zone est principalement destin\u00e9e \u00e0 l'exp\u00e9rimentation et \u00e0 l'exploration. Il est utilis\u00e9 pour le prototype et l'innovation en m\u00e9langeant \u00e0 la fois vos propres ensembles de donn\u00e9es avec des ensembles de donn\u00e9es de production. Cette zone ne remplace pas un lac de donn\u00e9es de d\u00e9veloppement ou de test requis pour un d\u00e9veloppement plus minutieux. Chaque projet de lac de donn\u00e9es wil aurait sa propre zone de laboratoire via un dossier. Les autorisations dans cette zone sont g\u00e9n\u00e9ralement en lecture et en \u00e9criture pour chaque utilisateur/projet. Pour plus d'informations sur la structuration du conteneur de lac de donn\u00e9es de vos projets : Construire votre Data Lake sur Azure Data Lake Storage gen2 Concevoir Azure Data Lake Storage","title":"Zone Laboratoire"},{"location":"BonnesPratiques/#je-recois-une-exception-de-memoire-insuffisante-dans-databricks","text":"","title":"Je re\u00e7ois une exception de m\u00e9moire insuffisante dans Databricks?"},{"location":"BonnesPratiques/#option-1","text":"Le moyen le plus rapide et le plus co\u00fbteux de r\u00e9soudre ce probl\u00e8me consiste \u00e0 augmenter la taille de votre cluster. Pour augmenter la taille du cluster, veuillez contacter l'\u00e9quipe d'assistance CAE pour augmenter la taille du cluster.","title":"Option 1:"},{"location":"BonnesPratiques/#option-2","text":"Pour une r\u00e9ponse plus programmatique, si vous utilisez des pandas, il est \u00e9galement sugg\u00e9r\u00e9 de basculer et d'utiliser pySpark ou koalas. PySpark et les koalas peuvent s'ex\u00e9cuter plus rapidement que les pandas, ils ont de meilleurs avantages \u00e0 utiliser des pipelines d'ingestion de donn\u00e9es et fonctionnent \u00e9galement efficacement car ils s'ex\u00e9cutent en parall\u00e8le sur diff\u00e9rents n\u0153uds d'un cluster. Op\u00e9rations Spark Dataframe","title":"Option 2:"},{"location":"BonnesPratiques/#option-3","text":"Pensez \u00e0 utiliser un sous-ensemble de vos donn\u00e9es lorsque vous effectuez des requ\u00eates si possible. Si vous ne travaillez qu'avec une certaine section de l'ensemble de donn\u00e9es mais que vous l'interrogez sur l'ensemble, il est possible d'utiliser uniquement le sous-ensemble.","title":"Option 3:"},{"location":"BonnesPratiques/#option-4","text":"Envisagez de changer le format de fichier en quelque chose comme Parquet ou Avro qui utilise moins d'espace qu'un fichier CSV traditionnel. Conversion de CSV en parquet: % python testConvert = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/mnt/public-data/incoming/titanic.csv' ) testConvert . write . parquet ( \"/mnt/public-data/incoming/testingFile\" ) Conversion de CSV en Avro: % python diamonds = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/mnt/public-data/incoming/titanic.csv' ) diamonds . write . format ( \"avro\" ) . save ( \"/mnt/public-data/incoming/testingFile\" )","title":"Option 4:"},{"location":"BonnesPratiques/#comment-puis-je-facilement-convertir-le-code-sas-en-python-ou-r","text":"Il n'est pas possible de convertir facilement le code SAS en Python ou R automatiquement, le seul moyen connu de convertir est de faire manuellement la conversion.","title":"Comment puis-je facilement convertir le code SAS en Python ou R?"},{"location":"BonnesPratiques/#comment-puis-je-valider-que-je-developpe-mon-application-de-la-maniere-la-plus-rentable-dans-le-cloud-a-laide-des-technologies-microsoft-cae","text":"Il existe de nombreuses fa\u00e7ons de valider que votre d\u00e9veloppement est le plus rentable possible: Profitez de Spark dans les databricks. a. Spark est un excellent ajout aux databricks qui s'ex\u00e9cutent plus rapidement et mieux, en particulier pour les grands ensembles de donn\u00e9es. Utiliser Spark co\u00fbterait moins cher car cela prend moins de temps pour faire sa t\u00e2che. Assurez-vous que votre cluster est en cours d'ex\u00e9cution pendant une dur\u00e9e minimale. a. Si le cluster n'est plus n\u00e9cessaire ou n'est pas utilis\u00e9, assurez-vous qu'il n'est pas en cours d'ex\u00e9cution et qu'il ne s'ex\u00e9cute que lorsqu'il est n\u00e9cessaire. Assurez-vous que votre cluster de briques de donn\u00e9es est correctement dimensionn\u00e9. a. Assurez-vous que vous devez corriger le nombre de travailleurs dans votre cluster, un trop grand nombre de clusters entra\u00eene un co\u00fbt plus \u00e9lev\u00e9. Supprimez les fichiers de donn\u00e9es que vous n'utilisez pas. a. Assurez-vous que tous les fichiers qui ne sont plus n\u00e9cessaires ou qui ne sont plus utilis\u00e9s sont supprim\u00e9s du conteneur. Essayez de ne pas effectuer de traitement sur une machine virtuelle cloud. Demandez un examen de votre architecture. Revue de code. Si vous utilisez des Pandas, c'est une bonne id\u00e9e de passer aux Koalas. Utilisez un format de fichier optimal pour votre charge de travail (par exemple, Parquet, Avro)","title":"Comment puis-je valider que je d\u00e9veloppe mon application de la mani\u00e8re la plus rentable dans le cloud \u00e0 l'aide des technologies Microsoft (CAE)?"},{"location":"BonnesPratiques/#comment-les-donnees-doivent-elles-etre-structurees-si-nous-prevoyons-dutiliser-power-bi","text":"Les donn\u00e9es doivent \u00eatre structur\u00e9es \u00e0 l'aide du sch\u00e9ma en \u00e9toile. Pour plus de d\u00e9tails sur l'utilisation de Star Schema, cliquez sur le lien ci-dessous pour en savoir plus sur l'utilisation de Star Schema et les avantages avec Power BI: Documentation du Star Schema","title":"Comment les donn\u00e9es doivent-elles \u00eatre structur\u00e9es si nous pr\u00e9voyons d'utiliser Power BI?"},{"location":"BonnesPratiques/#comment-lire-dans-un-fichier-excel-depuis-databricks","text":"Voici un exemple de lecture d'un fichier Excel \u00e0 l'aide de Python: % python import pandas as pd pd . read_excel ( \"/dbfs/mnt/ccei-ccie-ext/Daily charts.xlsx\" , engine = 'openpyxl' )","title":"Comment lire dans un fichier Excel depuis Databricks?"},{"location":"BonnesPratiques/#quels-types-de-fichiers-sont-les-meilleurs-a-utiliser-quand","text":"","title":"Quels types de fichiers sont les meilleurs \u00e0 utiliser quand?"},{"location":"BonnesPratiques/#parquet","text":"Il est bon \u00e0 utiliser pour de tr\u00e8s grands ensembles de donn\u00e9es. Il est \u00e9galement bon de l'utiliser si seule une section de l'ensemble de donn\u00e9es est n\u00e9cessaire pour lire les donn\u00e9es \u00e0 un rythme plus rapide. Lire: % python data = spark . read . parquet ( \"/tmp/testParquet\" ) display ( data ) \u00c9crivez: % python // Supposition qu 'un cadre de donn\u00e9es a d\u00e9j\u00e0 \u00e9t\u00e9 cr\u00e9\u00e9 data . write . parquet ( \"/tmp/tempFile\" )","title":"Parquet"},{"location":"BonnesPratiques/#avro","text":"Tout comme Parquet, il est id\u00e9al pour les tr\u00e8s grands ensembles de donn\u00e9es. Pour comparer, il est pr\u00e9f\u00e9rable de l'utiliser pour \u00e9diter/\u00e9crire dans un ensemble de donn\u00e9es et pour interroger toutes les colonnes de l'ensemble de donn\u00e9es. Lire: data = spark . read . format ( \"avro\" ) . load ( \"/tmp/test_dataset\" ) display ( data ) \u00c9crivez: % scala val ex = Seq (( 132 , \"baseball\" ), ( 148 , \"softball\" ), ( 172 , \"slow pitch\" )). toDF ( \"players\" , \"sport\" ) ex . write . format ( \"avro\" ). save ( \"/tmp/testExample\" )","title":"Avro"},{"location":"BonnesPratiques/#csv","text":"Il convient de l'utiliser avec des ensembles de donn\u00e9es l\u00e9g\u00e8rement plus petits, car les fichiers CSV ne se chargent pas bien lorsque la taille du fichier est tr\u00e8s volumineuse. Mais avec des ensembles de donn\u00e9es plus petits, c'est simple et lisible par l'homme. Pour \u00e9crire dans un fichier CSV, il est \u00e9galement bon de noter que vous pouvez modifier le fichier avec Office. Lire: % python data = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/tmp/test_dataCSV.csv' ) display ( data )","title":"CSV"},{"location":"BonnesPratiques/#excel","text":"Veuillez voir ci-dessus comment utiliser Excel. Les autres formats ci-dessus sont pr\u00e9f\u00e9rables \u00e0 Excel.","title":"Excel"},{"location":"BonnesPratiques/#comment-convertir-des-fichiers-csv-text-json-en-parquet-a-laide-de-databricks","text":"La r\u00e8gle de base lors de la conversion d'un fichier en parquet est de d'abord lire le fichier, puis d'\u00e9crire un nouveau fichier dans le parquet. CSV vers Parquet: % python testConvert = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/mnt/public-data/incoming/titanic.csv' ) testConvert . write . parquet ( \"/mnt/public-data/incoming/testingFile\" ) JSON vers Parquet: % python testConvert = spark . read . json ( 'tmp/test.json' ) testConvert . write . parquet ( 'tmp/testingJson' ) Text vers Parquet: % python testConvert = spark . read . text ( \"/mnt/public-data/incoming/testing.txt\" ) testConvert . write . parquet ( \"/mnt/public-data/incoming/testingFile\" )","title":"Comment convertir des fichiers (CSV, Text, JSON) en parquet \u00e0 l'aide de databricks?"},{"location":"BonnesPratiques/#puis-je-lire-un-document-word-dans-databricks","text":"Il est pr\u00e9f\u00e9rable de lire les documents Word via Office \u00e0 la place.","title":"Puis-je lire un document Word dans Databricks?"},{"location":"BonnesPratiques/#quand-devons-nous-utiliser-adf-ou-databricks-pour-lingestion-de-donnees","text":"Databricks est capable de diffuser en temps r\u00e9el via l'API Apache Spark qui peut g\u00e9rer les charges de travail d'analyse de streaming. Databricks n'a pas besoin que vous encapsulez le code python dans des fonctions ou des modules ex\u00e9cutables, tout le code peut fonctionner tel quel. Databricks prend \u00e9galement en charge le Machine Learning, ce qui facilite \u00e9galement l'ingestion de donn\u00e9es. Pour tout code qui se trouve d\u00e9j\u00e0 dans une fonction Azure ou qui est facilement traduit en un ex\u00e9cutable, l'utilisation de la fabrique de donn\u00e9es est utilisable. L'usine de donn\u00e9es est \u00e9galement bonne \u00e0 utiliser s'il s'agit d'un algorithme lourd qui n'est pas utilisable dans Databricks.","title":"Quand devons-nous utiliser ADF ou Databricks pour l'ingestion de donn\u00e9es?"},{"location":"BonnesPratiques/#quelle-est-la-difference-entre-les-tables-temporelles-de-la-base-de-donnees-sql-et-delta-lake","text":"Les tables temporelles SQL sont sp\u00e9cifiques \u00e0 SQL 2018 et ne sont actuellement pas disponibles dans Azure Synapse. D'autre part, Delta Lake est disponible \u00e0 la fois dans Azure Synapse et dans Databricks. Une autre diff\u00e9rence est que les tables temporelles SQL ne sont disponibles qu'avec des requ\u00eates SQL, tandis que le voyage dans le temps du lac Delta est disponible en Scala, Python et SQL.","title":"Quelle est la diff\u00e9rence entre les tables temporelles de la base de donn\u00e9es SQL et Delta Lake?"},{"location":"BonnesPratiques/#quand-utiliser-power-bi-ou-r-shiny","text":"Il est recommand\u00e9 d'utiliser Power BI sur R-shiny car moins de codage est requis lors de l'utilisation de Power BI. L'utilisation de Power BI pr\u00e9sente de nombreux avantages, notamment la quantit\u00e9 suppl\u00e9mentaire de types de graphiques disponibles, la visualisation des donn\u00e9es dans des graphiques est plus facile \u00e0 utiliser dans Power BI par rapport \u00e0 R-Shiny, la cr\u00e9ation d'un tableau de bord est plus rapide dans PowerBI, et la facilit\u00e9 de connectivit\u00e9 avec d'autres applications au sein d'Azure.","title":"Quand utiliser Power BI ou R-Shiny?"},{"location":"BonnesPratiques/#quel-est-le-bon-moment-pour-utiliser-azure-synapse-vs-adf-et-databricks","text":"Azure Synapse est utile pour effectuer des requ\u00eates et des analyses de donn\u00e9es via le lac de donn\u00e9es, effectuer des analyses SQL et un entreposage de donn\u00e9es, et utiliser des services suppl\u00e9mentaires comme Power BI. Il est facile d'interroger les donn\u00e9es du lac de donn\u00e9es \u00e0 l'aide d'Azure Snapse et vous n'avez pas besoin de monter le lac de donn\u00e9es sur l'espace de travail. En ce qui concerne les analyses de donn\u00e9es et l'entreposage de donn\u00e9es, synapse est pr\u00e9f\u00e9r\u00e9 car il permet des mod\u00e8les de donn\u00e9es r\u00e9els complets, fournit toutes les fonctionnalit\u00e9s SQL et utilise \u00e9galement Delta Lake. Synapse inclut \u00e9galement des services directs avec Power BI pour une facilit\u00e9 d'utilisation. D'autre part, Databricks est pr\u00e9f\u00e9r\u00e9 lors du d\u00e9veloppement de l'apprentissage automatique et des transformations en temps r\u00e9el. Databricks inclut son propre d\u00e9veloppement d'apprentissage automatique qui inclut des biblioth\u00e8ques populaires telles que PyTorch, g\u00e8re la version de MLflow. Databricks est \u00e9galement pr\u00e9f\u00e9r\u00e9 pour les transformations en temps r\u00e9el car il utilise le streaming structur\u00e9 Spark et il vous donne la possibilit\u00e9 de visualiser les modifications des autres utilisateurs en temps r\u00e9el.","title":"Quel est le bon moment pour utiliser Azure Synapse vs ADF et Databricks?"},{"location":"BonnesPratiques/#quand-devrions-nous-utiliser-un-entrepot-de-donnees-de-base-de-donnees-sql-par-rapport-a-delta-lake","text":"La meilleure pratique serait d'utiliser Delta Lake sur le serveur SQL car il n'utilise pas de ressources de calcul SQL suppl\u00e9mentaires et r\u00e9duira les co\u00fbts globaux du cloud.","title":"Quand devrions-nous utiliser un entrep\u00f4t de donn\u00e9es de base de donn\u00e9es SQL par rapport \u00e0 Delta Lake?"},{"location":"BonnesPratiques/#comment-puis-je-facilement-convertir-des-fichiers-sas-dans-un-autre-format","text":"Les utilisateurs de Statcan peuvent utiliser SAS sur le r\u00e9seau interne stats-can pour le convertir en un format de fichier pris en charge. Vous pouvez convertir un fichier SAS en CSV ou JSON avec cette m\u00e9thode: Ouvrez d'abord les databricks et installez le convertisseur sas7bdat dans votre ordinateur portable. python %pip install sas7bdat-converter \u00c0 l'aide de python et de l'\u00e9diteur de code de votre choix, saisissez ce code avec le r\u00e9pertoire du fichier dans lequel se trouve le fichier et le r\u00e9pertoire dans lequel vous souhaitez que le fichier de sortie se trouve. ```python %python import sas7bdat_converter file_dicts = [{ 'sas7bdat_file': '/dbfs/mnt/public-data/ToNetA/sas7bdat/tablea_1_10k.sas7bdat', 'export_file': '/dbfs/mnt/public-data/testFolder/testingConvert.csv', }] sas7bdat_converter.batch_to_csv(file_dicts) ``` Vous obtiendrez alors le fichier de sortie dans le r\u00e9pertoire que vous avez sp\u00e9cifi\u00e9. Pour plus d'informations sur le convertisseur, veuillez vous r\u00e9f\u00e9rer \u00e0 ce lien : Documentation du sas7bdat","title":"Comment puis-je facilement convertir des fichiers SAS dans un autre format?"},{"location":"BonnesPratiques/#puis-je-convertir-un-document-word-en-bloc-notes","text":"Il n'y a pas de moyen facile de convertir un document Word en bloc-notes. Une solution manuelle pour convertir un document Word en bloc-notes consiste \u00e0 copier le code contenu dans le document Word dans un bloc-notes.","title":"Puis-je convertir un document Word en bloc-notes?"},{"location":"BonnesPratiques/#quelle-taille-de-table-dataframespark-peut-on-stocker-dans-lespace-de-travail","text":"Les tables Spark sont stock\u00e9es en tant que fichiers parquet et sont stock\u00e9es dans le compte de stockage interne li\u00e9 \u00e0 l'espace de travail Databricks, mais il est recommand\u00e9 de supprimer la table si elle n'est plus utilis\u00e9e.","title":"Quelle taille de table dataframe/spark peut-on stocker dans l'espace de travail?"},{"location":"BonnesPratiques/#quelle-est-la-meilleure-facon-dobtenir-des-fichiers-de-donnees-dans-azure-ml","text":"Le meilleur moyen serait de t\u00e9l\u00e9charger vos fichiers sur le lac de donn\u00e9es. Si vous devez ajouter un nouveau compte de stockage cloud, contactez l'\u00e9quipe CAE pour ajouter le compte de stockage au studio Azure ML.","title":"Quelle est la meilleure fa\u00e7on d'obtenir des fichiers de donn\u00e9es dans Azure ML?"},{"location":"BonnesPratiques/#quelle-est-la-difference-avec-le-machine-learning-dans-databricks-ou-dans-azure-ml","text":"La principale diff\u00e9rence entre Azure ML et Databricks r\u00e9side dans le langage utilis\u00e9 par chaque application. Azure ML utilise des biblioth\u00e8ques bas\u00e9es sur python ou R tandis que Databricks utilise la plate-forme Apache Spark et MLFlow. Azure ML contient \u00e9galement un syst\u00e8me de suivi capable de suivre les ex\u00e9cutions individuelles de l'exp\u00e9rience et d'inclure les m\u00e9triques sp\u00e9cifiques de ce qui doit \u00eatre vu. Databricks inclut MLflow qui permet \u00e9galement le suivi mais n'offre pas autant de fonctionnalit\u00e9s qu'Azure ML. \u00c0 titre de recommandation, il est recommand\u00e9 d'utiliser Databricks pour la pr\u00e9paration des donn\u00e9es et pour les grands ensembles de donn\u00e9es, mais d'utiliser Azure ML pour leur syst\u00e8me de suivi, l'apprentissage automatique sur les ensembles de donn\u00e9es normaux, l'apprentissage profond sur les GPU et l'op\u00e9rationnalisation.","title":"Quelle est la diff\u00e9rence avec le Machine Learning dans Databricks ou dans Azure ML?"},{"location":"BonnesPratiques/#comment-creer-une-table-dans-databricks","text":"","title":"Comment cr\u00e9er une table dans Databricks?"},{"location":"BonnesPratiques/#option-1-utiliser-la-fonction-creer-une-table","text":"Dans Databricks, s\u00e9lectionnez Donn\u00e9es et dans la base de donn\u00e9es que vous avez s\u00e9lectionn\u00e9e, cliquez sur Cr\u00e9er une table. Pour plus d'informations sur cette option, veuillez consulter ce lien : Databricks cr\u00e9er un tableau","title":"Option 1 : Utiliser la fonction Cr\u00e9er une table"},{"location":"BonnesPratiques/#option-2-creer-une-table-a-partir-dune-table-dataframe","text":"Python: df . write . saveAsTable ( \"Table-Name\" ) SQL: CREATE TABLE IF NOT EXISTS Table - Name AS SELECT * FROM df","title":"Option\u00a02\u00a0:\u00a0Cr\u00e9er une table \u00e0 partir d'une table Dataframe"},{"location":"BonnesPratiques/#option-3-create-table-programatically","text":"SQL: CREATE TABLE example (id INT, name STRING, age INT) USING CSV;","title":"Option 3: Create Table Programatically"},{"location":"BonnesPratiques/#quand-utiliser-spark-dataframe-ou-spark-table","text":"Il n'y a vraiment aucune diff\u00e9rence entre l'utilisation d'un Spark Dataframe ou Spark Table. Actuellement avec Databricks, la meilleure pratique en ce moment serait de stocker les tables en tant que tables delta car elles sont enregistr\u00e9es au format parquet et donnent les capacit\u00e9s de suivi.","title":"Quand utiliser Spark Dataframe ou Spark Table?"},{"location":"BonnesPratiques/#que-dois-je-faire-si-la-taille-de-la-table-diffusee-depasse-de-loin-les-estimations-et-depasse-la-limite-de-sparkdrivermaxresultsize-_____","text":"Modifiez la configuration Spark \"spark.driver.maxResultSize\" en \"0\" (signifie pas de limite) ou quelque chose de plus grand que vos besoins.","title":"Que dois-je faire si la taille de la table diffus\u00e9e d\u00e9passe de loin les estimations et d\u00e9passe la limite de spark.driver.maxResultSize = _____?"},{"location":"BonnesPratiques/#que-dois-je-faire-si-je-ne-peux-pas-diffuser-la-table-dont-la-taille-est-superieure-a-8gb","text":"Cela se produit uniquement avec BroadcastHashJoin. Il y a 2 possibilit\u00e9s : Remplacez la configuration Spark \"spark.sql.autoBroadcastJoinThreshold\" par \"-1\". Cela force Databricks \u00e0 effectuer un SortMergeJoin.","title":"Que dois-je faire si je ne peux pas diffuser la table dont la taille est sup\u00e9rieure \u00e0 8GB?"},{"location":"BonnesPratiques/#remarque-sur-la-modification-de-la-configuration-spark","text":"Avertissement : La modification des configurations Spark peut entra\u00eener des erreurs de m\u00e9moire insuffisante Approche normale: - spark.conf.set(\"configuration\", \"value\") Si vous n'\u00eates pas autoris\u00e9 \u00e0 modifier certaines configurations, cela semble \u00eatre une solution: - conf = spark.sparkContext._cibf,setAkk([(\"configuration\", \"value\"), (\"configuration\", \"value\")]) Comment obtenir la configuration Spark : - spark.conf.get(\"configuration\") \u00c9tapes pour \u00e9viter de modifier les configurations: a. Partitionner le DataFrame A en plusieurs parties. b. Effectuez des jointures avec chaque partition de DataFrame A avec DataFrame B (c'est simultan\u00e9ment le moyen le plus rapide mais peut n\u00e9cessiter l'\u00e9criture de Dataframes dans un fichier pour la lecture \u00e0 l'\u00e9tape suivante). c. Effectuez une union sur tous les DataFrames joints.","title":"Remarque sur la modification de la configuration Spark"},{"location":"ContactezNous/","text":"Soutien technique Abonnez-vous au canal de Slack suivant : https://cae-eac.slack.com Commentaires Utilisez le formulaire Commentaires sur ce site Web : https://www.statcan.gc.ca/data-analytics-service/fr Documentation https://statcan.github.io/cae-eac/fr/ Foire aux questions (FAQ) Bonnes pratiques Documentation Microsoft Documentation Azure","title":"Contactez-nous"},{"location":"ContactezNous/#soutien-technique","text":"Abonnez-vous au canal de Slack suivant : https://cae-eac.slack.com","title":"Soutien technique"},{"location":"ContactezNous/#commentaires","text":"Utilisez le formulaire Commentaires sur ce site Web : https://www.statcan.gc.ca/data-analytics-service/fr","title":"Commentaires"},{"location":"ContactezNous/#documentation","text":"https://statcan.github.io/cae-eac/fr/ Foire aux questions (FAQ) Bonnes pratiques","title":"Documentation"},{"location":"ContactezNous/#documentation-microsoft","text":"Documentation Azure","title":"Documentation Microsoft"},{"location":"DataBricks/","text":"Acc\u00e8s \u00e0 Databricks Tableau de bord Consultez la section Tableau de bord de cette documentation pour obtenir de plus amples renseignements. Cliquez sur le menu Tableau de bord dans le portail Azure. Adresse URL de Databricks Rendez-vous au site https://canadacentral.azuredatabricks.net/ , ouvrez une session en utilisant vos justificatifs d'identit\u00e9 du compte infonuagique et s\u00e9lectionnez l'espace de travail Databricks qui a \u00e9t\u00e9 cr\u00e9\u00e9 pour vous. Portail Azure Dans la bo\u00eete de recherche du portail Azure, recherchez Databricks . Vous devriez alors voir une liste des espaces de travail Databricks auxquels vous pouvez acc\u00e9der. Commencer Une fois dans Databricks, vous pouvez cr\u00e9er un carnet ou ouvrir un carnet existant. Pour plus d'information, veuillez consulter le processus \u00e0 suivre pour acc\u00e9der \u00e0 Databricks pour la premi\u00e8re fois . Cr\u00e9ation d'un cluster Si aucun cluster n'a \u00e9t\u00e9 cr\u00e9\u00e9 pour vous ou si vous avez besoin d'apporter des changements \u00e0 votre cluster, veuillez envoyer un message sur Slack , puisque vous n'avez pas la permission de cr\u00e9er un cluster. Note : Vous devez avoir un cluster en mode actif avant de pouvoir ex\u00e9cuter le code dans votre carnet. Pour obtenir des renseignements sur la fa\u00e7on de d\u00e9marrer un cluster, consultez la section ci-dessous ou la FAQ . Cr\u00e9ation d'un carnet Une fa\u00e7on de cr\u00e9er un carnet consiste \u00e0 cliquer sur l'option Nouveau carnet depuis la page principale de Databricks. Vous pouvez ensuite nommer votre carnet et choisir le langage par d\u00e9faut. Dans le champ \u00ab Grappe \u00bb, s\u00e9lectionnez parmis la liste propos\u00e9e le cluster auquel vous souhaitez attacher votre carnet. Pour d\u00e9marrer un cluster ou le modifier \u00e0 partir d'un carnet, ouvrez le carnet et, dans le coin sup\u00e9rieur droit, cliquez sur le menu d\u00e9roulant pour cluster. Vous pourrez alors d\u00e9marrer le cluster ou le d\u00e9tacher et en attacher un autre. Partage de carnets dans Databricks Pour partager un carnet ou inviter d'autres collaborateurs, depuis le menu Espace de travail, faites un clic droit sur le fichier ou le dossier du carnet souhait\u00e9 et s\u00e9lectionnez l'option Autorisations . Vous pouvez \u00e9galement le faire \u00e0 partir d'un carnet, en cliquant sur le bouton Autorisations . Une fois que le carnet aura \u00e9t\u00e9 partag\u00e9, plusieurs auteurs pourront le consulter et le modifier simultan\u00e9ment. Note : Pour ajouter un utilisateur \u00e0 l'espace de travail Databricks, veuillez envoyer un message Slack . Ingestion de donn\u00e9es dans Databricks Les donn\u00e9es peuvent \u00eatre mont\u00e9es ou t\u00e9l\u00e9vers\u00e9es dans le Syst\u00e8me de fichiers Databricks (DBFS), qui est un espace de stockage propre \u00e0 l'espace de travail Databricks. Vous pouvez lire des donn\u00e9es d'une source de donn\u00e9es ou m\u00eame t\u00e9l\u00e9verser un fichier de donn\u00e9es (p. ex. CSV) directement dans le DBFS. Note : Le conteneur interne de lac de donn\u00e9es pour votre environnement a d\u00e9j\u00e0 \u00e9t\u00e9 mont\u00e9 pour vous, et vous pouvez travailler directement avec le conteneur. Si vous ne connaissez pas le nom de votre conteneur de lac de donn\u00e9es, veuillez envoyer un message Slack . Ajout de donn\u00e9es dans Databricks Lecture de fichiers mont\u00e9s Exemple : %python testData = spark . read . format ( ' csv ' ). options ( header = ' true ' , inferSchema = ' true ' ). load ( ' / mnt / mad - du / incoming / age - single - years -2018 - census - csv . csv ' ) display ( testData ) Modification du langage par d\u00e9faut dans un carnet Utilisation de plusieurs langages dans un carnet Vous pouvez changer les param\u00e8tres de langage par d\u00e9faut en entrant la commande sp\u00e9ciale % au d\u00e9but d'une cellule. Les commandes sp\u00e9ciales prises en charge sont les suivantes : %python, %r, %scala et %sql. Note: Lorsque vous utilisez une commande sp\u00e9ciale Langage, celle-ci est distribu\u00e9e \u00e0 la valeur REPL dans le contexte d'ex\u00e9cution du carnet. Les variables d\u00e9finies dans un langage (et par cons\u00e9quent dans la REPL pour ce langage) ne sont pas offertes dans la valeur REPL d'un autre langage. Les valeurs REPL permettent de partager l'\u00e9tat uniquement par l'interm\u00e9diaire de ressources externes, telles que des fichiers dans le DBFS ou des objets stock\u00e9s. Les carnets prennent \u00e9galement en charge quelques commandes sp\u00e9ciales auxiliaires: Vous permet d'ex\u00e9cuter du code Shell dans votre carnet. Pour faire \u00e9chouer la cellule si la commande Shell a un \u00e9tat de sortie diff\u00e9rent de z\u00e9ro, ajoutez l'-e option. Cette commande s'ex\u00e9cute uniquement sur le pilote Apache Spark, et non sur les processus de travail. Pour ex\u00e9cuter une commande Shell sur tous les n\u0153uds, utilisez un script d'initialisation (script init). Vous permet d'utiliser des commandes de type syst\u00e8me de fichiers (dbutils). Vous permet d'inclure diff\u00e9rents types de documentation, notamment du texte, des images ainsi que des formules et des \u00e9quations math\u00e9matiques. D\u00e9marrage de clusters dans Databricks Cliquez sur la liste d\u00e9roulante de clusters. S\u00e9lectionnez un cluster \u00e0 partir de cette liste. Cliquez sur le bouton D\u00e9but pour d\u00e9marrer le cluster. Configuration de Databricks Connect sur une machine virtuelle Databricks connect permet l'acc\u00e8s \u00e0 un environnement Databricks sans avoir besoin de se connecter via le portail Azure ou l'IU Databricks. Il permet d'utiliser d'autres EDI pour travailler du code Databricks. Voici les \u00e9tapes pour installer et tester Databricks Connect sur votre machine virtuelle: Il y a un conflit entre Databricks Connect et l'installation Pyspark qui se trouve sur les images Data Science Virtual Machine. Par d\u00e9faut, cette installation de Pyspark se trouve dans C:\\dsvm\\tools\\spark-2.4.4-bin-hadoop2.7 . Veuillez supprimer ou d\u00e9placer ce dossier afin d'installer Databricks Connect. Avant d'installer Databricks Connect, cr\u00e9ez un environment conda. Pour ce faire, ouvrez une invite de commandes et execut\u00e9z les commandes suivantes: conda create --name dbconnect python=3.7 conda activate dbconnect type pip install -U databricks-connect==X.Y.* REMARQUE: Remplacez X et Y avec le num\u00e9ro de version de votre cluster Databricks. Vous pouvez trouvez cette valeur en ouvrant l'espace de travail Databricks du portail Azure. Cliquez sur Clusters dans le menu \u00e0 gauche, et notez la version Runtime pour votre cluster. Dans une invite de commandes, entrez databricks-connect configure , et entez les valeurs suivantes quand demand\u00e9es: H\u00f4te Databricks: https://canadacentral.azuredatabricks.net Jeton : le jeton d\u2019acc\u00e8s personnel g\u00e9n\u00e9r\u00e9 dans les param\u00e8tres utilisateur de votre espace de travail Databricks ID du cluster : la valeur indiqu\u00e9e sous Instance de calcul --> Options avanc\u00e9es--> \u00c9tiquettes dans votre espace de travail Databricks ID de l\u2019organisation : la partie de l\u2019URL de Databricks qui se trouve apr\u00e8s .net/?o= Port : conserver la valeur existante Changez la valeur de la variable d'environnement SPARK_HOME \u00e0 c:\\miniconda\\envs\\(conda env name))\\lib\\site-packages\\pyspark , et red\u00e9marrez votre machine virtuelle. (Veuillez demander de l'aide via un message Slack si vous ne savez pas comment changer des variables d'environnement.) Testez la connectivit\u00e9 avec Azure Databricks en ex\u00e9cutant databricks-connect test dans une invite de commandes. Si votre cluster Databricks est arr\u00eat\u00e9 quand vous commencez ce test, vous recevrez des messages d'avertissement jusqu'\u00e0 ce qu'il ait d\u00e9marr\u00e9, ce qui peut prendre du temps. Troubleshooting : 1- Si vous utilisez databricks connect sur windows vous pouvez avoir cette erreur : Cannot find winutils.exe , Dans ce cas referez-vous a https://docs.microsoft.com/en-us/azure/databricks/dev-toolsdatabricks-connect#cannot-find-winutilsexe-on-windows Installation de librairies Cluster Databricks Veuillez contacter le canal Slack pour que l'\u00e9quipe d'assistance puisse installer les librairies pour vous. Carnet Veuillez utiliser les commandes suivantes pour installer une librairie dans une session de carnet. Python: dbutils . library . installPyPI ( \"pypipackage\" , version = \"version\" , repo = \"repo\" , extras = \"extras\" ) dbutils . library . restartPython () # Supprime l'\u00e9tat Python, mais certaines biblioth\u00e8ques peuvent ne pas fonctionner sans appeler cette fonction R Code: install.packages ( \"library\" ) Documentation Microsoft Acc\u00e9der \u00e0 Databricks pour la premi\u00e8re fois En savoir plus sur Databricks (en anglais seulement) Databricks Connect (en anglais seulement) Installer des biblioth\u00e8ques dans la session active d'un carnet Gestion des biblioth\u00e8ques pour les administrateurs","title":"Azure Databricks"},{"location":"DataBricks/#acces-a-databricks","text":"","title":"Acc\u00e8s \u00e0 Databricks"},{"location":"DataBricks/#tableau-de-bord","text":"Consultez la section Tableau de bord de cette documentation pour obtenir de plus amples renseignements. Cliquez sur le menu Tableau de bord dans le portail Azure.","title":"Tableau de bord"},{"location":"DataBricks/#adresse-url-de-databricks","text":"Rendez-vous au site https://canadacentral.azuredatabricks.net/ , ouvrez une session en utilisant vos justificatifs d'identit\u00e9 du compte infonuagique et s\u00e9lectionnez l'espace de travail Databricks qui a \u00e9t\u00e9 cr\u00e9\u00e9 pour vous.","title":"Adresse\u00a0URL de Databricks"},{"location":"DataBricks/#portail-azure","text":"Dans la bo\u00eete de recherche du portail Azure, recherchez Databricks . Vous devriez alors voir une liste des espaces de travail Databricks auxquels vous pouvez acc\u00e9der.","title":"Portail\u00a0Azure"},{"location":"DataBricks/#commencer","text":"Une fois dans Databricks, vous pouvez cr\u00e9er un carnet ou ouvrir un carnet existant. Pour plus d'information, veuillez consulter le processus \u00e0 suivre pour acc\u00e9der \u00e0 Databricks pour la premi\u00e8re fois .","title":"Commencer"},{"location":"DataBricks/#creation-dun-cluster","text":"Si aucun cluster n'a \u00e9t\u00e9 cr\u00e9\u00e9 pour vous ou si vous avez besoin d'apporter des changements \u00e0 votre cluster, veuillez envoyer un message sur Slack , puisque vous n'avez pas la permission de cr\u00e9er un cluster. Note : Vous devez avoir un cluster en mode actif avant de pouvoir ex\u00e9cuter le code dans votre carnet. Pour obtenir des renseignements sur la fa\u00e7on de d\u00e9marrer un cluster, consultez la section ci-dessous ou la FAQ .","title":"Cr\u00e9ation d'un cluster"},{"location":"DataBricks/#creation-dun-carnet","text":"Une fa\u00e7on de cr\u00e9er un carnet consiste \u00e0 cliquer sur l'option Nouveau carnet depuis la page principale de Databricks. Vous pouvez ensuite nommer votre carnet et choisir le langage par d\u00e9faut. Dans le champ \u00ab Grappe \u00bb, s\u00e9lectionnez parmis la liste propos\u00e9e le cluster auquel vous souhaitez attacher votre carnet. Pour d\u00e9marrer un cluster ou le modifier \u00e0 partir d'un carnet, ouvrez le carnet et, dans le coin sup\u00e9rieur droit, cliquez sur le menu d\u00e9roulant pour cluster. Vous pourrez alors d\u00e9marrer le cluster ou le d\u00e9tacher et en attacher un autre.","title":"Cr\u00e9ation d'un carnet"},{"location":"DataBricks/#partage-de-carnets-dans-databricks","text":"Pour partager un carnet ou inviter d'autres collaborateurs, depuis le menu Espace de travail, faites un clic droit sur le fichier ou le dossier du carnet souhait\u00e9 et s\u00e9lectionnez l'option Autorisations . Vous pouvez \u00e9galement le faire \u00e0 partir d'un carnet, en cliquant sur le bouton Autorisations . Une fois que le carnet aura \u00e9t\u00e9 partag\u00e9, plusieurs auteurs pourront le consulter et le modifier simultan\u00e9ment. Note : Pour ajouter un utilisateur \u00e0 l'espace de travail Databricks, veuillez envoyer un message Slack .","title":"Partage de carnets dans Databricks"},{"location":"DataBricks/#ingestion-de-donnees-dans-databricks","text":"Les donn\u00e9es peuvent \u00eatre mont\u00e9es ou t\u00e9l\u00e9vers\u00e9es dans le Syst\u00e8me de fichiers Databricks (DBFS), qui est un espace de stockage propre \u00e0 l'espace de travail Databricks. Vous pouvez lire des donn\u00e9es d'une source de donn\u00e9es ou m\u00eame t\u00e9l\u00e9verser un fichier de donn\u00e9es (p. ex. CSV) directement dans le DBFS. Note : Le conteneur interne de lac de donn\u00e9es pour votre environnement a d\u00e9j\u00e0 \u00e9t\u00e9 mont\u00e9 pour vous, et vous pouvez travailler directement avec le conteneur. Si vous ne connaissez pas le nom de votre conteneur de lac de donn\u00e9es, veuillez envoyer un message Slack .","title":"Ingestion de donn\u00e9es dans Databricks"},{"location":"DataBricks/#ajout-de-donnees-dans-databricks","text":"","title":"Ajout de donn\u00e9es dans Databricks"},{"location":"DataBricks/#lecture-de-fichiers-montes","text":"Exemple : %python testData = spark . read . format ( ' csv ' ). options ( header = ' true ' , inferSchema = ' true ' ). load ( ' / mnt / mad - du / incoming / age - single - years -2018 - census - csv . csv ' ) display ( testData )","title":"Lecture de fichiers mont\u00e9s"},{"location":"DataBricks/#modification-du-langage-par-defaut-dans-un-carnet","text":"","title":"Modification du langage par d\u00e9faut dans un carnet"},{"location":"DataBricks/#utilisation-de-plusieurs-langages-dans-un-carnet","text":"Vous pouvez changer les param\u00e8tres de langage par d\u00e9faut en entrant la commande sp\u00e9ciale % au d\u00e9but d'une cellule. Les commandes sp\u00e9ciales prises en charge sont les suivantes : %python, %r, %scala et %sql. Note: Lorsque vous utilisez une commande sp\u00e9ciale Langage, celle-ci est distribu\u00e9e \u00e0 la valeur REPL dans le contexte d'ex\u00e9cution du carnet. Les variables d\u00e9finies dans un langage (et par cons\u00e9quent dans la REPL pour ce langage) ne sont pas offertes dans la valeur REPL d'un autre langage. Les valeurs REPL permettent de partager l'\u00e9tat uniquement par l'interm\u00e9diaire de ressources externes, telles que des fichiers dans le DBFS ou des objets stock\u00e9s. Les carnets prennent \u00e9galement en charge quelques commandes sp\u00e9ciales auxiliaires: Vous permet d'ex\u00e9cuter du code Shell dans votre carnet. Pour faire \u00e9chouer la cellule si la commande Shell a un \u00e9tat de sortie diff\u00e9rent de z\u00e9ro, ajoutez l'-e option. Cette commande s'ex\u00e9cute uniquement sur le pilote Apache Spark, et non sur les processus de travail. Pour ex\u00e9cuter une commande Shell sur tous les n\u0153uds, utilisez un script d'initialisation (script init). Vous permet d'utiliser des commandes de type syst\u00e8me de fichiers (dbutils). Vous permet d'inclure diff\u00e9rents types de documentation, notamment du texte, des images ainsi que des formules et des \u00e9quations math\u00e9matiques.","title":"Utilisation de plusieurs langages dans un carnet"},{"location":"DataBricks/#demarrage-de-clusters-dans-databricks","text":"Cliquez sur la liste d\u00e9roulante de clusters. S\u00e9lectionnez un cluster \u00e0 partir de cette liste. Cliquez sur le bouton D\u00e9but pour d\u00e9marrer le cluster.","title":"D\u00e9marrage de clusters dans Databricks"},{"location":"DataBricks/#configuration-de-databricks-connect-sur-une-machine-virtuelle","text":"Databricks connect permet l'acc\u00e8s \u00e0 un environnement Databricks sans avoir besoin de se connecter via le portail Azure ou l'IU Databricks. Il permet d'utiliser d'autres EDI pour travailler du code Databricks. Voici les \u00e9tapes pour installer et tester Databricks Connect sur votre machine virtuelle: Il y a un conflit entre Databricks Connect et l'installation Pyspark qui se trouve sur les images Data Science Virtual Machine. Par d\u00e9faut, cette installation de Pyspark se trouve dans C:\\dsvm\\tools\\spark-2.4.4-bin-hadoop2.7 . Veuillez supprimer ou d\u00e9placer ce dossier afin d'installer Databricks Connect. Avant d'installer Databricks Connect, cr\u00e9ez un environment conda. Pour ce faire, ouvrez une invite de commandes et execut\u00e9z les commandes suivantes: conda create --name dbconnect python=3.7 conda activate dbconnect type pip install -U databricks-connect==X.Y.* REMARQUE: Remplacez X et Y avec le num\u00e9ro de version de votre cluster Databricks. Vous pouvez trouvez cette valeur en ouvrant l'espace de travail Databricks du portail Azure. Cliquez sur Clusters dans le menu \u00e0 gauche, et notez la version Runtime pour votre cluster. Dans une invite de commandes, entrez databricks-connect configure , et entez les valeurs suivantes quand demand\u00e9es: H\u00f4te Databricks: https://canadacentral.azuredatabricks.net Jeton : le jeton d\u2019acc\u00e8s personnel g\u00e9n\u00e9r\u00e9 dans les param\u00e8tres utilisateur de votre espace de travail Databricks ID du cluster : la valeur indiqu\u00e9e sous Instance de calcul --> Options avanc\u00e9es--> \u00c9tiquettes dans votre espace de travail Databricks ID de l\u2019organisation : la partie de l\u2019URL de Databricks qui se trouve apr\u00e8s .net/?o= Port : conserver la valeur existante Changez la valeur de la variable d'environnement SPARK_HOME \u00e0 c:\\miniconda\\envs\\(conda env name))\\lib\\site-packages\\pyspark , et red\u00e9marrez votre machine virtuelle. (Veuillez demander de l'aide via un message Slack si vous ne savez pas comment changer des variables d'environnement.) Testez la connectivit\u00e9 avec Azure Databricks en ex\u00e9cutant databricks-connect test dans une invite de commandes. Si votre cluster Databricks est arr\u00eat\u00e9 quand vous commencez ce test, vous recevrez des messages d'avertissement jusqu'\u00e0 ce qu'il ait d\u00e9marr\u00e9, ce qui peut prendre du temps.","title":"Configuration de Databricks Connect sur une machine virtuelle"},{"location":"DataBricks/#troubleshooting","text":"1- Si vous utilisez databricks connect sur windows vous pouvez avoir cette erreur : Cannot find winutils.exe , Dans ce cas referez-vous a https://docs.microsoft.com/en-us/azure/databricks/dev-toolsdatabricks-connect#cannot-find-winutilsexe-on-windows","title":"Troubleshooting :"},{"location":"DataBricks/#installation-de-librairies","text":"","title":"Installation de librairies"},{"location":"DataBricks/#cluster-databricks","text":"Veuillez contacter le canal Slack pour que l'\u00e9quipe d'assistance puisse installer les librairies pour vous.","title":"Cluster Databricks"},{"location":"DataBricks/#carnet","text":"Veuillez utiliser les commandes suivantes pour installer une librairie dans une session de carnet. Python: dbutils . library . installPyPI ( \"pypipackage\" , version = \"version\" , repo = \"repo\" , extras = \"extras\" ) dbutils . library . restartPython () # Supprime l'\u00e9tat Python, mais certaines biblioth\u00e8ques peuvent ne pas fonctionner sans appeler cette fonction R Code: install.packages ( \"library\" )","title":"Carnet"},{"location":"DataBricks/#documentation-microsoft","text":"Acc\u00e9der \u00e0 Databricks pour la premi\u00e8re fois En savoir plus sur Databricks (en anglais seulement) Databricks Connect (en anglais seulement) Installer des biblioth\u00e8ques dans la session active d'un carnet Gestion des biblioth\u00e8ques pour les administrateurs","title":"Documentation Microsoft"},{"location":"DataFactory/","text":"Acc\u00e8s \u00e0 Data Factory Tableau de bord Consultez la section Tableau de bord de cette documentation pour obtenir de plus amples renseignements. Cliquez sur le menu Tableau de bord dans le portail Azure. ADRESSE URL D'ADF Rendez-vous \u00e0 https://adf.azure.com et s\u00e9lectionnez l'instance Data Factory qui a \u00e9t\u00e9 cr\u00e9\u00e9e pour vous. Portail Azure Dans la bo\u00eete de recherche du portail Azure, recherchez Data factories . Vous devriez alors voir la liste des Data Factories auxquelles vous avez obtenu la permission d'acc\u00e9der. Auteur Cliquez sur Author & Monitor . Dans Data Factory, vous avez la capacit\u00e9 de cr\u00e9er et de d\u00e9ployer des ressources. Voir le document Cr\u00e9ation visuelle dans Azure Data Factory pour obtenir de plus amples renseignements. Vous pouvez \u00e9galement utiliser certains des divers assistants fournis sur la page d' aper\u00e7u (Overview) de Data Factory . NOTE : La configuration de SSIS Integration n'est PAS recommand\u00e9e. Si vous avez des questions, communiquez avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack . Voir les tutoriels de la documentation Azure pour obtenir plus de d\u00e9tails. Acc\u00e8s \u00e0 Data Lake \u00e0 partir d'ADF Une connexion au compte de stockage Data Lake a \u00e9t\u00e9 pr\u00e9configur\u00e9e pour votre environnement. Cliquez sur l' ic\u00f4ne de la mallette . Cliquez sur Services li\u00e9s . Le service li\u00e9 au type Stockage Azure Data Lake Gen2 est votre compte de stockage Data Lake . Note : On vous a accord\u00e9 l'acc\u00e8s \u00e0 des contenants particuliers cr\u00e9\u00e9s dans le compte de stockage Data Lake pour votre environnement. Acc\u00e8s \u00e0 la Base de donn\u00e9es SQL Azure Certains projets ont une instance de Base de donn\u00e9es SQL Azure. Cliquez sur l' ic\u00f4ne de la mallette . Cliquez sur Services li\u00e9s . Les services li\u00e9s au type Base de donn\u00e9es SQL Azure sont vos bases de donn\u00e9es . Enregistrement ou publication de vos ressources dans Data Factory Azure Data Factory peut \u00eatre configur\u00e9 pour enregistrer votre travail dans les emplacements suivants: D\u00e9p\u00f4t Git Publier directement dans Data Factory Git (lorsqu'accessible) Lorsque Git est activ\u00e9, vous pouvez voir votre configuration et enregistrer votre travail dans une branche particuli\u00e8re. Cliquez sur l' ic\u00f4ne de la mallette . Cliquez sur Configuration de Git . V\u00e9rifiez la configuration de Git qui a \u00e9t\u00e9 mise en place pour vous. Lorsque vous cr\u00e9ez un flux de travail, vous pouvez l'enregistrer dans votre branche. Cliquez sur + Nouvelle branche dans le menu d\u00e9roulant des branches pour cr\u00e9er une branche de fonctionnalit\u00e9s. Lorsque vous serez pr\u00eat \u00e0 fusionner les changements de votre branche de fonctionnalit\u00e9s dans votre branche de collaboration (master), cliquez sur le menu d\u00e9roulant des branches et s\u00e9lectionnez Cr\u00e9er la demande de tirage (pull request) . Cette action vous dirigera vers Azure DevOps Git, o\u00f9 vous pourrez cr\u00e9er des demandes de tirage, proc\u00e9der \u00e0 des revues du code et fusionner les modifications dans votre branche de collaboration (master) d\u00e8s que la demande aura \u00e9t\u00e9 approuv\u00e9e. Apr\u00e8s avoir fusionn\u00e9 les modifications dans la branche de collaboration (master), cliquez sur Publier pour publier les changements de votre code de la branche dans Azure Data Factory. Si vous obtenez un message d'erreur au moment de la publication, communiquez avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack . Service Data Factory Lorsque Data Factory n'est pas int\u00e9gr\u00e9 au contr\u00f4le source, vos flux de travail sont stock\u00e9s directement dans le service Data Factory. Vous ne pouvez pas enregistrer les changements partiels, vous pouvez seulement utiliser l'option Tout publier , ce qui \u00e9crase l'\u00e9tat actuel de Data Factory avec vos changements qui deviennent alors visibles \u00e0 tous. Ingestion et transformation des donn\u00e9es avec ADF Assistant de copie de donn\u00e9es Mappage de flux de donn\u00e9es -- Processus ETC ax\u00e9 sur l'interface utilisateur graphique Runtimes d'int\u00e9gration AutoResolveIntegrationRuntime Ne pas utiliser. Veuillez utiliser les runtimes d'int\u00e9gration canadaCentralIR-4nodesDataFlow ou selfHostedCovidIaaSVnet au lieu. Le runtime d'int\u00e9gration de r\u00e9solution automatique (\"auto resolve\") est cr\u00e9\u00e9 par d\u00e9faut avec le Data Factory, et va s\u00e9lectionner le centre de donn\u00e9es Azure le plus pr\u00e8s des donn\u00e9es, ce qui pourrait contrevenir aux politiques de r\u00e9sidence des donn\u00e9es. canadaCentralIR-4nodesDataFlow Ceci est partag\u00e9 par tous les utilisateurs et fonctionne tout le temps. Peut acc\u00e9der: Lac de donn\u00e9es interne Compte de stockage externe Sources de donn\u00e9es externes (Internet) Ne peut pas acc\u00e9der: Base de donn\u00e9es Azure SQL selfHostedCovidIaaSVnet Situ\u00e9 \u00e0 l'int\u00e9rieur du r\u00e9seau virtuel interne. Peut acc\u00e9der: Lac de donn\u00e9es interne Base de donn\u00e9es Azure SQL Ne peut pas acc\u00e9der: Compte de stockage externe Sources de donn\u00e9es externes (Internet) Exemple : Comment se connecter aux donn\u00e9es de John Hopkins Il y a un exemple de flux de travail qui montre la fa\u00e7on d'ing\u00e9rer des donn\u00e9es \u00e0 partir de GitHub \u00e0 l'aide d'un pipeline de Data Factory. Les donn\u00e9es peuvent \u00eatre filtr\u00e9es depuis Data Factory. Il est aussi possible d'extraire des donn\u00e9es de GitHub au moyen d'un code dans un notebook de Databricks. Documentation Microsoft Pr\u00e9sentation d'Azure Data Factory -- Azure Data Factory Cr\u00e9er une fabrique de donn\u00e9es Azure \u00e0 l'aide de l'interface utilisateur d'Azure Data Factory -- Azure Data Factory Copier des donn\u00e9es avec l'outil d'Azure Copier des donn\u00e9es -- Azure Data Factory Cr\u00e9er un flux de donn\u00e9es de mappage -- Azure Data Factory Fonctions d'expression dans le flux de donn\u00e9es de mappage --Azure Data Factory Mode de d\u00e9bogage du mappage de flux de donn\u00e9es -- Azure Data Factory Supervision visuelle du flux de donn\u00e9es de mappage -- Azure Data Factory Vid\u00e9o YouTube (en anglais seulement) Ingest, prepare & transform using Azure Databricks & Data Factory Azure Friday Azure Friday Visually build pipelines for Azure Data Factory V2 How to prepare data using wrangling data flows in Azure Data Factory Azure Friday How to develop and debug with Azure Data Factory Azure Friday Building Data Flows in Azure Data Factory","title":"Azure Data Factory"},{"location":"DataFactory/#acces-a-data-factory","text":"","title":"Acc\u00e8s \u00e0 Data\u00a0Factory"},{"location":"DataFactory/#tableau-de-bord","text":"Consultez la section Tableau de bord de cette documentation pour obtenir de plus amples renseignements. Cliquez sur le menu Tableau de bord dans le portail Azure.","title":"Tableau de bord"},{"location":"DataFactory/#adresse-url-dadf","text":"Rendez-vous \u00e0 https://adf.azure.com et s\u00e9lectionnez l'instance Data Factory qui a \u00e9t\u00e9 cr\u00e9\u00e9e pour vous.","title":"ADRESSE\u00a0URL D'ADF"},{"location":"DataFactory/#portail-azure","text":"Dans la bo\u00eete de recherche du portail Azure, recherchez Data factories . Vous devriez alors voir la liste des Data Factories auxquelles vous avez obtenu la permission d'acc\u00e9der.","title":"Portail\u00a0Azure"},{"location":"DataFactory/#auteur","text":"Cliquez sur Author & Monitor . Dans Data Factory, vous avez la capacit\u00e9 de cr\u00e9er et de d\u00e9ployer des ressources. Voir le document Cr\u00e9ation visuelle dans Azure Data Factory pour obtenir de plus amples renseignements. Vous pouvez \u00e9galement utiliser certains des divers assistants fournis sur la page d' aper\u00e7u (Overview) de Data Factory . NOTE : La configuration de SSIS Integration n'est PAS recommand\u00e9e. Si vous avez des questions, communiquez avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack . Voir les tutoriels de la documentation Azure pour obtenir plus de d\u00e9tails.","title":"Auteur"},{"location":"DataFactory/#acces-a-data-lake-a-partir-dadf","text":"Une connexion au compte de stockage Data Lake a \u00e9t\u00e9 pr\u00e9configur\u00e9e pour votre environnement. Cliquez sur l' ic\u00f4ne de la mallette . Cliquez sur Services li\u00e9s . Le service li\u00e9 au type Stockage Azure Data Lake Gen2 est votre compte de stockage Data Lake . Note : On vous a accord\u00e9 l'acc\u00e8s \u00e0 des contenants particuliers cr\u00e9\u00e9s dans le compte de stockage Data Lake pour votre environnement.","title":"Acc\u00e8s \u00e0 Data\u00a0Lake \u00e0 partir d'ADF"},{"location":"DataFactory/#acces-a-la-base-de-donnees-sql-azure","text":"Certains projets ont une instance de Base de donn\u00e9es SQL Azure. Cliquez sur l' ic\u00f4ne de la mallette . Cliquez sur Services li\u00e9s . Les services li\u00e9s au type Base de donn\u00e9es SQL Azure sont vos bases de donn\u00e9es .","title":"Acc\u00e8s \u00e0 la Base de donn\u00e9es SQL\u00a0Azure"},{"location":"DataFactory/#enregistrement-ou-publication-de-vos-ressources-dans-data-factory","text":"Azure Data Factory peut \u00eatre configur\u00e9 pour enregistrer votre travail dans les emplacements suivants: D\u00e9p\u00f4t Git Publier directement dans Data Factory","title":"Enregistrement ou publication de vos ressources dans Data\u00a0Factory"},{"location":"DataFactory/#git-lorsquaccessible","text":"Lorsque Git est activ\u00e9, vous pouvez voir votre configuration et enregistrer votre travail dans une branche particuli\u00e8re. Cliquez sur l' ic\u00f4ne de la mallette . Cliquez sur Configuration de Git . V\u00e9rifiez la configuration de Git qui a \u00e9t\u00e9 mise en place pour vous. Lorsque vous cr\u00e9ez un flux de travail, vous pouvez l'enregistrer dans votre branche. Cliquez sur + Nouvelle branche dans le menu d\u00e9roulant des branches pour cr\u00e9er une branche de fonctionnalit\u00e9s. Lorsque vous serez pr\u00eat \u00e0 fusionner les changements de votre branche de fonctionnalit\u00e9s dans votre branche de collaboration (master), cliquez sur le menu d\u00e9roulant des branches et s\u00e9lectionnez Cr\u00e9er la demande de tirage (pull request) . Cette action vous dirigera vers Azure DevOps Git, o\u00f9 vous pourrez cr\u00e9er des demandes de tirage, proc\u00e9der \u00e0 des revues du code et fusionner les modifications dans votre branche de collaboration (master) d\u00e8s que la demande aura \u00e9t\u00e9 approuv\u00e9e. Apr\u00e8s avoir fusionn\u00e9 les modifications dans la branche de collaboration (master), cliquez sur Publier pour publier les changements de votre code de la branche dans Azure Data Factory. Si vous obtenez un message d'erreur au moment de la publication, communiquez avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack .","title":"Git (lorsqu'accessible)"},{"location":"DataFactory/#service-data-factory","text":"Lorsque Data Factory n'est pas int\u00e9gr\u00e9 au contr\u00f4le source, vos flux de travail sont stock\u00e9s directement dans le service Data Factory. Vous ne pouvez pas enregistrer les changements partiels, vous pouvez seulement utiliser l'option Tout publier , ce qui \u00e9crase l'\u00e9tat actuel de Data Factory avec vos changements qui deviennent alors visibles \u00e0 tous.","title":"Service Data\u00a0Factory"},{"location":"DataFactory/#ingestion-et-transformation-des-donnees-avec-adf","text":"Assistant de copie de donn\u00e9es Mappage de flux de donn\u00e9es -- Processus ETC ax\u00e9 sur l'interface utilisateur graphique","title":"Ingestion et transformation des donn\u00e9es avec ADF"},{"location":"DataFactory/#runtimes-dintegration","text":"","title":"Runtimes d'int\u00e9gration"},{"location":"DataFactory/#autoresolveintegrationruntime","text":"Ne pas utiliser. Veuillez utiliser les runtimes d'int\u00e9gration canadaCentralIR-4nodesDataFlow ou selfHostedCovidIaaSVnet au lieu. Le runtime d'int\u00e9gration de r\u00e9solution automatique (\"auto resolve\") est cr\u00e9\u00e9 par d\u00e9faut avec le Data Factory, et va s\u00e9lectionner le centre de donn\u00e9es Azure le plus pr\u00e8s des donn\u00e9es, ce qui pourrait contrevenir aux politiques de r\u00e9sidence des donn\u00e9es.","title":"AutoResolveIntegrationRuntime"},{"location":"DataFactory/#canadacentralir-4nodesdataflow","text":"Ceci est partag\u00e9 par tous les utilisateurs et fonctionne tout le temps.","title":"canadaCentralIR-4nodesDataFlow"},{"location":"DataFactory/#peut-acceder","text":"Lac de donn\u00e9es interne Compte de stockage externe Sources de donn\u00e9es externes (Internet)","title":"Peut acc\u00e9der:"},{"location":"DataFactory/#ne-peut-pas-acceder","text":"Base de donn\u00e9es Azure SQL","title":"Ne peut pas acc\u00e9der:"},{"location":"DataFactory/#selfhostedcovidiaasvnet","text":"Situ\u00e9 \u00e0 l'int\u00e9rieur du r\u00e9seau virtuel interne.","title":"selfHostedCovidIaaSVnet"},{"location":"DataFactory/#peut-acceder_1","text":"Lac de donn\u00e9es interne Base de donn\u00e9es Azure SQL","title":"Peut acc\u00e9der:"},{"location":"DataFactory/#ne-peut-pas-acceder_1","text":"Compte de stockage externe Sources de donn\u00e9es externes (Internet)","title":"Ne peut pas acc\u00e9der:"},{"location":"DataFactory/#exemple-comment-se-connecter-aux-donnees-de-john-hopkins","text":"Il y a un exemple de flux de travail qui montre la fa\u00e7on d'ing\u00e9rer des donn\u00e9es \u00e0 partir de GitHub \u00e0 l'aide d'un pipeline de Data Factory. Les donn\u00e9es peuvent \u00eatre filtr\u00e9es depuis Data Factory. Il est aussi possible d'extraire des donn\u00e9es de GitHub au moyen d'un code dans un notebook de Databricks.","title":"Exemple\u00a0: Comment se connecter aux donn\u00e9es de John\u00a0Hopkins"},{"location":"DataFactory/#documentation-microsoft","text":"Pr\u00e9sentation d'Azure Data Factory -- Azure Data Factory Cr\u00e9er une fabrique de donn\u00e9es Azure \u00e0 l'aide de l'interface utilisateur d'Azure Data Factory -- Azure Data Factory Copier des donn\u00e9es avec l'outil d'Azure Copier des donn\u00e9es -- Azure Data Factory Cr\u00e9er un flux de donn\u00e9es de mappage -- Azure Data Factory Fonctions d'expression dans le flux de donn\u00e9es de mappage --Azure Data Factory Mode de d\u00e9bogage du mappage de flux de donn\u00e9es -- Azure Data Factory Supervision visuelle du flux de donn\u00e9es de mappage -- Azure Data Factory","title":"Documentation Microsoft"},{"location":"DataFactory/#video-youtube-en-anglais-seulement","text":"Ingest, prepare & transform using Azure Databricks & Data Factory Azure Friday Azure Friday Visually build pipelines for Azure Data Factory V2 How to prepare data using wrangling data flows in Azure Data Factory Azure Friday How to develop and debug with Azure Data Factory Azure Friday Building Data Flows in Azure Data Factory","title":"Vid\u00e9o\u00a0YouTube (en anglais seulement)"},{"location":"DeltaLake/","text":"Delta Lake est une couche de stockage \u00e0 code source ouvert qui s\u2019ex\u00e9cute au-dessus d\u2019un lac de donn\u00e9es existant, ajoutant les capacit\u00e9s des propri\u00e9t\u00e9s et des transactions ACID (atomicit\u00e9, coh\u00e9rence, isolation, durabilit\u00e9). Delta Lake est enti\u00e8rement compatible avec Apache Spark dans Azure Databricks et Azure Synapse. Azure Data Lake n\u2019est pas conforme \u00e0 la norme ACID. Il convient donc d\u2019utiliser Delta Lake lorsque l\u2019int\u00e9grit\u00e9 et la fiabilit\u00e9 des donn\u00e9es sont essentielles, ou lorsqu\u2019il existe un risque de mauvaises donn\u00e9es. Documentation Microsoft Pr\u00e9sentation de Delta Lake Delta Lake sur Azure Documents Officielle Documentation Delta Lake Comment fonctionne le Delta Lake Un lac delta est essentiellement un dossier \u00e0 l'int\u00e9rieur du lac de donn\u00e9es contenant des fichiers journaux (dans le sous-dossier _delta_log ) et des fichiers de donn\u00e9es (stock\u00e9s au format parquet dans le dossier racine) pour chaque version d'un tableau. Tant que les fichiers journaux et de donn\u00e9es existent, vous pouvez utiliser la fonction de voyage dans le temps pour interroger les versions pr\u00e9c\u00e9dentes d'une table delta et afficher l'historique de cette table. Si les fichiers journaux sont supprim\u00e9s , vous ne pourrez pas du tout lire le tableau. Pour r\u00e9soudre ce probl\u00e8me, vous devrez vider le dossier du Delta Lake (supprimer tout ce qu'il contient), puis y \u00e9crire votre fichier de donn\u00e9es d'origine pour recommencer. Delta fonctionne au niveau des tables, ce qui signifie que les requ\u00eates et les jointures sur plusieurs tables ne sont pas prises en charge. Quand utiliser Delta Lake Delta Lake est pr\u00e9f\u00e9rable: Pour toute table permanente dans Databricks; Pour les grandes quantit\u00e9s de donn\u00e9es semi-structur\u00e9es (10 millions d\u2019enregistrements ou plus pour obtenir les meilleurs avantages sur le plan de la performance); Lorsque vous souhaitez un contr\u00f4le de version ou un suivi de l\u2019acc\u00e8s aux donn\u00e9es (les fichiers journaux delta permettent de savoir chaque fois que les donn\u00e9es sont modifi\u00e9es et par qui). D\u00e9placement dans le temps Vous pouvez utiliser le d\u00e9placement dans le temps pour interroger un ancien instantan\u00e9 d\u2019une table, soit par num\u00e9ro de version, soit par horodatage. Par d\u00e9faut, les fichiers de donn\u00e9es sont conserv\u00e9s pendant 30 jours. Exemple: SQL SELECT * FROM example_table TIMESTAMP AS OF '2018-10-18T22:15:12.013Z' SELECT * FROM delta.`/delta/example_table` VERSION AS OF 12 Python df1 = spark . read . format ( \"delta\" ) . option ( \"timestampAsOf\" , 2020 - 03 - 13 ) . load ( \"/delta/example_table\" ) df2 = spark . read . format ( \"delta\" ) . option ( \"timestampAsOf\" , 2019 - 01 - 01 T00 : 00 : 00.000 Z ) . load ( \"/delta/example_table\" ) df3 = spark . read . format ( \"delta\" ) . option ( \"versionAsOf\" , version ) . load ( \"/delta/example_table\" ) Suppression des anciens fichiers de donn\u00e9es Pour supprimer les anciens fichiers de donn\u00e9es (pas les fichiers journaux) qui ne sont plus r\u00e9f\u00e9renc\u00e9s par une table delta, vous pouvez ex\u00e9cuter la commande vacuum . Exemple: SQL VACUUM example_table -- fichiers vacuum non requis par les versions ant\u00e9rieures \u00e0 la p\u00e9riode de r\u00e9tention par d\u00e9faut VACUUM '/data/example_table' -- fichiers vides dans une table bas\u00e9e sur les chemins VACUUM delta.`/data/example_table/` VACUUM delta.`/data/example_table/` RETAIN 100 HOURS -- fichiers vides non requis par les versions de plus de 100 heures VACUUM example_table DRY RUN -- faire un essai \u00e0 sec pour obtenir la liste des fichiers \u00e0 supprimer Python from delta.tables import * deltaTable = DeltaTable . forPath ( spark , pathToTable ) deltaTable . vacuum () # fichiers vacuum non requis par les versions ant\u00e9rieures \u00e0 la p\u00e9riode de r\u00e9tention par d\u00e9faut deltaTable . vacuum ( 100 ) # fichiers vides non requis par les versions de plus de 100 heures Scala import io.delta.tables._ val deltaTable = DeltaTable . forPath ( spark , pathToTable ) deltaTable . vacuum () // fichiers vacuum non requis par les versions ant\u00e9rieures \u00e0 la p\u00e9riode de r\u00e9tention par d\u00e9faut deltaTable . vacuum ( 100 ) // fichiers vides non requis par les versions de plus de 100 heures Revenir \u00e0 Une Version Ant\u00e9rieure Vous pouvez revenir \u00e0 une version pr\u00e9c\u00e9dente de votre table et y travailler en utilisant la fonction de voyage dans le temps pour lire votre version cible en tant que trame de donn\u00e9es, puis la r\u00e9\u00e9crire dans le dossier delta lake. Cela cr\u00e9era une nouvelle version identique \u00e0 la version cible, \u00e0 partir de laquelle vous pourrez ensuite travailler. Les autres versions pr\u00e9c\u00e9dentes restent intactes. Exemple: Python # lire dans l'ancienne version du tableau df = spark . read . format ( \"delta\" ) . option ( \"versionAsOf\" , 0 ) . load ( delta_table_path ) # r\u00e9\u00e9crire dans la nouvelle version de la table, doit d\u00e9finir le mode sur \"overwrite\" df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( delta_table_path ) Documents officiels Interroger un ancien instantan\u00e9 d'une table (voyage dans le temps) Supprimer les fichiers qui ne sont plus r\u00e9f\u00e9renc\u00e9s par une table delta Utilisation de Delta Lake dans Databricks Databricks prend en charge en natif Delta Lake et peut ex\u00e9cuter des requ\u00eates \u00e0 l'aide de Python, R, Scala et SQL. Vous devez d'abord cr\u00e9er un r\u00e9pertoire pour stocker les fichiers delta et noter le chemin d'acc\u00e8s \u00e0 ce r\u00e9pertoire. Lisez votre fichier de donn\u00e9es, puis \u00e9crivez-le au format \"delta\" et enregistrez-le dans le r\u00e9pertoire cr\u00e9\u00e9 ci-dessus. ``` # lire le fichier de donn\u00e9es testData = spark.read.format('json').options(header='true', inferSchema='true', multiline='true').load('/mnt/public-data/incoming/covid_tracking.json') \u00e9crire au format delta testData.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/public-data/delta\") ``` Facultatif ( pas une bonne pratique ) : cr\u00e9ez une table SQL \u00e0 l'aide de delta: spark.sql(\"CREATE TABLE sample_table USING DELTA LOCATION '/mnt/public-data/delta/'\") Vous pouvez maintenant ex\u00e9cuter des requ\u00eates SQL sur votre table delta, y compris des requ\u00eates par num\u00e9ro de version ou horodatage pour \"voyager dans le temps\" vers les versions pr\u00e9c\u00e9dentes de vos donn\u00e9es. Si vous avez cr\u00e9\u00e9 une table \u00e0 l'\u00e9tape 3 , vous pouvez ex\u00e9cuter des requ\u00eates en utilisant le nom de la table. Sinon (meilleure pratique) , \u00e0 la place du nom de la table, vous pouvez utiliser delta.`{delta_table_path}` (remplacez {delta_table_path} par le chemin r\u00e9el). ``` %sql SELECT * FROM sample_table VERSION AS OF 0 SELECT * FROM delta. /mnt/public-data/delta/ ``` Documentation Microsoft D\u00e9marrage rapide du Delta Lake Utilisation de Delta dans Azure Synapse Delta Lake est compatible avec Azure Synapse. Les tables delta peuvent \u00eatre cr\u00e9\u00e9es et interrog\u00e9es dans les blocs-notes Synapse de la m\u00eame mani\u00e8re que Databricks, avec la prise en charge du langage pour PySpark, Scala et .NET (C#). Notez que SQL n'est pas pris en charge avec la version actuelle. Lisez votre fichier de donn\u00e9es. data = spark.read.format('csv').options(header='true', inferSchema='true', multiline='true').load('abfss://public-data@statsconviddsinternal.dfs.core.windows.net/incoming/data_duplicate.csv') \u00c9crivez au format delta et enregistrez dans votre r\u00e9pertoire de table delta. data.write.format(\"delta\").save(delta_table_path) Facultatif : cr\u00e9ez une table SQL \u00e0 l'aide de delta (requis uniquement si vous souhaitez ex\u00e9cuter des requ\u00eates SQL, pas n\u00e9cessaire si vous utilisez uniquement Python, Scala ou C# ). spark.sql(\"CREATE TABLE example USING DELTA LOCATION '{0}'\".format(delta_table_path)) Vous pouvez maintenant ex\u00e9cuter des requ\u00eates sur vos donn\u00e9es. Documentation Microsoft Travailler avec Delta Lake Utilisation de Delta Lake dans Data Factory Vous pouvez utiliser Azure Data Factory pour copier des donn\u00e9es vers et depuis un Delta Lake stock\u00e9 dans Azure Data Lake. Exemple : copier des donn\u00e9es dans Delta Lake Cr\u00e9ez un nouveau flux de donn\u00e9es et ajoutez une source. Sous l'onglet Param\u00e8tres source , ajoutez l'ensemble de donn\u00e9es \u00e0 partir duquel vous souhaitez copier. Configurez tous les autres param\u00e8tres pertinents. Cliquez sur le bouton plus \u00e0 droite de votre source et ajoutez un r\u00e9cepteur. Sous l'onglet R\u00e9cepteur , choisissez En ligne comme type de r\u00e9cepteur et Delta comme type de jeu de donn\u00e9es en ligne. Sous l'onglet Param\u00e8tres , d\u00e9finissez le chemin du dossier (le chemin vers lequel vos fichiers delta seront stock\u00e9s). Documentation Microsoft Format Delta dans Azure Data Factory Utilisation de Delta avec Power BI Pour lire les tables delta de mani\u00e8re native dans Power BI, veuillez consulter cette documentation sur GitHub . Delta dans Azure Machine Learning Le Delta Lake n'est actuellement pas pris en charge dans Azure ML.","title":"Delta Lake"},{"location":"DeltaLake/#documentation-microsoft","text":"Pr\u00e9sentation de Delta Lake Delta Lake sur Azure","title":"Documentation Microsoft"},{"location":"DeltaLake/#documents-officielle","text":"Documentation Delta Lake","title":"Documents Officielle"},{"location":"DeltaLake/#comment-fonctionne-le-delta-lake","text":"Un lac delta est essentiellement un dossier \u00e0 l'int\u00e9rieur du lac de donn\u00e9es contenant des fichiers journaux (dans le sous-dossier _delta_log ) et des fichiers de donn\u00e9es (stock\u00e9s au format parquet dans le dossier racine) pour chaque version d'un tableau. Tant que les fichiers journaux et de donn\u00e9es existent, vous pouvez utiliser la fonction de voyage dans le temps pour interroger les versions pr\u00e9c\u00e9dentes d'une table delta et afficher l'historique de cette table. Si les fichiers journaux sont supprim\u00e9s , vous ne pourrez pas du tout lire le tableau. Pour r\u00e9soudre ce probl\u00e8me, vous devrez vider le dossier du Delta Lake (supprimer tout ce qu'il contient), puis y \u00e9crire votre fichier de donn\u00e9es d'origine pour recommencer. Delta fonctionne au niveau des tables, ce qui signifie que les requ\u00eates et les jointures sur plusieurs tables ne sont pas prises en charge.","title":"Comment fonctionne le Delta Lake"},{"location":"DeltaLake/#quand-utiliser-delta-lake","text":"Delta Lake est pr\u00e9f\u00e9rable: Pour toute table permanente dans Databricks; Pour les grandes quantit\u00e9s de donn\u00e9es semi-structur\u00e9es (10 millions d\u2019enregistrements ou plus pour obtenir les meilleurs avantages sur le plan de la performance); Lorsque vous souhaitez un contr\u00f4le de version ou un suivi de l\u2019acc\u00e8s aux donn\u00e9es (les fichiers journaux delta permettent de savoir chaque fois que les donn\u00e9es sont modifi\u00e9es et par qui).","title":"Quand utiliser Delta Lake"},{"location":"DeltaLake/#deplacement-dans-le-temps","text":"Vous pouvez utiliser le d\u00e9placement dans le temps pour interroger un ancien instantan\u00e9 d\u2019une table, soit par num\u00e9ro de version, soit par horodatage. Par d\u00e9faut, les fichiers de donn\u00e9es sont conserv\u00e9s pendant 30 jours. Exemple: SQL SELECT * FROM example_table TIMESTAMP AS OF '2018-10-18T22:15:12.013Z' SELECT * FROM delta.`/delta/example_table` VERSION AS OF 12 Python df1 = spark . read . format ( \"delta\" ) . option ( \"timestampAsOf\" , 2020 - 03 - 13 ) . load ( \"/delta/example_table\" ) df2 = spark . read . format ( \"delta\" ) . option ( \"timestampAsOf\" , 2019 - 01 - 01 T00 : 00 : 00.000 Z ) . load ( \"/delta/example_table\" ) df3 = spark . read . format ( \"delta\" ) . option ( \"versionAsOf\" , version ) . load ( \"/delta/example_table\" )","title":"D\u00e9placement dans le temps"},{"location":"DeltaLake/#suppression-des-anciens-fichiers-de-donnees","text":"Pour supprimer les anciens fichiers de donn\u00e9es (pas les fichiers journaux) qui ne sont plus r\u00e9f\u00e9renc\u00e9s par une table delta, vous pouvez ex\u00e9cuter la commande vacuum . Exemple: SQL VACUUM example_table -- fichiers vacuum non requis par les versions ant\u00e9rieures \u00e0 la p\u00e9riode de r\u00e9tention par d\u00e9faut VACUUM '/data/example_table' -- fichiers vides dans une table bas\u00e9e sur les chemins VACUUM delta.`/data/example_table/` VACUUM delta.`/data/example_table/` RETAIN 100 HOURS -- fichiers vides non requis par les versions de plus de 100 heures VACUUM example_table DRY RUN -- faire un essai \u00e0 sec pour obtenir la liste des fichiers \u00e0 supprimer Python from delta.tables import * deltaTable = DeltaTable . forPath ( spark , pathToTable ) deltaTable . vacuum () # fichiers vacuum non requis par les versions ant\u00e9rieures \u00e0 la p\u00e9riode de r\u00e9tention par d\u00e9faut deltaTable . vacuum ( 100 ) # fichiers vides non requis par les versions de plus de 100 heures Scala import io.delta.tables._ val deltaTable = DeltaTable . forPath ( spark , pathToTable ) deltaTable . vacuum () // fichiers vacuum non requis par les versions ant\u00e9rieures \u00e0 la p\u00e9riode de r\u00e9tention par d\u00e9faut deltaTable . vacuum ( 100 ) // fichiers vides non requis par les versions de plus de 100 heures","title":"Suppression des anciens fichiers de donn\u00e9es"},{"location":"DeltaLake/#revenir-a-une-version-anterieure","text":"Vous pouvez revenir \u00e0 une version pr\u00e9c\u00e9dente de votre table et y travailler en utilisant la fonction de voyage dans le temps pour lire votre version cible en tant que trame de donn\u00e9es, puis la r\u00e9\u00e9crire dans le dossier delta lake. Cela cr\u00e9era une nouvelle version identique \u00e0 la version cible, \u00e0 partir de laquelle vous pourrez ensuite travailler. Les autres versions pr\u00e9c\u00e9dentes restent intactes. Exemple: Python # lire dans l'ancienne version du tableau df = spark . read . format ( \"delta\" ) . option ( \"versionAsOf\" , 0 ) . load ( delta_table_path ) # r\u00e9\u00e9crire dans la nouvelle version de la table, doit d\u00e9finir le mode sur \"overwrite\" df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( delta_table_path )","title":"Revenir \u00e0 Une Version Ant\u00e9rieure"},{"location":"DeltaLake/#documents-officiels","text":"Interroger un ancien instantan\u00e9 d'une table (voyage dans le temps) Supprimer les fichiers qui ne sont plus r\u00e9f\u00e9renc\u00e9s par une table delta","title":"Documents officiels"},{"location":"DeltaLake/#utilisation-de-delta-lake-dans-databricks","text":"Databricks prend en charge en natif Delta Lake et peut ex\u00e9cuter des requ\u00eates \u00e0 l'aide de Python, R, Scala et SQL. Vous devez d'abord cr\u00e9er un r\u00e9pertoire pour stocker les fichiers delta et noter le chemin d'acc\u00e8s \u00e0 ce r\u00e9pertoire. Lisez votre fichier de donn\u00e9es, puis \u00e9crivez-le au format \"delta\" et enregistrez-le dans le r\u00e9pertoire cr\u00e9\u00e9 ci-dessus. ``` # lire le fichier de donn\u00e9es testData = spark.read.format('json').options(header='true', inferSchema='true', multiline='true').load('/mnt/public-data/incoming/covid_tracking.json')","title":"Utilisation de Delta Lake dans Databricks"},{"location":"DeltaLake/#ecrire-au-format-delta","text":"testData.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/public-data/delta\") ``` Facultatif ( pas une bonne pratique ) : cr\u00e9ez une table SQL \u00e0 l'aide de delta: spark.sql(\"CREATE TABLE sample_table USING DELTA LOCATION '/mnt/public-data/delta/'\") Vous pouvez maintenant ex\u00e9cuter des requ\u00eates SQL sur votre table delta, y compris des requ\u00eates par num\u00e9ro de version ou horodatage pour \"voyager dans le temps\" vers les versions pr\u00e9c\u00e9dentes de vos donn\u00e9es. Si vous avez cr\u00e9\u00e9 une table \u00e0 l'\u00e9tape 3 , vous pouvez ex\u00e9cuter des requ\u00eates en utilisant le nom de la table. Sinon (meilleure pratique) , \u00e0 la place du nom de la table, vous pouvez utiliser delta.`{delta_table_path}` (remplacez {delta_table_path} par le chemin r\u00e9el). ``` %sql SELECT * FROM sample_table VERSION AS OF 0 SELECT * FROM delta. /mnt/public-data/delta/ ```","title":"\u00e9crire au format delta"},{"location":"DeltaLake/#documentation-microsoft_1","text":"D\u00e9marrage rapide du Delta Lake","title":"Documentation Microsoft"},{"location":"DeltaLake/#utilisation-de-delta-dans-azure-synapse","text":"Delta Lake est compatible avec Azure Synapse. Les tables delta peuvent \u00eatre cr\u00e9\u00e9es et interrog\u00e9es dans les blocs-notes Synapse de la m\u00eame mani\u00e8re que Databricks, avec la prise en charge du langage pour PySpark, Scala et .NET (C#). Notez que SQL n'est pas pris en charge avec la version actuelle. Lisez votre fichier de donn\u00e9es. data = spark.read.format('csv').options(header='true', inferSchema='true', multiline='true').load('abfss://public-data@statsconviddsinternal.dfs.core.windows.net/incoming/data_duplicate.csv') \u00c9crivez au format delta et enregistrez dans votre r\u00e9pertoire de table delta. data.write.format(\"delta\").save(delta_table_path) Facultatif : cr\u00e9ez une table SQL \u00e0 l'aide de delta (requis uniquement si vous souhaitez ex\u00e9cuter des requ\u00eates SQL, pas n\u00e9cessaire si vous utilisez uniquement Python, Scala ou C# ). spark.sql(\"CREATE TABLE example USING DELTA LOCATION '{0}'\".format(delta_table_path)) Vous pouvez maintenant ex\u00e9cuter des requ\u00eates sur vos donn\u00e9es.","title":"Utilisation de Delta dans Azure Synapse"},{"location":"DeltaLake/#documentation-microsoft_2","text":"Travailler avec Delta Lake","title":"Documentation Microsoft"},{"location":"DeltaLake/#utilisation-de-delta-lake-dans-data-factory","text":"Vous pouvez utiliser Azure Data Factory pour copier des donn\u00e9es vers et depuis un Delta Lake stock\u00e9 dans Azure Data Lake.","title":"Utilisation de Delta Lake dans Data Factory"},{"location":"DeltaLake/#exemple-copier-des-donnees-dans-delta-lake","text":"Cr\u00e9ez un nouveau flux de donn\u00e9es et ajoutez une source. Sous l'onglet Param\u00e8tres source , ajoutez l'ensemble de donn\u00e9es \u00e0 partir duquel vous souhaitez copier. Configurez tous les autres param\u00e8tres pertinents. Cliquez sur le bouton plus \u00e0 droite de votre source et ajoutez un r\u00e9cepteur. Sous l'onglet R\u00e9cepteur , choisissez En ligne comme type de r\u00e9cepteur et Delta comme type de jeu de donn\u00e9es en ligne. Sous l'onglet Param\u00e8tres , d\u00e9finissez le chemin du dossier (le chemin vers lequel vos fichiers delta seront stock\u00e9s).","title":"Exemple\u00a0: copier des donn\u00e9es dans Delta Lake"},{"location":"DeltaLake/#documentation-microsoft_3","text":"Format Delta dans Azure Data Factory","title":"Documentation Microsoft"},{"location":"DeltaLake/#utilisation-de-delta-avec-power-bi","text":"Pour lire les tables delta de mani\u00e8re native dans Power BI, veuillez consulter cette documentation sur GitHub .","title":"Utilisation de Delta avec Power BI"},{"location":"DeltaLake/#delta-dans-azure-machine-learning","text":"Le Delta Lake n'est actuellement pas pris en charge dans Azure ML.","title":"Delta dans Azure Machine Learning"},{"location":"FAQ/","text":"Ingestion de donn\u00e9es Comment puis-je ing\u00e9rer des donn\u00e9es (y compris des fichiers volumineux) dans la plateforme? Compte de stockage externe Les fichiers peuvent \u00eatre t\u00e9l\u00e9vers\u00e9s dans dans le conteneur inbox ou to-vers-int d'un compte de stockage externe, comme indiqu\u00e9 dans l'[ Explorateur de stockage Azure ] (AzureStorage.md). Ces fichiers seront alors automatiquement transf\u00e9r\u00e9s dans un compte de stockage interne (Data Lake) et rendus accessibles \u00e0 partir des services autoris\u00e9s. Remarque: Les comptes de stockage externe ont la convention de d\u00e9nomination stats project-acronym external . Service de transfert \u00e9lectronique de fichiers (TEF) Les employ\u00e9s de Statistique Canada peuvent utiliser le TEF pour transf\u00e9rer des fichiers de / vers les r\u00e9seaux sur site (R\u00e9s. A ou B) vers / depuis l'environnement infonuagique Azure. Veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack pour plus d'information sur ce processus. Outils de plateforme Des outils de plateforme comme Databricks ou Data Factory peuvent \u00eatre utilis\u00e9s pour ing\u00e9rer des donn\u00e9es provenant de sources dedonn\u00e9es publiques. Explorateur de stockage Comment puis-je configurer les param\u00e8tres du proxy de l'Explorateur de stockage Azure sur un VDI du r\u00e9seau B? Pour les employ\u00e9s de Statistique Canada seulement La configuration des param\u00e8tres du proxy est n\u00e9cessaire, si vous recevez le message d'erreur suivant : Dans l'Explorateur de stockage Azure, allez \u00e0 Modifier Param\u00e8tres du proxy . Entrez les param\u00e8tres du proxy n\u00e9cessaires et cliquez sur OK . Comment puis-je demander un nouveau jeton SAP (requis pour l'Explorateur de stockage Azure sur un VDI du r\u00e9seau B)? Pour les employ\u00e9s de Statistique Canada seulement Pour demander un jeton SAP temporaire, veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack . Pourquoi est-ce que j'obtiens un message d'erreur lorsque j'acc\u00e8de au compte de stockage interne Data Lake? Le compte de stockage interne Data Lake n'est accessible qu'\u00e0 partir d'une machine virtuelle dans l'Environnement d'analyse collaboratif (EAC). Il n'est pas accessible \u00e0 partir de votre ordinateur personnel, ni de votre ordinateur portable de travail, ni du VDI du r\u00e9seau B, ni d'une autre machine virtuelle sur nuage. Contr\u00f4le de code source Comment puis-je relier mon compte Visual Studio \u00e0 mon compte infonuagique de StatCan? Connectez-vous \u00e0 votre compte Visual Studio sur le site https://visualstudio.microsoft.com/fr/subscriptions/ en utilisant l'adresse \u00e9lectronique de votre organisation. Pour les employ\u00e9s de StatCan, il s'agit de votre adresse \u00e9lectronique qui se termine par \u00ab canada.ca \u00bb. Ajoutez votre compte infonuagique comme compte secondaire. Vous pourrez ainsi utiliser vos licences pour Visual Studio et Azure DevOps dans l'EAC. Pour les employ\u00e9s de Statistique Canada : Si vous n'avez pas d'abonnement Visual Studio, veuillez communiquer avec votre superviseur. S'il d\u00e9cide que vous avez besoin d'un abonnement, il pourra alors soumettre une demande de soutien en votre nom aupr\u00e8s de la Gestion des biens logiciels de StatCan pour vous obtenir une licence. Machines virtuelles Que dois-je faire si j'ai oubli\u00e9 le mot de passe de ma machine virtuelle? Si vous oubliez le mot de passe de votre machine virtuelle, veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack pour r\u00e9initialiser votre mot de passe. Vous pouvez \u00e9galement supprimer votre machine virtuelle, puis en cr\u00e9er une autre. Ce faisant, vous perdrez malheureusement les donn\u00e9es et les logiciels sur votre ancienne machine. Que dois-je faire si je dois ex\u00e9cuter un travail de longue dur\u00e9e sur ma machine virtuelle? Les machines sont arr\u00eat\u00e9es tous les jours \u00e0 19 h (HNE), afin de r\u00e9duire les frais d'exploitation. Pour ex\u00e9cuter un travail de longue dur\u00e9e, il est recommand\u00e9 d'utiliser Databricks ou Data Factory. AVERTISSEMENT : Il n'est pas recommand\u00e9 de d\u00e9sactiver l'arr\u00eat automatique, car cela pourrait entra\u00eener des frais importants. Pour d\u00e9sactiver l'arr\u00eat automatique : Acc\u00e9dez \u00e0 votre machine virtuelle dans le portail Azure. D\u00e9sactivez l'arr\u00eat automatique. Comment puis-je apporter des changements \u00e0 ma machine virtuelle? Si la machine virtuelle que vous utilisez actuellement ne r\u00e9pond pas \u00e0 vos besoins, veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack . Databricks Pourquoi suis-je incapable d'ex\u00e9cuter le code \u00e0 partir de mon notebook dans Databricks? Vous devez d'abord d\u00e9marrer un cluster dans Databricks qui a d\u00e9j\u00e0 \u00e9t\u00e9 cr\u00e9\u00e9 pour vous : 1. Cliquez sur Clusters. Naviguez vers votre cluster et cliquez sur le bouton D\u00e9marrer (ic\u00f4ne de la fl\u00e8che). Quels types de clusters sont disponibles dans Databricks? Voir le lien suivant pour les diff\u00e9rents types de clusters disponibles: https://docs.microsoft.com/en-us/azure/databricks/release-notes/runtime/releases#:~:text=Supported%20Databricks%20runtime%20releases%20and%20support%20schedule%20,Sep%2002%2C%202021%20%2022%20more%20rows%20 Que se passe-t-il lorsque les clusters sont mis \u00e0 niveau? LTS (support \u00e0 long terme) a un support pendant 1-2 ans. Ils devront \u00eatre p\u00e9riodiquement mis \u00e0 jour vers une version plus r\u00e9cente. Lors de la mise \u00e0 niveau, tout le code doit \u00eatre r\u00e9ex\u00e9cut\u00e9 pour s'assurer qu'il n'y a pas de probl\u00e8mes lors de la mise \u00e0 jour d'un cluster. Comment lire un fichier excel avec databricks/python? Voici un exemple de lecture dans un fichier excel : % python import pandas as pd pd . read_excel ( \"/dbfs/mnt/ccei-ccie-ext/Daily charts.xlsx\" , engine = 'openyxl' ) Tableau de bord Comment changer mon abonnement pour voir mes ressources? Dans le portail Azure, cliquez sur l'ic\u00f4ne R\u00e9pertoire + abonnement . S\u00e9lectionnez l'abonnement vdl . Autre Comment puis-je me connecter au compte de stockage interne Data Lake avec Power BI Desktop? Pr\u00e9requis: - Une machine virtuelle dans l'Environnement d'analyse collaborative (EAC). - Power BI Desktop. (Offert par d\u00e9faut dans les images de machine virtuelle Data Science Virtual Machine.) \u00c9tapes: Connectez-vous \u00e0 votre machine virtuelle dans l'EAC. Lancez Power BI Desktop. Suivez les \u00e9tapes dans Analysez des donn\u00e9es dans Azure Data Lake Storage Gen2 avec Power BI - Cr\u00e9er un rapport dans Power BI Desktop (document anglais) . S.v.p. envoyez un message \u00e0 Slack si vous ne connaissez pas l'URL du Azure Data Lake Storage Gen2. Comment les employ\u00e9s de Statistique Canada peuvent-ils transf\u00e9rer des fichiers de leur centre de donn\u00e9es? Pour les employ\u00e9s de Statistique Canada, ils peuvent se r\u00e9f\u00e9rer \u00e0 cette documentation interne: Data Ingestion Comment puis-je ajouter une question \u00e0 la FAQ? Veuillez faire parvenir votre suggestion de question par l'interm\u00e9diaire du canal Slack .","title":"FAQ"},{"location":"FAQ/#ingestion-de-donnees","text":"","title":"Ingestion de donn\u00e9es"},{"location":"FAQ/#comment-puis-je-ingerer-des-donnees-y-compris-des-fichiers-volumineux-dans-la-plateforme","text":"","title":"Comment puis-je ing\u00e9rer des donn\u00e9es (y compris des fichiers volumineux) dans la plateforme?"},{"location":"FAQ/#compte-de-stockage-externe","text":"Les fichiers peuvent \u00eatre t\u00e9l\u00e9vers\u00e9s dans dans le conteneur inbox ou to-vers-int d'un compte de stockage externe, comme indiqu\u00e9 dans l'[ Explorateur de stockage Azure ] (AzureStorage.md). Ces fichiers seront alors automatiquement transf\u00e9r\u00e9s dans un compte de stockage interne (Data Lake) et rendus accessibles \u00e0 partir des services autoris\u00e9s. Remarque: Les comptes de stockage externe ont la convention de d\u00e9nomination stats project-acronym external .","title":"Compte de stockage externe"},{"location":"FAQ/#service-de-transfert-electronique-de-fichiers-tef","text":"Les employ\u00e9s de Statistique Canada peuvent utiliser le TEF pour transf\u00e9rer des fichiers de / vers les r\u00e9seaux sur site (R\u00e9s. A ou B) vers / depuis l'environnement infonuagique Azure. Veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack pour plus d'information sur ce processus.","title":"Service de transfert \u00e9lectronique de fichiers (TEF)"},{"location":"FAQ/#outils-de-plateforme","text":"Des outils de plateforme comme Databricks ou Data Factory peuvent \u00eatre utilis\u00e9s pour ing\u00e9rer des donn\u00e9es provenant de sources dedonn\u00e9es publiques.","title":"Outils de plateforme"},{"location":"FAQ/#explorateur-de-stockage","text":"","title":"Explorateur de stockage"},{"location":"FAQ/#comment-puis-je-configurer-les-parametres-du-proxy-de-lexplorateur-de-stockage-azure-sur-un-vdi-du-reseau-b","text":"Pour les employ\u00e9s de Statistique Canada seulement La configuration des param\u00e8tres du proxy est n\u00e9cessaire, si vous recevez le message d'erreur suivant : Dans l'Explorateur de stockage Azure, allez \u00e0 Modifier Param\u00e8tres du proxy . Entrez les param\u00e8tres du proxy n\u00e9cessaires et cliquez sur OK .","title":"Comment puis-je configurer les param\u00e8tres du proxy de l'Explorateur de stockage\u00a0Azure sur un VDI du r\u00e9seau\u00a0B?"},{"location":"FAQ/#comment-puis-je-demander-un-nouveau-jeton-sap-requis-pour-lexplorateur-de-stockage-azure-sur-un-vdi-du-reseau-b","text":"Pour les employ\u00e9s de Statistique Canada seulement Pour demander un jeton SAP temporaire, veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack .","title":"Comment puis-je demander un nouveau jeton\u00a0SAP (requis pour l'Explorateur de stockage\u00a0Azure sur un VDI du r\u00e9seau\u00a0B)?"},{"location":"FAQ/#pourquoi-est-ce-que-jobtiens-un-message-derreur-lorsque-jaccede-au-compte-de-stockage-interne-data-lake","text":"Le compte de stockage interne Data Lake n'est accessible qu'\u00e0 partir d'une machine virtuelle dans l'Environnement d'analyse collaboratif (EAC). Il n'est pas accessible \u00e0 partir de votre ordinateur personnel, ni de votre ordinateur portable de travail, ni du VDI du r\u00e9seau B, ni d'une autre machine virtuelle sur nuage.","title":"Pourquoi est-ce que j'obtiens un message d'erreur lorsque j'acc\u00e8de au compte de stockage interne Data\u00a0Lake?"},{"location":"FAQ/#controle-de-code-source","text":"","title":"Contr\u00f4le de code source"},{"location":"FAQ/#comment-puis-je-relier-mon-compte-visual-studio-a-mon-compte-infonuagique-de-statcan","text":"Connectez-vous \u00e0 votre compte Visual Studio sur le site https://visualstudio.microsoft.com/fr/subscriptions/ en utilisant l'adresse \u00e9lectronique de votre organisation. Pour les employ\u00e9s de StatCan, il s'agit de votre adresse \u00e9lectronique qui se termine par \u00ab canada.ca \u00bb. Ajoutez votre compte infonuagique comme compte secondaire. Vous pourrez ainsi utiliser vos licences pour Visual Studio et Azure DevOps dans l'EAC. Pour les employ\u00e9s de Statistique Canada : Si vous n'avez pas d'abonnement Visual Studio, veuillez communiquer avec votre superviseur. S'il d\u00e9cide que vous avez besoin d'un abonnement, il pourra alors soumettre une demande de soutien en votre nom aupr\u00e8s de la Gestion des biens logiciels de StatCan pour vous obtenir une licence.","title":"Comment puis-je relier mon compte Visual\u00a0Studio \u00e0 mon compte infonuagique de StatCan?"},{"location":"FAQ/#machines-virtuelles","text":"","title":"Machines virtuelles"},{"location":"FAQ/#que-dois-je-faire-si-jai-oublie-le-mot-de-passe-de-ma-machine-virtuelle","text":"Si vous oubliez le mot de passe de votre machine virtuelle, veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack pour r\u00e9initialiser votre mot de passe. Vous pouvez \u00e9galement supprimer votre machine virtuelle, puis en cr\u00e9er une autre. Ce faisant, vous perdrez malheureusement les donn\u00e9es et les logiciels sur votre ancienne machine.","title":"Que dois-je faire si j'ai oubli\u00e9 le mot de passe de ma machine virtuelle?"},{"location":"FAQ/#que-dois-je-faire-si-je-dois-executer-un-travail-de-longue-duree-sur-ma-machine-virtuelle","text":"Les machines sont arr\u00eat\u00e9es tous les jours \u00e0 19 h (HNE), afin de r\u00e9duire les frais d'exploitation. Pour ex\u00e9cuter un travail de longue dur\u00e9e, il est recommand\u00e9 d'utiliser Databricks ou Data Factory. AVERTISSEMENT : Il n'est pas recommand\u00e9 de d\u00e9sactiver l'arr\u00eat automatique, car cela pourrait entra\u00eener des frais importants. Pour d\u00e9sactiver l'arr\u00eat automatique : Acc\u00e9dez \u00e0 votre machine virtuelle dans le portail Azure. D\u00e9sactivez l'arr\u00eat automatique.","title":"Que dois-je faire si je dois ex\u00e9cuter un travail de longue dur\u00e9e sur ma machine virtuelle?"},{"location":"FAQ/#comment-puis-je-apporter-des-changements-a-ma-machine-virtuelle","text":"Si la machine virtuelle que vous utilisez actuellement ne r\u00e9pond pas \u00e0 vos besoins, veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal Slack .","title":"Comment puis-je apporter des changements \u00e0 ma machine virtuelle?"},{"location":"FAQ/#databricks","text":"","title":"Databricks"},{"location":"FAQ/#pourquoi-suis-je-incapable-dexecuter-le-code-a-partir-de-mon-notebook-dans-databricks","text":"Vous devez d'abord d\u00e9marrer un cluster dans Databricks qui a d\u00e9j\u00e0 \u00e9t\u00e9 cr\u00e9\u00e9 pour vous : 1. Cliquez sur Clusters. Naviguez vers votre cluster et cliquez sur le bouton D\u00e9marrer (ic\u00f4ne de la fl\u00e8che).","title":"Pourquoi suis-je incapable d'ex\u00e9cuter le code \u00e0 partir de mon notebook dans Databricks?"},{"location":"FAQ/#quels-types-de-clusters-sont-disponibles-dans-databricks","text":"Voir le lien suivant pour les diff\u00e9rents types de clusters disponibles: https://docs.microsoft.com/en-us/azure/databricks/release-notes/runtime/releases#:~:text=Supported%20Databricks%20runtime%20releases%20and%20support%20schedule%20,Sep%2002%2C%202021%20%2022%20more%20rows%20","title":"Quels types de clusters sont disponibles dans Databricks?"},{"location":"FAQ/#que-se-passe-t-il-lorsque-les-clusters-sont-mis-a-niveau","text":"LTS (support \u00e0 long terme) a un support pendant 1-2 ans. Ils devront \u00eatre p\u00e9riodiquement mis \u00e0 jour vers une version plus r\u00e9cente. Lors de la mise \u00e0 niveau, tout le code doit \u00eatre r\u00e9ex\u00e9cut\u00e9 pour s'assurer qu'il n'y a pas de probl\u00e8mes lors de la mise \u00e0 jour d'un cluster.","title":"Que se passe-t-il lorsque les clusters sont mis \u00e0 niveau?"},{"location":"FAQ/#comment-lire-un-fichier-excel-avec-databrickspython","text":"Voici un exemple de lecture dans un fichier excel : % python import pandas as pd pd . read_excel ( \"/dbfs/mnt/ccei-ccie-ext/Daily charts.xlsx\" , engine = 'openyxl' )","title":"Comment lire un fichier excel avec databricks/python?"},{"location":"FAQ/#tableau-de-bord","text":"","title":"Tableau de bord"},{"location":"FAQ/#comment-changer-mon-abonnement-pour-voir-mes-ressources","text":"Dans le portail Azure, cliquez sur l'ic\u00f4ne R\u00e9pertoire + abonnement . S\u00e9lectionnez l'abonnement vdl .","title":"Comment changer mon abonnement pour voir mes ressources?"},{"location":"FAQ/#autre","text":"","title":"Autre"},{"location":"FAQ/#comment-puis-je-me-connecter-au-compte-de-stockage-interne-data-lake-avec-power-bi-desktop","text":"Pr\u00e9requis: - Une machine virtuelle dans l'Environnement d'analyse collaborative (EAC). - Power BI Desktop. (Offert par d\u00e9faut dans les images de machine virtuelle Data Science Virtual Machine.) \u00c9tapes: Connectez-vous \u00e0 votre machine virtuelle dans l'EAC. Lancez Power BI Desktop. Suivez les \u00e9tapes dans Analysez des donn\u00e9es dans Azure Data Lake Storage Gen2 avec Power BI - Cr\u00e9er un rapport dans Power BI Desktop (document anglais) . S.v.p. envoyez un message \u00e0 Slack si vous ne connaissez pas l'URL du Azure Data Lake Storage Gen2.","title":"Comment puis-je me connecter au compte de stockage interne Data Lake avec Power BI Desktop?"},{"location":"FAQ/#comment-les-employes-de-statistique-canada-peuvent-ils-transferer-des-fichiers-de-leur-centre-de-donnees","text":"Pour les employ\u00e9s de Statistique Canada, ils peuvent se r\u00e9f\u00e9rer \u00e0 cette documentation interne: Data Ingestion","title":"Comment les employ\u00e9s de Statistique Canada peuvent-ils transf\u00e9rer des fichiers de leur centre de donn\u00e9es?"},{"location":"FAQ/#comment-puis-je-ajouter-une-question-a-la-faq","text":"Veuillez faire parvenir votre suggestion de question par l'interm\u00e9diaire du canal Slack .","title":"Comment puis-je ajouter une question \u00e0 la FAQ?"},{"location":"GitHubCommencer/","text":"GitHub.com est une plateforme en ligne utilis\u00e9e \u00e0 des fins de collaboration ainsi que pour assurer le suivi des modifications et des versions pour divers types de projets. Ce document montre la fa\u00e7on de commencer \u00e0 utiliser Git avec divers services Azure dont l\u2019int\u00e9gration Git est d\u00e9j\u00e0 configur\u00e9e. Voir GitHub - Configuration pour obtenir des instructions sur la fa\u00e7on de configurer ce syst\u00e8me. Important : ne stockez pas de donn\u00e9es class\u00e9es Prot\u00e9g\u00e9 B sur GitHub. Cr\u00e9ation d\u2019un compte GitHub Des renseignements sur la cr\u00e9ation d\u2019un compte GitHub (ou sur l\u2019utilisation de votre compte existant) se trouvent \u00e0 l\u2019adresse Des renseignements sur la cr\u00e9ation d\u2019un compte GitHub (ou sur l\u2019utilisation de votre compte existant) se trouvent \u00e0 l\u2019adresse https://digital.statcan.gc.ca/drafts/guides-platforms-github . Azure Data Factory Si l\u2019int\u00e9gration Git est configur\u00e9e pour votre Data Factory, chaque fois que vous enregistrez ou que vous publiez des modifications, celles-ci seront automatiquement synchronis\u00e9es avec le d\u00e9p\u00f4t GitHub. Pour changer la division dans laquelle vous travaillez (la division de collaboration par d\u00e9faut est main [principale]), cliquez sur la fl\u00e8che orient\u00e9e vers le bas, \u00e0 c\u00f4t\u00e9 du nom de la division, en haut \u00e0 gauche de l\u2019\u00e9cran. \u00c0 partir de l\u00e0, vous pouvez s\u00e9lectionner une autre division ou en cr\u00e9er une nouvelle. Databricks Configuration d\u2019un jeton d\u2019acc\u00e8s personnel Avant de pouvoir travailler avec les d\u00e9p\u00f4ts GitHub dans Databricks, vous devez d\u2019abord configurer un jeton d\u2019acc\u00e8s personnel (ce qui permet \u00e0 Databricks d\u2019acc\u00e9der \u00e0 votre compte GitHub). Dans Databricks, ouvrez User Settings (param\u00e8tres d\u2019utilisateur), puis cliquez sur l\u2019onglet Git integration (int\u00e9gration de Git). Sous Git provider (fournisseur Git), s\u00e9lectionnez GitHub. Entrez votre nom d\u2019utilisateur GitHub. \u00c0 partir de votre compte GitHub, suivez les instructions pour cr\u00e9er un jeton d\u2019acc\u00e8s personnel , en vous assurant que la case de permission repo (d\u00e9p\u00f4t) est coch\u00e9e. Si vous avez fix\u00e9 une date d\u2019expiration, vous devrez r\u00e9p\u00e9ter ce processus pour cr\u00e9er un nouveau jeton une fois la date fix\u00e9e pass\u00e9e. Copiez le jeton et collez-le dans Databricks. Cliquez sur Save (enregistrer). Cr\u00e9ation et modification de divisions Une pratique exemplaire consiste \u00e0 effectuer tous vos travaux dans votre propre branche division (et non pas dans la division principale), puis \u00e0 fusionner vos modifications avec la division principale une fois que vous \u00eates pr\u00eat \u00e0 publier vos travaux. Pour cr\u00e9er une nouvelle division ou modifier une division existante, cliquez sur l\u2019onglet Repos (d\u00e9p\u00f4ts). \u00c0 partir de l\u00e0, cliquez sur le dossier contenant le d\u00e9p\u00f4t GitHub pour l\u2019ouvrir. Cliquez sur la fl\u00e8che orient\u00e9e vers le bas, \u00e0 c\u00f4t\u00e9 du nom de la division, puis cliquez sur Git... . Cliquez sur la fl\u00e8che orient\u00e9e vers le bas pour trouver une division existante, ou cliquez sur le signe \u00ab plus \u00bb pour en cr\u00e9er une nouvelle. Votre division doit inclure votre nom. Une fois votre branche cr\u00e9\u00e9e, recherchez-la dans le menu d\u00e9roulant et cliquez dessus pour y basculer. Cliquez sur fermer . Tout votre travail sera d\u00e9sormais enregistr\u00e9 dans cette branche, \u00e0 moins que vous ne le changiez \u00e0 nouveau plus tard. Cr\u00e9ation, d\u00e9placement et clonage de blocs-notes Pour cr\u00e9er un nouveau bloc-notes dans le r\u00e9f\u00e9rentiel, dans le menu d\u00e9roulant \u00e0 c\u00f4t\u00e9 du nom de votre branche, survolez Cr\u00e9er , puis cliquez sur Bloc-notes . Vous pouvez \u00e9galement cr\u00e9er des dossiers de cette fa\u00e7on. Pour cloner un bloc-notes existant de votre espace de travail, naviguez jusqu\u2019au bloc-notes (dans l\u2019onglet Workspace [espace de travail]), cliquez sur la fl\u00e8che orient\u00e9e vers le bas, \u00e0 c\u00f4t\u00e9 du nom du bloc-notes, puis cliquez sur Clone (cloner) pour cr\u00e9er une copie du bloc-notes dans le d\u00e9p\u00f4t. Pour d\u00e9placer le bloc-notes de l\u2019espace de travail au d\u00e9p\u00f4t, s\u00e9lectionnez Move (d\u00e9placer). Trouvez le d\u00e9p\u00f4t dans le menu contextuel et naviguez jusqu\u2019au dossier dans lequel vous souhaitez cloner ou d\u00e9placer le bloc-notes. Cliquez sur Clone/Select (cloner/s\u00e9lectionner). Validation et acheminement des modifications Dans l\u2019onglet Repos (d\u00e9p\u00f4ts), cliquez sur le dossier contenant le d\u00e9p\u00f4t GitHub pour l\u2019ouvrir. Cliquez sur la fl\u00e8che orient\u00e9e vers le bas, \u00e0 c\u00f4t\u00e9 du nom de la division, puis cliquez sur Git... . Assurez-vous que toutes les modifications que vous souhaitez apporter sont coch\u00e9es. Saisissez un bref r\u00e9sum\u00e9 d\u00e9crivant les modifications apport\u00e9es, puis cliquez sur Commit & Push (valider et acheminer). Azure Synapse Voir Azure Data Factory plus haut. Visual Studio Code Comment cloner un d\u00e9p\u00f4t Cliquez sur l\u2019onglet Source Control (contr\u00f4le de la source). Vous pouvez alors soit ouvrir un dossier contenant un d\u00e9p\u00f4t Git si vous en avez d\u00e9j\u00e0 un sur votre machine virtuelle (MV) en nuage, soit cloner un d\u00e9p\u00f4t \u00e0 partir de l\u2019adresse d\u2019une page Web (URL). Pour cloner un d\u00e9p\u00f4t, cliquez sur Clone Repository (cloner le d\u00e9p\u00f4t). Copiez l\u2019adresse URL du d\u00e9p\u00f4t depuis GitHub (par exemple, https://github.com/nom d\u2019utilisateur/nom du d\u00e9p\u00f4t), collez-la dans la bo\u00eete de texte et cliquez sur Clone from URL (cloner \u00e0 partir de l\u2019URL). Choisissez un dossier sur votre MV en nuage o\u00f9 le d\u00e9p\u00f4t Git sera stock\u00e9 localement. Vous serez peut-\u00eatre invit\u00e9 \u00e0 vous connecter \u00e0 votre compte GitHub. Une fois le d\u00e9p\u00f4t clon\u00e9 sur votre machine, vous pouvez ouvrir le dossier local dans Visual Studio Code. Comment valider des modifications Avant de pouvoir valider des modifications, vous devez configurer votre nom d\u2019utilisateur et votre adresse \u00e9lectronique. Ouvrez une fen\u00eatre de terminal (cliquez sur Terminal , puis sur New Terminal [nouveau terminal] dans la barre de menus). Dans le terminal, tapez ce qui suit : git config user.name \"First Last\" git config user.email \"pr\u00e9nom.nom@canada.ca\" Lorsque vous \u00eates pr\u00eat \u00e0 publier vos modifications sur GitHub, tapez un message de validation dans l\u2019onglet Source control (contr\u00f4le de la source), puis cliquez sur la coche. Cliquez sur le bouton du menu de contr\u00f4le de la source, puis sur Push (acheminer). Vous obtiendrez un message d\u2019erreur si votre copie locale du d\u00e9p\u00f4t n\u2019est pas \u00e0 jour par rapport \u00e0 la version stock\u00e9e sur GitHub. Dans ce cas, cliquez sur Pull , puis Push (r\u00e9cup\u00e9rer, puis acheminer) pour fusionner vos modifications. R Studio Remarque : les instructions sont les m\u00eames, que vous utilisiez la version de bureau de R-Studio \u00e0 partir d\u2019une MV en nuage ou la version Web via Databricks. Configuration Dans le menu File (fichier), cliquez sur New Project... (nouveau projet), puis s\u00e9lectionnez Version Control (contr\u00f4le de version). S\u00e9lectionnez Git . Saisissez l\u2019adresse URL du d\u00e9p\u00f4t GitHub que vous souhaitez cloner. Choisissez un dossier o\u00f9 seront stock\u00e9s les fichiers locaux, puis cliquez sur Create project (cr\u00e9er le projet). Si vous \u00eates invit\u00e9 \u00e0 vous connecter \u00e0 votre compte GitHub, saisissez votre nom d\u2019utilisateur GitHub et un jeton d\u2019acc\u00e8s personnel comme mot de passe. Comment valider des modifications Lorsque vous \u00eates pr\u00eat \u00e0 publier vos modifications sur GitHub, cliquez sur l\u2019onglet Git . Ensuite, cliquez sur Commit (valider). Cliquez sur la case \u00e0 cocher pour chacune des modifications que vous souhaitez valider. Saisissez un message de validation d\u00e9crivant bri\u00e8vement vos modifications, puis cliquez sur Commit (valider). Une fen\u00eatre contextuelle s\u2019affichera pour confirmer que votre validation a fonctionn\u00e9. Cliquez sur Close (fermer). Cliquez sur le bouton Push (acheminer) pour t\u00e9l\u00e9charger vos modifications sur GitHub. Une fen\u00eatre contextuelle s\u2019affichera pour confirmer que l\u2019acheminement a fonctionn\u00e9. Vous obtiendrez un message d\u2019erreur si votre copie locale du d\u00e9p\u00f4t n\u2019est pas \u00e0 jour par rapport \u00e0 la version stock\u00e9e sur GitHub. Dans ce cas, cliquez sur Pull , puis Push (r\u00e9cup\u00e9rer, puis acheminer) pour fusionner vos modifications. FAQ J\u2019obtiens une erreur lorsque j\u2019essaie d\u2019acheminer les modifications. Pourquoi? Cela peut signifier que votre copie locale du d\u00e9p\u00f4t n\u2019est pas \u00e0 jour par rapport \u00e0 la copie stock\u00e9e sur GitHub. Essayez d\u2019abord de cliquer sur pull (r\u00e9cup\u00e9rer), puis d\u2019acheminer vos modifications. Une pratique exemplaire consiste \u00e0 toujours ex\u00e9cuter une commande de r\u00e9cup\u00e9ration avant de commencer \u00e0 travailler avec un d\u00e9p\u00f4t afin de s\u2019assurer que vous travaillez sur la derni\u00e8re version, et qu\u2019une seule personne peut modifier le m\u00eame fichier en m\u00eame temps. Comment revenir \u00e0 une validation ant\u00e9rieure? Cette op\u00e9ration doit \u00eatre effectu\u00e9e \u00e0 partir de Visual Studio (VS) Code (accessible par une machine virtuelle en nuage), quel que soit l\u2019endroit o\u00f9 vous utilisez principalement Git. Dans VS Code, suivez les \u00e9tapes pour cloner un d\u00e9p\u00f4t si vous ne l\u2019avez pas d\u00e9j\u00e0 fait. Ouvrez un terminal (cliquez sur Terminal , puis New Terminal [terminal, puis nouveau terminal]). Dans la fen\u00eatre du terminal, tapez git revert HEAD~n --no-edit et appuyez sur Entr\u00e9e (remplacez n par le nombre de validations que vous souhaitez r\u00e9tablir, en commen\u00e7ant par 0). Remarque : vous pouvez trouver l\u2019historique de vos validations sur GitHub en cliquant sur x commits (x validations) en haut \u00e0 droite de votre d\u00e9p\u00f4t. S\u2019il y a des conflits de fusion, cliquez sur Accept incoming change (accepter la modification entrante), puis validez et acheminez les modifications comme d\u2019habitude (voir les instructions d\u00e9taill\u00e9es sur la fa\u00e7on de le faire dans VS Code).","title":"Commencer"},{"location":"GitHubCommencer/#creation-dun-compte-github","text":"Des renseignements sur la cr\u00e9ation d\u2019un compte GitHub (ou sur l\u2019utilisation de votre compte existant) se trouvent \u00e0 l\u2019adresse Des renseignements sur la cr\u00e9ation d\u2019un compte GitHub (ou sur l\u2019utilisation de votre compte existant) se trouvent \u00e0 l\u2019adresse https://digital.statcan.gc.ca/drafts/guides-platforms-github .","title":"Cr\u00e9ation d\u2019un compte GitHub"},{"location":"GitHubCommencer/#azure-data-factory","text":"Si l\u2019int\u00e9gration Git est configur\u00e9e pour votre Data Factory, chaque fois que vous enregistrez ou que vous publiez des modifications, celles-ci seront automatiquement synchronis\u00e9es avec le d\u00e9p\u00f4t GitHub. Pour changer la division dans laquelle vous travaillez (la division de collaboration par d\u00e9faut est main [principale]), cliquez sur la fl\u00e8che orient\u00e9e vers le bas, \u00e0 c\u00f4t\u00e9 du nom de la division, en haut \u00e0 gauche de l\u2019\u00e9cran. \u00c0 partir de l\u00e0, vous pouvez s\u00e9lectionner une autre division ou en cr\u00e9er une nouvelle.","title":"Azure Data Factory"},{"location":"GitHubCommencer/#databricks","text":"","title":"Databricks"},{"location":"GitHubCommencer/#configuration-dun-jeton-dacces-personnel","text":"Avant de pouvoir travailler avec les d\u00e9p\u00f4ts GitHub dans Databricks, vous devez d\u2019abord configurer un jeton d\u2019acc\u00e8s personnel (ce qui permet \u00e0 Databricks d\u2019acc\u00e9der \u00e0 votre compte GitHub). Dans Databricks, ouvrez User Settings (param\u00e8tres d\u2019utilisateur), puis cliquez sur l\u2019onglet Git integration (int\u00e9gration de Git). Sous Git provider (fournisseur Git), s\u00e9lectionnez GitHub. Entrez votre nom d\u2019utilisateur GitHub. \u00c0 partir de votre compte GitHub, suivez les instructions pour cr\u00e9er un jeton d\u2019acc\u00e8s personnel , en vous assurant que la case de permission repo (d\u00e9p\u00f4t) est coch\u00e9e. Si vous avez fix\u00e9 une date d\u2019expiration, vous devrez r\u00e9p\u00e9ter ce processus pour cr\u00e9er un nouveau jeton une fois la date fix\u00e9e pass\u00e9e. Copiez le jeton et collez-le dans Databricks. Cliquez sur Save (enregistrer).","title":"Configuration d\u2019un jeton d\u2019acc\u00e8s personnel"},{"location":"GitHubCommencer/#creation-et-modification-de-divisions","text":"Une pratique exemplaire consiste \u00e0 effectuer tous vos travaux dans votre propre branche division (et non pas dans la division principale), puis \u00e0 fusionner vos modifications avec la division principale une fois que vous \u00eates pr\u00eat \u00e0 publier vos travaux. Pour cr\u00e9er une nouvelle division ou modifier une division existante, cliquez sur l\u2019onglet Repos (d\u00e9p\u00f4ts). \u00c0 partir de l\u00e0, cliquez sur le dossier contenant le d\u00e9p\u00f4t GitHub pour l\u2019ouvrir. Cliquez sur la fl\u00e8che orient\u00e9e vers le bas, \u00e0 c\u00f4t\u00e9 du nom de la division, puis cliquez sur Git... . Cliquez sur la fl\u00e8che orient\u00e9e vers le bas pour trouver une division existante, ou cliquez sur le signe \u00ab plus \u00bb pour en cr\u00e9er une nouvelle. Votre division doit inclure votre nom. Une fois votre branche cr\u00e9\u00e9e, recherchez-la dans le menu d\u00e9roulant et cliquez dessus pour y basculer. Cliquez sur fermer . Tout votre travail sera d\u00e9sormais enregistr\u00e9 dans cette branche, \u00e0 moins que vous ne le changiez \u00e0 nouveau plus tard.","title":"Cr\u00e9ation et modification de divisions"},{"location":"GitHubCommencer/#creation-deplacement-et-clonage-de-blocs-notes","text":"Pour cr\u00e9er un nouveau bloc-notes dans le r\u00e9f\u00e9rentiel, dans le menu d\u00e9roulant \u00e0 c\u00f4t\u00e9 du nom de votre branche, survolez Cr\u00e9er , puis cliquez sur Bloc-notes . Vous pouvez \u00e9galement cr\u00e9er des dossiers de cette fa\u00e7on. Pour cloner un bloc-notes existant de votre espace de travail, naviguez jusqu\u2019au bloc-notes (dans l\u2019onglet Workspace [espace de travail]), cliquez sur la fl\u00e8che orient\u00e9e vers le bas, \u00e0 c\u00f4t\u00e9 du nom du bloc-notes, puis cliquez sur Clone (cloner) pour cr\u00e9er une copie du bloc-notes dans le d\u00e9p\u00f4t. Pour d\u00e9placer le bloc-notes de l\u2019espace de travail au d\u00e9p\u00f4t, s\u00e9lectionnez Move (d\u00e9placer). Trouvez le d\u00e9p\u00f4t dans le menu contextuel et naviguez jusqu\u2019au dossier dans lequel vous souhaitez cloner ou d\u00e9placer le bloc-notes. Cliquez sur Clone/Select (cloner/s\u00e9lectionner).","title":"Cr\u00e9ation, d\u00e9placement et clonage de blocs-notes"},{"location":"GitHubCommencer/#validation-et-acheminement-des-modifications","text":"Dans l\u2019onglet Repos (d\u00e9p\u00f4ts), cliquez sur le dossier contenant le d\u00e9p\u00f4t GitHub pour l\u2019ouvrir. Cliquez sur la fl\u00e8che orient\u00e9e vers le bas, \u00e0 c\u00f4t\u00e9 du nom de la division, puis cliquez sur Git... . Assurez-vous que toutes les modifications que vous souhaitez apporter sont coch\u00e9es. Saisissez un bref r\u00e9sum\u00e9 d\u00e9crivant les modifications apport\u00e9es, puis cliquez sur Commit & Push (valider et acheminer).","title":"Validation et acheminement des modifications"},{"location":"GitHubCommencer/#azure-synapse","text":"Voir Azure Data Factory plus haut.","title":"Azure Synapse"},{"location":"GitHubCommencer/#visual-studio-code","text":"","title":"Visual Studio Code"},{"location":"GitHubCommencer/#comment-cloner-un-depot","text":"Cliquez sur l\u2019onglet Source Control (contr\u00f4le de la source). Vous pouvez alors soit ouvrir un dossier contenant un d\u00e9p\u00f4t Git si vous en avez d\u00e9j\u00e0 un sur votre machine virtuelle (MV) en nuage, soit cloner un d\u00e9p\u00f4t \u00e0 partir de l\u2019adresse d\u2019une page Web (URL). Pour cloner un d\u00e9p\u00f4t, cliquez sur Clone Repository (cloner le d\u00e9p\u00f4t). Copiez l\u2019adresse URL du d\u00e9p\u00f4t depuis GitHub (par exemple, https://github.com/nom d\u2019utilisateur/nom du d\u00e9p\u00f4t), collez-la dans la bo\u00eete de texte et cliquez sur Clone from URL (cloner \u00e0 partir de l\u2019URL). Choisissez un dossier sur votre MV en nuage o\u00f9 le d\u00e9p\u00f4t Git sera stock\u00e9 localement. Vous serez peut-\u00eatre invit\u00e9 \u00e0 vous connecter \u00e0 votre compte GitHub. Une fois le d\u00e9p\u00f4t clon\u00e9 sur votre machine, vous pouvez ouvrir le dossier local dans Visual Studio Code.","title":"Comment cloner un d\u00e9p\u00f4t"},{"location":"GitHubCommencer/#comment-valider-des-modifications","text":"Avant de pouvoir valider des modifications, vous devez configurer votre nom d\u2019utilisateur et votre adresse \u00e9lectronique. Ouvrez une fen\u00eatre de terminal (cliquez sur Terminal , puis sur New Terminal [nouveau terminal] dans la barre de menus). Dans le terminal, tapez ce qui suit : git config user.name \"First Last\" git config user.email \"pr\u00e9nom.nom@canada.ca\" Lorsque vous \u00eates pr\u00eat \u00e0 publier vos modifications sur GitHub, tapez un message de validation dans l\u2019onglet Source control (contr\u00f4le de la source), puis cliquez sur la coche. Cliquez sur le bouton du menu de contr\u00f4le de la source, puis sur Push (acheminer). Vous obtiendrez un message d\u2019erreur si votre copie locale du d\u00e9p\u00f4t n\u2019est pas \u00e0 jour par rapport \u00e0 la version stock\u00e9e sur GitHub. Dans ce cas, cliquez sur Pull , puis Push (r\u00e9cup\u00e9rer, puis acheminer) pour fusionner vos modifications.","title":"Comment valider des modifications"},{"location":"GitHubCommencer/#r-studio","text":"Remarque : les instructions sont les m\u00eames, que vous utilisiez la version de bureau de R-Studio \u00e0 partir d\u2019une MV en nuage ou la version Web via Databricks.","title":"R Studio"},{"location":"GitHubCommencer/#configuration","text":"Dans le menu File (fichier), cliquez sur New Project... (nouveau projet), puis s\u00e9lectionnez Version Control (contr\u00f4le de version). S\u00e9lectionnez Git . Saisissez l\u2019adresse URL du d\u00e9p\u00f4t GitHub que vous souhaitez cloner. Choisissez un dossier o\u00f9 seront stock\u00e9s les fichiers locaux, puis cliquez sur Create project (cr\u00e9er le projet). Si vous \u00eates invit\u00e9 \u00e0 vous connecter \u00e0 votre compte GitHub, saisissez votre nom d\u2019utilisateur GitHub et un jeton d\u2019acc\u00e8s personnel comme mot de passe.","title":"Configuration"},{"location":"GitHubCommencer/#comment-valider-des-modifications_1","text":"Lorsque vous \u00eates pr\u00eat \u00e0 publier vos modifications sur GitHub, cliquez sur l\u2019onglet Git . Ensuite, cliquez sur Commit (valider). Cliquez sur la case \u00e0 cocher pour chacune des modifications que vous souhaitez valider. Saisissez un message de validation d\u00e9crivant bri\u00e8vement vos modifications, puis cliquez sur Commit (valider). Une fen\u00eatre contextuelle s\u2019affichera pour confirmer que votre validation a fonctionn\u00e9. Cliquez sur Close (fermer). Cliquez sur le bouton Push (acheminer) pour t\u00e9l\u00e9charger vos modifications sur GitHub. Une fen\u00eatre contextuelle s\u2019affichera pour confirmer que l\u2019acheminement a fonctionn\u00e9. Vous obtiendrez un message d\u2019erreur si votre copie locale du d\u00e9p\u00f4t n\u2019est pas \u00e0 jour par rapport \u00e0 la version stock\u00e9e sur GitHub. Dans ce cas, cliquez sur Pull , puis Push (r\u00e9cup\u00e9rer, puis acheminer) pour fusionner vos modifications.","title":"Comment valider des modifications"},{"location":"GitHubCommencer/#faq","text":"J\u2019obtiens une erreur lorsque j\u2019essaie d\u2019acheminer les modifications. Pourquoi? Cela peut signifier que votre copie locale du d\u00e9p\u00f4t n\u2019est pas \u00e0 jour par rapport \u00e0 la copie stock\u00e9e sur GitHub. Essayez d\u2019abord de cliquer sur pull (r\u00e9cup\u00e9rer), puis d\u2019acheminer vos modifications. Une pratique exemplaire consiste \u00e0 toujours ex\u00e9cuter une commande de r\u00e9cup\u00e9ration avant de commencer \u00e0 travailler avec un d\u00e9p\u00f4t afin de s\u2019assurer que vous travaillez sur la derni\u00e8re version, et qu\u2019une seule personne peut modifier le m\u00eame fichier en m\u00eame temps. Comment revenir \u00e0 une validation ant\u00e9rieure? Cette op\u00e9ration doit \u00eatre effectu\u00e9e \u00e0 partir de Visual Studio (VS) Code (accessible par une machine virtuelle en nuage), quel que soit l\u2019endroit o\u00f9 vous utilisez principalement Git. Dans VS Code, suivez les \u00e9tapes pour cloner un d\u00e9p\u00f4t si vous ne l\u2019avez pas d\u00e9j\u00e0 fait. Ouvrez un terminal (cliquez sur Terminal , puis New Terminal [terminal, puis nouveau terminal]). Dans la fen\u00eatre du terminal, tapez git revert HEAD~n --no-edit et appuyez sur Entr\u00e9e (remplacez n par le nombre de validations que vous souhaitez r\u00e9tablir, en commen\u00e7ant par 0). Remarque : vous pouvez trouver l\u2019historique de vos validations sur GitHub en cliquant sur x commits (x validations) en haut \u00e0 droite de votre d\u00e9p\u00f4t. S\u2019il y a des conflits de fusion, cliquez sur Accept incoming change (accepter la modification entrante), puis validez et acheminez les modifications comme d\u2019habitude (voir les instructions d\u00e9taill\u00e9es sur la fa\u00e7on de le faire dans VS Code).","title":"FAQ"},{"location":"GitHubConfiguration/","text":"GitHub.com est une plateforme en ligne utilis\u00e9e \u00e0 des fins de collaboration ainsi que pour assurer le suivi des modifications et des versions pour divers types de projets Important : Ne stockez pas de donn\u00e9es class\u00e9es Prot\u00e9g\u00e9 B sur GitHub.com Cr\u00e9ation d\u2019un compte GitHub Des renseignements sur la cr\u00e9ation d\u2019un compte GitHub (ou sur l\u2019utilisation de votre compte existant) se trouvent \u00e0 l\u2019adresse: https://digital.statcan.gc.ca/drafts/guides-platforms-github . Azure Data Factory Dans l\u2019onglet Manage (g\u00e9rer), cliquez sur Git configuration (configuration de Git). Cliquez sur Configure (configurer). Sous Repository type (type de d\u00e9p\u00f4t), s\u00e9lectionnez GitHub , puis entrez le nom d\u2019utilisateur de votre compte GitHub. Cliquez sur Continue (continuer). Une fen\u00eatre contextuelle s\u2019affiche. Cliquez sur Authorize Azure Data Factory (autoriser Azure Data Factory), puis saisissez le mot de passe de votre compte GitHub. Configurez un d\u00e9p\u00f4t. Vous pouvez soit s\u00e9lectionner un d\u00e9p\u00f4t dont vous \u00eates propri\u00e9taire, soit saisir un lien vers un d\u00e9p\u00f4t. Pr\u00e9cisez tout param\u00e8tre suppl\u00e9mentaire, puis cliquez sur Apply (appliquer). D\u00e9finissez votre division de travail. Vous pouvez soit cr\u00e9er une nouvelle division, soit utiliser une division existante. Cliquez ensuite sur Save (enregistrer). Pour supprimer l\u2019int\u00e9gration GitHub , cliquez sur Disconnect (d\u00e9connecter) \u00e0 l\u2019\u00e9cran Git configuration (configuration de Git). Entrez le nom de l\u2019usine de donn\u00e9es, puis cliquez de nouveau sur Disconnect (d\u00e9connecter) pour confirmer. Azure Databricks Configuration de l\u2019int\u00e9gration de Git Ouvrez User Settings (param\u00e8tres d\u2019utilisateur), puis cliquez sur l\u2019onglet Git integration (int\u00e9gration de Git). Sous Git provider (fournisseur Git), s\u00e9lectionnez GitHub. Entrez votre nom d\u2019utilisateur GitHub. \u00c0 partir de votre compte GitHub, suivez les instructions pour cr\u00e9er un jeton d\u2019acc\u00e8s personnel , ensuring that the repo permission is checked. Copiez le jeton et collez-le dans Databricks . Cliquez sur Save (enregistrer). Ajout d\u2019un d\u00e9p\u00f4t Git Dans l\u2019onglet Repos (d\u00e9p\u00f4ts), cliquez sur Add Repo (ajouter un d\u00e9p\u00f4t). S\u00e9lectionnez Clone remote Git repo (cloner un d\u00e9p\u00f4t Git \u00e0 distance), entrez l\u2019adresse (URL) de la page Web de votre d\u00e9p\u00f4t GitHub. Le fournisseur Git et le nom du d\u00e9p\u00f4t devraient s\u2019afficher automatiquement. Cliquez sur Create (cr\u00e9er). Machines virtuelles de l\u2019environnement d\u2019analyse collaborative Visual Studio Code Pour apprendre \u00e0 utiliser GitHub avec Visual Studio Code, consultez la documentation GitHub - Getting Started . R-Studio Dans le menu File (fichier), cliquez sur New Project... (nouveau projet), puis s\u00e9lectionnez Version Control (contr\u00f4le de version). S\u00e9lectionnez Git . Saisissez l\u2019URL du d\u00e9p\u00f4t GitHub que vous souhaitez cloner, choisissez un dossier sur votre machine virtuelle (MV) o\u00f9 seront enregistr\u00e9s les fichiers locaux, puis cliquez sur Create Project (cr\u00e9er un projet). Azure Machine Learning Cr\u00e9ez une instance de calcul, puis ouvrez un terminal. Dans la fen\u00eatre du terminal , entrez ce qui suit (remplacez l\u2019exemple de courriel par le v\u00f4tre) : ssh-keygen -t rsa -b 4096 -C \"pr\u00e9nom.nom@canada.ca\" Appuyez sur Enter (entrer) jusqu\u2019\u00e0 ce que votre cl\u00e9 soit g\u00e9n\u00e9r\u00e9e. Entrez dans le terminal : cat ~/.ssh/id_rsa.pub. S\u00e9lectionnez l\u2019extrant et copiez-le dans le presse-papiers. Allez dans les param\u00e8tres de votre compte GitHub (sur GitHub.com), cliquez sur SSH and GPG keys (cl\u00e9s SSH et GPG), puis sur New SSH key (nouvelle cl\u00e9 SSH). Collez la cl\u00e9 que vous venez de copier, puis cliquez sur Add SSH key (ajouter une cl\u00e9 SSH). Dans la fen\u00eatre du terminal, tapez : git clone [url] (remplacez [url] par l\u2019url SSH de votre d\u00e9p\u00f4t GitHub, par exemple, git@github.com:nom d\u2019utilisateur/nom du d\u00e9p\u00f4t.git). Lorsque vous y \u00eates invit\u00e9, tapez yes (oui). Microsoft Documentation Int\u00e9gration de Git pour Azure Machine Learning Azure Synapse Dans l\u2019onglet Manage (g\u00e9rer), cliquez sur Git configuration (configuration de Git). Cliquez sur Configure (configurer). Sous Repository type (type de d\u00e9p\u00f4t), s\u00e9lectionnez GitHub , puis entrez le nom d\u2019utilisateur de votre compte GitHub. Cliquez sur Continue (continuer). Une fen\u00eatre contextuelle s\u2019affiche. Entrez les renseignements de connexion de votre compte GitHub, puis cliquez sur Authorize Azure Synapse (autoriser Azure Synapse). Configurez un d\u00e9p\u00f4t. Vous pouvez soit s\u00e9lectionner un d\u00e9p\u00f4t dont vous \u00eates propri\u00e9taire, soit saisir un lien vers un d\u00e9p\u00f4t. Pr\u00e9cisez tout param\u00e8tre suppl\u00e9mentaire, puis cliquez sur Apply (appliquer). D\u00e9finissez votre division de travail. Vous pouvez soit cr\u00e9er une nouvelle division, soit utiliser une division existante. Cliquez ensuite sur Save (enregistrer). Pour supprimer l\u2019int\u00e9gration de GitHub : \u00e0 l\u2019\u00e9cran de configuration de Git, cliquez sur Disconnect (d\u00e9connecter). Entrez le nom de l\u2019espace de travail, puis cliquez de nouveau sur Disconnect (d\u00e9connecter) pour confirmer.","title":"Configuration"},{"location":"GitHubConfiguration/#creation-dun-compte-github","text":"Des renseignements sur la cr\u00e9ation d\u2019un compte GitHub (ou sur l\u2019utilisation de votre compte existant) se trouvent \u00e0 l\u2019adresse: https://digital.statcan.gc.ca/drafts/guides-platforms-github .","title":"Cr\u00e9ation d\u2019un compte GitHub"},{"location":"GitHubConfiguration/#azure-data-factory","text":"Dans l\u2019onglet Manage (g\u00e9rer), cliquez sur Git configuration (configuration de Git). Cliquez sur Configure (configurer). Sous Repository type (type de d\u00e9p\u00f4t), s\u00e9lectionnez GitHub , puis entrez le nom d\u2019utilisateur de votre compte GitHub. Cliquez sur Continue (continuer). Une fen\u00eatre contextuelle s\u2019affiche. Cliquez sur Authorize Azure Data Factory (autoriser Azure Data Factory), puis saisissez le mot de passe de votre compte GitHub. Configurez un d\u00e9p\u00f4t. Vous pouvez soit s\u00e9lectionner un d\u00e9p\u00f4t dont vous \u00eates propri\u00e9taire, soit saisir un lien vers un d\u00e9p\u00f4t. Pr\u00e9cisez tout param\u00e8tre suppl\u00e9mentaire, puis cliquez sur Apply (appliquer). D\u00e9finissez votre division de travail. Vous pouvez soit cr\u00e9er une nouvelle division, soit utiliser une division existante. Cliquez ensuite sur Save (enregistrer). Pour supprimer l\u2019int\u00e9gration GitHub , cliquez sur Disconnect (d\u00e9connecter) \u00e0 l\u2019\u00e9cran Git configuration (configuration de Git). Entrez le nom de l\u2019usine de donn\u00e9es, puis cliquez de nouveau sur Disconnect (d\u00e9connecter) pour confirmer.","title":"Azure Data Factory"},{"location":"GitHubConfiguration/#azure-databricks","text":"","title":"Azure Databricks"},{"location":"GitHubConfiguration/#configuration-de-lintegration-de-git","text":"Ouvrez User Settings (param\u00e8tres d\u2019utilisateur), puis cliquez sur l\u2019onglet Git integration (int\u00e9gration de Git). Sous Git provider (fournisseur Git), s\u00e9lectionnez GitHub. Entrez votre nom d\u2019utilisateur GitHub. \u00c0 partir de votre compte GitHub, suivez les instructions pour cr\u00e9er un jeton d\u2019acc\u00e8s personnel , ensuring that the repo permission is checked. Copiez le jeton et collez-le dans Databricks . Cliquez sur Save (enregistrer).","title":"Configuration de l\u2019int\u00e9gration de Git"},{"location":"GitHubConfiguration/#ajout-dun-depot-git","text":"Dans l\u2019onglet Repos (d\u00e9p\u00f4ts), cliquez sur Add Repo (ajouter un d\u00e9p\u00f4t). S\u00e9lectionnez Clone remote Git repo (cloner un d\u00e9p\u00f4t Git \u00e0 distance), entrez l\u2019adresse (URL) de la page Web de votre d\u00e9p\u00f4t GitHub. Le fournisseur Git et le nom du d\u00e9p\u00f4t devraient s\u2019afficher automatiquement. Cliquez sur Create (cr\u00e9er).","title":"Ajout d\u2019un d\u00e9p\u00f4t Git"},{"location":"GitHubConfiguration/#machines-virtuelles-de-lenvironnement-danalyse-collaborative","text":"","title":"Machines virtuelles de l\u2019environnement d\u2019analyse collaborative"},{"location":"GitHubConfiguration/#visual-studio-code","text":"Pour apprendre \u00e0 utiliser GitHub avec Visual Studio Code, consultez la documentation GitHub - Getting Started .","title":"Visual Studio Code"},{"location":"GitHubConfiguration/#r-studio","text":"Dans le menu File (fichier), cliquez sur New Project... (nouveau projet), puis s\u00e9lectionnez Version Control (contr\u00f4le de version). S\u00e9lectionnez Git . Saisissez l\u2019URL du d\u00e9p\u00f4t GitHub que vous souhaitez cloner, choisissez un dossier sur votre machine virtuelle (MV) o\u00f9 seront enregistr\u00e9s les fichiers locaux, puis cliquez sur Create Project (cr\u00e9er un projet).","title":"R-Studio"},{"location":"GitHubConfiguration/#azure-machine-learning","text":"Cr\u00e9ez une instance de calcul, puis ouvrez un terminal. Dans la fen\u00eatre du terminal , entrez ce qui suit (remplacez l\u2019exemple de courriel par le v\u00f4tre) : ssh-keygen -t rsa -b 4096 -C \"pr\u00e9nom.nom@canada.ca\" Appuyez sur Enter (entrer) jusqu\u2019\u00e0 ce que votre cl\u00e9 soit g\u00e9n\u00e9r\u00e9e. Entrez dans le terminal : cat ~/.ssh/id_rsa.pub. S\u00e9lectionnez l\u2019extrant et copiez-le dans le presse-papiers. Allez dans les param\u00e8tres de votre compte GitHub (sur GitHub.com), cliquez sur SSH and GPG keys (cl\u00e9s SSH et GPG), puis sur New SSH key (nouvelle cl\u00e9 SSH). Collez la cl\u00e9 que vous venez de copier, puis cliquez sur Add SSH key (ajouter une cl\u00e9 SSH). Dans la fen\u00eatre du terminal, tapez : git clone [url] (remplacez [url] par l\u2019url SSH de votre d\u00e9p\u00f4t GitHub, par exemple, git@github.com:nom d\u2019utilisateur/nom du d\u00e9p\u00f4t.git). Lorsque vous y \u00eates invit\u00e9, tapez yes (oui).","title":"Azure Machine Learning"},{"location":"GitHubConfiguration/#microsoft-documentation","text":"Int\u00e9gration de Git pour Azure Machine Learning","title":"Microsoft Documentation"},{"location":"GitHubConfiguration/#azure-synapse","text":"Dans l\u2019onglet Manage (g\u00e9rer), cliquez sur Git configuration (configuration de Git). Cliquez sur Configure (configurer). Sous Repository type (type de d\u00e9p\u00f4t), s\u00e9lectionnez GitHub , puis entrez le nom d\u2019utilisateur de votre compte GitHub. Cliquez sur Continue (continuer). Une fen\u00eatre contextuelle s\u2019affiche. Entrez les renseignements de connexion de votre compte GitHub, puis cliquez sur Authorize Azure Synapse (autoriser Azure Synapse). Configurez un d\u00e9p\u00f4t. Vous pouvez soit s\u00e9lectionner un d\u00e9p\u00f4t dont vous \u00eates propri\u00e9taire, soit saisir un lien vers un d\u00e9p\u00f4t. Pr\u00e9cisez tout param\u00e8tre suppl\u00e9mentaire, puis cliquez sur Apply (appliquer). D\u00e9finissez votre division de travail. Vous pouvez soit cr\u00e9er une nouvelle division, soit utiliser une division existante. Cliquez ensuite sur Save (enregistrer). Pour supprimer l\u2019int\u00e9gration de GitHub : \u00e0 l\u2019\u00e9cran de configuration de Git, cliquez sur Disconnect (d\u00e9connecter). Entrez le nom de l\u2019espace de travail, puis cliquez de nouveau sur Disconnect (d\u00e9connecter) pour confirmer.","title":"Azure Synapse"},{"location":"InscrivezProjet/","text":"Inscrivez-vous ici pour faire partie de la Communaut\u00e9 des premiers utilisateurs de l'Analyse des donn\u00e9es en tant que service (ADS): https://forms.office.com/r/ErV7YkMPBF Cliquez sur le bouton \u00abFran\u00e7ais (Canada)\u00bb du formulaire pour changer la langue en fran\u00e7ais. Si vous avez d\u00e9j\u00e0 obtenu l'acc\u00e8s \u00e0 la plateforme, consultez les directives pour savoir comment ouvrir une session .","title":"Inscrivez votre projet"},{"location":"Langue/","text":"Le document d\u00e9crit la fa\u00e7on de changer la langue dans les diff\u00e9rents services offerts. Portail Azure Pour modifier les param\u00e8tres de langue du portail Azure, voici les \u00e9tapes \u00e0 suivre: Cliquez sur le menu Param\u00e8tres dans l'en-t\u00eate de la page principale. Cliquez sur l'onglet Langue et r\u00e9gion . Utilisez les menus d\u00e9roulants pour choisir la langue pr\u00e9f\u00e9r\u00e9e et le format r\u00e9gional appropri\u00e9. Cliquez sur le bouton Appliquer pour mettre \u00e0 jour vos param\u00e8tres de langue et de format r\u00e9gional. Tableau de bord Pour acc\u00e9der au tableau de bord en fran\u00e7ais de l\u2019environnement d\u2019Analyse collaborative (EAC), voici les \u00e9tapes \u00e0 suivre: \u00c0 partir de la liste des tableaux de bords, cliquez sur la fl\u00e8che correspondant au nom du tableau de bord. S\u00e9lectionnez le tableau de bord Environnement d\u2019Analyse Collaborative dans la liste affich\u00e9e. Note: Si ce tableau de bord n'est pas dans la liste, cliquez sur Parcourir tous les tableaux de bord pour acc\u00e9der \u00e0 la liste compl\u00e8te des tableaux de bord. Machines Virtuelles Serveur Windows Pour configurer la langue d'affichage dans une machine virtuelle Windows, voici les \u00e9tapes \u00e0 suivre: Selectionnez Param\u00e8tres . S\u00e9lectionnez Heure et langue . S\u00e9lectionnez Langue . Utilisez le menu d\u00e9roulant sous l'en-t\u00eate Langue d\u2019affichage de Windows pour choisir la langue d\u00e9sir\u00e9e. La zone de langue d'affichage de Windows doit maintenant comprendre la langue choisie. Pour \u00eatre en mesure d'appliquer la nouvelle langue, d\u00e9connectez-vous de la session Windows actuelle, puis reconnectez-vous. Serveur Ubuntu Si vous utilisez l'application X2GO pour acc\u00e9der \u00e0 l'interface graphique de votre machine Ubuntu, notez que par d\u00e9faut la session est disponible en anglais uniquement. Il sera donc necessaire d'installer des modules supplementaires de langue manuellement. Azure Apprentissage automatique Pour modifier les param\u00e8tres de langue dans l\u2019espace de travail d'apprentissage automatique Microsoft Azure, voici les \u00e9tapes \u00e0 suivre: Cliquez sur le menu Param\u00e8tres dans l'en-t\u00eate de la page principale. Sous Langue et formats , utilisez les menus d\u00e9roulants pour choisir la langue pr\u00e9f\u00e9r\u00e9e et le format r\u00e9gional appropri\u00e9. Cliquez sur le bouton Appliquer pour mettre \u00e0 jour vos param\u00e8tres de langue et de format r\u00e9gional. Azure Apprentissage automatique - Jupyter Lab Ex\u00e9cutez dans le terminal d'instance de calcul Azure ML: sh pip install jupyterlab==3 Red\u00e9marrez l'instance de calcul Ex\u00e9cutez dans le terminal d'instance de calcul Azure ML: sh pip install git+https://github.com/StatCan/jupyterlab-language-pack-fr_FR Dans JupyterLab, s\u00e9lectionnez Param\u00e8tres - Langue - Fran\u00e7ais Slack Pour modifier les param\u00e8tres de langue dans l\u2019application Slack, voici les \u00e9tapes \u00e0 suivre: 1. Cliquez sur l\u2019 ic\u00f4ne de profil dans l'en-t\u00eate de la page principale. Cliquez sur Pr\u00e9f\u00e9rences . S\u00e9lectionnez l\u2019onglet Langue et r\u00e9gion . Sous Langue , utilisez le menu d\u00e9roulant pour choisir la langue pr\u00e9f\u00e9r\u00e9e. Fermez la fen\u00eatre Pr\u00e9f\u00e9rences . Explorateur de stockage Microsoft Azure Par d\u00e9faut, cette application d\u00e9termine la langue d'utilisation en fonction des pr\u00e9f\u00e9rences linguistiques d\u00e9finies sur votre ordinateur. Pour modifier les param\u00e8tres de langue dans l\u2019Explorateur de stockage Microsoft Azure, voici les \u00e9tapes \u00e0 suivre: Cliquez sur Modifier . Cliquez sur Param\u00e8tres . Dans la page des param\u00e8tres, s\u00e9lectionnez Application . Sous Param\u00e8tres r\u00e9gionaux , utilisez le menu d\u00e9roulant pour choisir votre langue pr\u00e9f\u00e9r\u00e9e. Pour appliquer la nouvelle langue, fermez puis relancer l\u2019application. Power BI Pour obtenir plus de renseignements, vous pouvez consulter l\u2019article \u00ab Langues et pays/r\u00e9gions pris en charge pour Power BI \u00bb. Service Power BI Par d\u00e9faut, le service Power BI d\u00e9termine la langue d\u2019utilisation en fonction des pr\u00e9f\u00e9rences linguistiques d\u00e9finies sur votre ordinateur. Les \u00e9tapes \u00e0 suivre pour afficher et modifier ces pr\u00e9f\u00e9rences peuvent varier selon votre syst\u00e8me d\u2019exploitation et votre fureteur. Pour changer la langue du menu dans le service Power BI, voici les \u00e9tapes \u00e0 suivre: Dans le service Power BI, cliquez sur l\u2019 ic\u00f4ne Param\u00e8tres , puis s\u00e9lectionnez Param\u00e8tres . Sous l\u2019onglet G\u00e9n\u00e9ral , s\u00e9lectionnez Langue . S\u00e9lectionnez votre langue pr\u00e9f\u00e9r\u00e9e, puis cliquez sur Appliquer . Pour obtenir plus de renseignements, consultez l\u2019article \u00ab Langues disponibles pour le service Power BI \u00bb. Power BI Desktop Par d\u00e9faut, - la Langue de l'application est fond\u00e9e sur la Langue de Windows - la Langue du mod\u00e8le est fond\u00e9e sur la Langue de l'application - les \u00e9tapes de la requ\u00eate sont fond\u00e9es sur la Langue de l'application Il est recommand\u00e9 de fixer la langue du mod\u00e8le \u00e0 English (United States) . La langue du mod\u00e8le s'applique seulement au moment de la cr\u00e9ation du rapport et ne peut pas \u00eatre chang\u00e9e dans les rapports existants. Ainsi, il est recommand\u00e9 de fixer la langue du mod\u00e8le \u00e0 English (United States) , sauf si vous avez besoin d\u2019utiliser une autre langue pour le mod\u00e8le de rapport. Les comparaisons de cha\u00eene de caract\u00e8res et les champs de date internes sont affect\u00e9s par cette configuration. Pour changer la langue du menu et la langue du mod\u00e8le dans Power BI Desktop, voici les \u00e9tapes \u00e0 suivre: Ouvrez le menu Options . Sous GLOBAL , cliquez sur Param\u00e8tres r\u00e9gionaux , et fixez la langue de l\u2019application et la langue du mod\u00e8le \u00e0 la langue souhait\u00e9e. NOTE : La langue d'importation des donn\u00e9es est fix\u00e9e s\u00e9par\u00e9ment dans les Param\u00e8tres r\u00e9gionaux de la section FICHIER ACTIF . Vous devez la changer seulement si vous importez des fichiers de donn\u00e9es comportant des nombres ou des dates suivant des param\u00e8tres r\u00e9gionaux pr\u00e9cis (p. ex. le format de date JJ/MM/AAAA de l\u2019anglais du Canada et le format de date MM/JJ/AAAA de l\u2019anglais des \u00c9tats-Unis). Azure Databricks Pour modifier les param\u00e8tres de langue dans Databricks: S\u00e9lectionnez le menu d\u00e9roulant de l'utilisateur en haut \u00e0 droite et s\u00e9lectionnez Param\u00e8tres utilisateur . Sur la page, s\u00e9lectionnez Param\u00e8tres de langue . Cliquez sur la liste d\u00e9roulante et s\u00e9lectionnez la langue de votre choix. Azure Data Factory Pour configurer la langue, voici les \u00e9tapes \u00e0 suivre: Dans Azure Data Factory, s\u00e9lectionnez Param\u00e8tres . S\u00e9lectionnez Fran\u00e7ais . Cliquez sur Appliquer . JupyterLab Pour modifier les param\u00e8tres de langue dans JupyterLab: Dans JupyterLab, ouvrez une console ou un terminal. Installez l'exemple de langue de votre choix \u00e0 l'aide de pip. Exemple: python pip install jupyterlab-language-pack-fr-FR Sous settings , mettez en surbrillance Language et s\u00e9lectionnez la langue que vous avez install\u00e9e. Cliquez sur OK pour rafra\u00eechir la page, vous verrez le changement de langue. Pour plus d'informations sur le changement de langue: https://jupyterlab.readthedocs.io/en/stable/user/language.html#changing-the-display-language Visual Studio Code (VSCode) Pour changer la langue d'affichage dans VSCode: Ouvrez VSCode et ouvrez la palette de commandes (Ctrl+Maj+P). Dans la palette de commandes, tapez \"affichage\" et s\u00e9lectionnez installer des langues suppl\u00e9mentaires Remarque : si vous avez d\u00e9j\u00e0 install\u00e9 la langue souhait\u00e9e, vous pouvez la s\u00e9lectionner dans la liste d\u00e9roulante. Sur le c\u00f4t\u00e9 gauche de VSCode, les langues qui peuvent \u00eatre install\u00e9es appara\u00eetront, s\u00e9lectionnez la langue de votre choix. Une fen\u00eatre contextuelle peut appara\u00eetre en bas \u00e0 droite de l'\u00e9cran dans laquelle vous pouvez changer la langue et red\u00e9marrera VSCode. Visual Studio Si vous avez d\u00e9j\u00e0 install\u00e9 des packages de langue dans Visual Studio: Dans la barre sup\u00e9rieure, s\u00e9lectionnez Outils puis Options . Dans le menu, sous l'onglet Environnement , s\u00e9lectionnez Param\u00e8tres internationaux Dans le menu d\u00e9roulant sous Langue , s\u00e9lectionnez la langue de votre choix. Si vous n'avez pas install\u00e9 d'autres packages de langue dans Visual Studio : Sur votre ordinateur, ouvrez le programme d'installation de Visual Studio. Dans le programme d'installation, s\u00e9lectionnez le bouton Modifier. Dans la nouvelle fen\u00eatre, s\u00e9lectionnez Packs linguistiques . S\u00e9lectionnez toutes les langues que vous souhaitez ajouter, puis s\u00e9lectionnez modifier. \u00c0 partir de l\u00e0, vous pouvez suivre les \u00e9tapes d'utilisation des packages de langue install\u00e9s dans Visual Studio. Pour configurer RStudio dans une autre langue : Ouvrez RStudio et ouvrez la console. Dans la console, tapez \"Sys.getenv(LANGUAGE = \"fr\") Note : \"fr\" est pour la langue fran\u00e7aise, pour une liste d'autres langues qui peuvent \u00eatre utilis\u00e9es : https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes Pour le tester, vous pouvez taper \"2+x\" et cela devrait donner une erreur dans la langue saisie. Fureteurs Web Chrome Safari Edge Firefox Opera (en anglais seulement)","title":"Langue"},{"location":"Langue/#portail-azure","text":"Pour modifier les param\u00e8tres de langue du portail Azure, voici les \u00e9tapes \u00e0 suivre: Cliquez sur le menu Param\u00e8tres dans l'en-t\u00eate de la page principale. Cliquez sur l'onglet Langue et r\u00e9gion . Utilisez les menus d\u00e9roulants pour choisir la langue pr\u00e9f\u00e9r\u00e9e et le format r\u00e9gional appropri\u00e9. Cliquez sur le bouton Appliquer pour mettre \u00e0 jour vos param\u00e8tres de langue et de format r\u00e9gional.","title":"Portail Azure"},{"location":"Langue/#tableau-de-bord","text":"Pour acc\u00e9der au tableau de bord en fran\u00e7ais de l\u2019environnement d\u2019Analyse collaborative (EAC), voici les \u00e9tapes \u00e0 suivre: \u00c0 partir de la liste des tableaux de bords, cliquez sur la fl\u00e8che correspondant au nom du tableau de bord. S\u00e9lectionnez le tableau de bord Environnement d\u2019Analyse Collaborative dans la liste affich\u00e9e. Note: Si ce tableau de bord n'est pas dans la liste, cliquez sur Parcourir tous les tableaux de bord pour acc\u00e9der \u00e0 la liste compl\u00e8te des tableaux de bord.","title":"Tableau de bord"},{"location":"Langue/#machines-virtuelles","text":"","title":"Machines Virtuelles"},{"location":"Langue/#serveur-windows","text":"Pour configurer la langue d'affichage dans une machine virtuelle Windows, voici les \u00e9tapes \u00e0 suivre: Selectionnez Param\u00e8tres . S\u00e9lectionnez Heure et langue . S\u00e9lectionnez Langue . Utilisez le menu d\u00e9roulant sous l'en-t\u00eate Langue d\u2019affichage de Windows pour choisir la langue d\u00e9sir\u00e9e. La zone de langue d'affichage de Windows doit maintenant comprendre la langue choisie. Pour \u00eatre en mesure d'appliquer la nouvelle langue, d\u00e9connectez-vous de la session Windows actuelle, puis reconnectez-vous.","title":"Serveur Windows"},{"location":"Langue/#serveur-ubuntu","text":"Si vous utilisez l'application X2GO pour acc\u00e9der \u00e0 l'interface graphique de votre machine Ubuntu, notez que par d\u00e9faut la session est disponible en anglais uniquement. Il sera donc necessaire d'installer des modules supplementaires de langue manuellement.","title":"Serveur Ubuntu"},{"location":"Langue/#azure-apprentissage-automatique","text":"Pour modifier les param\u00e8tres de langue dans l\u2019espace de travail d'apprentissage automatique Microsoft Azure, voici les \u00e9tapes \u00e0 suivre: Cliquez sur le menu Param\u00e8tres dans l'en-t\u00eate de la page principale. Sous Langue et formats , utilisez les menus d\u00e9roulants pour choisir la langue pr\u00e9f\u00e9r\u00e9e et le format r\u00e9gional appropri\u00e9. Cliquez sur le bouton Appliquer pour mettre \u00e0 jour vos param\u00e8tres de langue et de format r\u00e9gional.","title":"Azure Apprentissage automatique"},{"location":"Langue/#azure-apprentissage-automatique-jupyter-lab","text":"Ex\u00e9cutez dans le terminal d'instance de calcul Azure ML: sh pip install jupyterlab==3 Red\u00e9marrez l'instance de calcul Ex\u00e9cutez dans le terminal d'instance de calcul Azure ML: sh pip install git+https://github.com/StatCan/jupyterlab-language-pack-fr_FR Dans JupyterLab, s\u00e9lectionnez Param\u00e8tres - Langue - Fran\u00e7ais","title":"Azure Apprentissage automatique - Jupyter Lab"},{"location":"Langue/#slack","text":"Pour modifier les param\u00e8tres de langue dans l\u2019application Slack, voici les \u00e9tapes \u00e0 suivre: 1. Cliquez sur l\u2019 ic\u00f4ne de profil dans l'en-t\u00eate de la page principale. Cliquez sur Pr\u00e9f\u00e9rences . S\u00e9lectionnez l\u2019onglet Langue et r\u00e9gion . Sous Langue , utilisez le menu d\u00e9roulant pour choisir la langue pr\u00e9f\u00e9r\u00e9e. Fermez la fen\u00eatre Pr\u00e9f\u00e9rences .","title":"Slack"},{"location":"Langue/#explorateur-de-stockage-microsoft-azure","text":"Par d\u00e9faut, cette application d\u00e9termine la langue d'utilisation en fonction des pr\u00e9f\u00e9rences linguistiques d\u00e9finies sur votre ordinateur. Pour modifier les param\u00e8tres de langue dans l\u2019Explorateur de stockage Microsoft Azure, voici les \u00e9tapes \u00e0 suivre: Cliquez sur Modifier . Cliquez sur Param\u00e8tres . Dans la page des param\u00e8tres, s\u00e9lectionnez Application . Sous Param\u00e8tres r\u00e9gionaux , utilisez le menu d\u00e9roulant pour choisir votre langue pr\u00e9f\u00e9r\u00e9e. Pour appliquer la nouvelle langue, fermez puis relancer l\u2019application.","title":"Explorateur de stockage Microsoft Azure"},{"location":"Langue/#power-bi","text":"Pour obtenir plus de renseignements, vous pouvez consulter l\u2019article \u00ab Langues et pays/r\u00e9gions pris en charge pour Power BI \u00bb.","title":"Power BI"},{"location":"Langue/#service-power-bi","text":"Par d\u00e9faut, le service Power BI d\u00e9termine la langue d\u2019utilisation en fonction des pr\u00e9f\u00e9rences linguistiques d\u00e9finies sur votre ordinateur. Les \u00e9tapes \u00e0 suivre pour afficher et modifier ces pr\u00e9f\u00e9rences peuvent varier selon votre syst\u00e8me d\u2019exploitation et votre fureteur. Pour changer la langue du menu dans le service Power BI, voici les \u00e9tapes \u00e0 suivre: Dans le service Power BI, cliquez sur l\u2019 ic\u00f4ne Param\u00e8tres , puis s\u00e9lectionnez Param\u00e8tres . Sous l\u2019onglet G\u00e9n\u00e9ral , s\u00e9lectionnez Langue . S\u00e9lectionnez votre langue pr\u00e9f\u00e9r\u00e9e, puis cliquez sur Appliquer . Pour obtenir plus de renseignements, consultez l\u2019article \u00ab Langues disponibles pour le service Power BI \u00bb.","title":"Service Power BI"},{"location":"Langue/#power-bi-desktop","text":"Par d\u00e9faut, - la Langue de l'application est fond\u00e9e sur la Langue de Windows - la Langue du mod\u00e8le est fond\u00e9e sur la Langue de l'application - les \u00e9tapes de la requ\u00eate sont fond\u00e9es sur la Langue de l'application Il est recommand\u00e9 de fixer la langue du mod\u00e8le \u00e0 English (United States) . La langue du mod\u00e8le s'applique seulement au moment de la cr\u00e9ation du rapport et ne peut pas \u00eatre chang\u00e9e dans les rapports existants. Ainsi, il est recommand\u00e9 de fixer la langue du mod\u00e8le \u00e0 English (United States) , sauf si vous avez besoin d\u2019utiliser une autre langue pour le mod\u00e8le de rapport. Les comparaisons de cha\u00eene de caract\u00e8res et les champs de date internes sont affect\u00e9s par cette configuration. Pour changer la langue du menu et la langue du mod\u00e8le dans Power BI Desktop, voici les \u00e9tapes \u00e0 suivre: Ouvrez le menu Options . Sous GLOBAL , cliquez sur Param\u00e8tres r\u00e9gionaux , et fixez la langue de l\u2019application et la langue du mod\u00e8le \u00e0 la langue souhait\u00e9e. NOTE : La langue d'importation des donn\u00e9es est fix\u00e9e s\u00e9par\u00e9ment dans les Param\u00e8tres r\u00e9gionaux de la section FICHIER ACTIF . Vous devez la changer seulement si vous importez des fichiers de donn\u00e9es comportant des nombres ou des dates suivant des param\u00e8tres r\u00e9gionaux pr\u00e9cis (p. ex. le format de date JJ/MM/AAAA de l\u2019anglais du Canada et le format de date MM/JJ/AAAA de l\u2019anglais des \u00c9tats-Unis).","title":"Power BI Desktop"},{"location":"Langue/#azure-databricks","text":"Pour modifier les param\u00e8tres de langue dans Databricks: S\u00e9lectionnez le menu d\u00e9roulant de l'utilisateur en haut \u00e0 droite et s\u00e9lectionnez Param\u00e8tres utilisateur . Sur la page, s\u00e9lectionnez Param\u00e8tres de langue . Cliquez sur la liste d\u00e9roulante et s\u00e9lectionnez la langue de votre choix.","title":"Azure Databricks"},{"location":"Langue/#azure-data-factory","text":"Pour configurer la langue, voici les \u00e9tapes \u00e0 suivre: Dans Azure Data Factory, s\u00e9lectionnez Param\u00e8tres . S\u00e9lectionnez Fran\u00e7ais . Cliquez sur Appliquer .","title":"Azure Data Factory"},{"location":"Langue/#jupyterlab","text":"Pour modifier les param\u00e8tres de langue dans JupyterLab: Dans JupyterLab, ouvrez une console ou un terminal. Installez l'exemple de langue de votre choix \u00e0 l'aide de pip. Exemple: python pip install jupyterlab-language-pack-fr-FR Sous settings , mettez en surbrillance Language et s\u00e9lectionnez la langue que vous avez install\u00e9e. Cliquez sur OK pour rafra\u00eechir la page, vous verrez le changement de langue. Pour plus d'informations sur le changement de langue: https://jupyterlab.readthedocs.io/en/stable/user/language.html#changing-the-display-language","title":"JupyterLab"},{"location":"Langue/#visual-studio-code-vscode","text":"Pour changer la langue d'affichage dans VSCode: Ouvrez VSCode et ouvrez la palette de commandes (Ctrl+Maj+P). Dans la palette de commandes, tapez \"affichage\" et s\u00e9lectionnez installer des langues suppl\u00e9mentaires Remarque : si vous avez d\u00e9j\u00e0 install\u00e9 la langue souhait\u00e9e, vous pouvez la s\u00e9lectionner dans la liste d\u00e9roulante. Sur le c\u00f4t\u00e9 gauche de VSCode, les langues qui peuvent \u00eatre install\u00e9es appara\u00eetront, s\u00e9lectionnez la langue de votre choix. Une fen\u00eatre contextuelle peut appara\u00eetre en bas \u00e0 droite de l'\u00e9cran dans laquelle vous pouvez changer la langue et red\u00e9marrera VSCode.","title":"Visual Studio Code (VSCode)"},{"location":"Langue/#visual-studio","text":"Si vous avez d\u00e9j\u00e0 install\u00e9 des packages de langue dans Visual Studio: Dans la barre sup\u00e9rieure, s\u00e9lectionnez Outils puis Options . Dans le menu, sous l'onglet Environnement , s\u00e9lectionnez Param\u00e8tres internationaux Dans le menu d\u00e9roulant sous Langue , s\u00e9lectionnez la langue de votre choix. Si vous n'avez pas install\u00e9 d'autres packages de langue dans Visual Studio : Sur votre ordinateur, ouvrez le programme d'installation de Visual Studio. Dans le programme d'installation, s\u00e9lectionnez le bouton Modifier. Dans la nouvelle fen\u00eatre, s\u00e9lectionnez Packs linguistiques . S\u00e9lectionnez toutes les langues que vous souhaitez ajouter, puis s\u00e9lectionnez modifier. \u00c0 partir de l\u00e0, vous pouvez suivre les \u00e9tapes d'utilisation des packages de langue install\u00e9s dans Visual Studio. Pour configurer RStudio dans une autre langue : Ouvrez RStudio et ouvrez la console. Dans la console, tapez \"Sys.getenv(LANGUAGE = \"fr\") Note : \"fr\" est pour la langue fran\u00e7aise, pour une liste d'autres langues qui peuvent \u00eatre utilis\u00e9es : https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes Pour le tester, vous pouvez taper \"2+x\" et cela devrait donner une erreur dans la langue saisie.","title":"Visual Studio"},{"location":"Langue/#fureteurs-web","text":"Chrome Safari Edge Firefox Opera (en anglais seulement)","title":"Fureteurs Web"},{"location":"MachinesVirtuelle/","text":"Trouvez votre laboratoire DevTest Dans le portail Azure, cliquez sur le laboratoire DevTest dans le tableau de bord personnalis\u00e9 de votre projet. S\u00e9lectionnez le DevTest Lab qui a \u00e9t\u00e9 attribu\u00e9. Cr\u00e9ez votre machine virtuelle Note : Dans certains cas, une machine virtuelle aura \u00e9t\u00e9 cr\u00e9\u00e9e au pr\u00e9alable pour vous et vous n'aurez pas l'autorisation d'en cr\u00e9er une. Si vous devez apporter des changements \u00e0 votre machine virtuelle, consultez la FAQ . \u00c0 partir de la page Vue d'ensemble du laboratoire DevTest, cliquez sur le bouton + Ajouter . Choisissez une base appropri\u00e9e pour votre machine virtuelle (p. ex. Data Science Virtual Machine - Windows Server 2019). Pour en savoir plus sur le logiciel inclus dans l'image de machine virtuelle pour la science des donn\u00e9es (Data Science Virtual Machine), veuillez cliquer ici . Entrez un nom pour votre machine virtuelle ainsi qu'un nom d'utilisateur et un mot de passe que vous utiliserez pour vous connecter \u00e0 la machine virtuelle. D\u00e9cochez les cases Utiliser un secret enregistr\u00e9 et Enregistrer comme mot de passe par d\u00e9faut . Vous pouvez cliquer sur le lien Changer la taille pour modifier la taille de votre machine virtuelle si vous le souhaitez. Laissez les autres champs par d\u00e9faut et cliquez sur le bouton Cr\u00e9er . Trouvez votre machine virtuelle Faites d\u00e9filer la page Vue d'ensemble du laboratoire DevTest jusqu'\u00e0 ce que vous voyiez votre machine virtuelle sous Mes machines virtuelles . Cliquez sur votre machine virtuelle pour en afficher la Vue d'ensemble . D\u00e9marrez votre machine virtuelle \u00c0 partir de la page Vue d'ensemble de votre machine virtuelle, cliquez sur le bouton D\u00e9marrer . Le demarrage de la machine virtuelle prendra quelques minutes. Surveillez l'avancement du d\u00e9marrage en cliquant sur l'ic\u00f4ne Notifications dans le coin sup\u00e9rieur droit de la fen\u00eatre. Connectez-vous \u00e0 votre machine virtuelle \u00c0 partir de la page Vue d'ensemble de votre machine virtuelle, cliquez sur Connexion du navigateur (si vous ne voyez pas ce bouton, vous devrez peut-\u00eatre cliquer sur le bouton Connecter , puis choisir Bastion dans le menu d\u00e9roulant). Veuillez \u00e0 cocher la case Ouvrir dans une nouvelle fen\u00eatre , puis entrez le nom d'utilisateur et le mot de passe que vous avez utilis\u00e9 au moment de cr\u00e9er votre machine virtuelle, et cliquez sur le bouton Connecter . Votre machine virtuelle devrait s'ouvrir dans un nouvel onglet du navigateur. Note : Par d\u00e9faut, la machine virtuelle Ubuntu s'ouvre en mode Serveur Terminal. Vous pouvez acceder \u00e0 l\u2019interface graphique de votre machine Ubuntu \u00e0 l'aide de l'application X2Go , \u00e0 partir de votre machine Windows. Note : Apr\u00e8s avoir tent\u00e9 de vous connecter pour la premi\u00e8re fois, une erreur peut appara\u00eetre indiquant qu'un bloqueur de fen\u00eatres contextuelles emp\u00eache l'ouverture d'une nouvelle fen\u00eatre. Pour le d\u00e9sactiver, une ic\u00f4ne appara\u00eetra dans la barre de recherche du navigateur, s\u00e9lectionnez le bouton et cliquez sur toujours autoriser . Arr\u00eatez votre machine virtuelle Les machines virtuelles entra\u00eenent des co\u00fbts uniquement quand elles sont en marche. Pour \u00e9viter les d\u00e9penses inutiles, vous devez arr\u00eater votre machine virtuelle lorsque vous ne l'utilisez pas. 1. \u00c0 partir de la page Vue d'ensemble de votre machine virtuelle, cliquez sur le bouton Arr\u00eater .","title":"Machines Virtuelle"},{"location":"MachinesVirtuelle/#trouvez-votre-laboratoire-devtest","text":"Dans le portail Azure, cliquez sur le laboratoire DevTest dans le tableau de bord personnalis\u00e9 de votre projet. S\u00e9lectionnez le DevTest Lab qui a \u00e9t\u00e9 attribu\u00e9.","title":"Trouvez votre laboratoire\u00a0DevTest"},{"location":"MachinesVirtuelle/#creez-votre-machine-virtuelle","text":"Note : Dans certains cas, une machine virtuelle aura \u00e9t\u00e9 cr\u00e9\u00e9e au pr\u00e9alable pour vous et vous n'aurez pas l'autorisation d'en cr\u00e9er une. Si vous devez apporter des changements \u00e0 votre machine virtuelle, consultez la FAQ . \u00c0 partir de la page Vue d'ensemble du laboratoire DevTest, cliquez sur le bouton + Ajouter . Choisissez une base appropri\u00e9e pour votre machine virtuelle (p. ex. Data Science Virtual Machine - Windows Server 2019). Pour en savoir plus sur le logiciel inclus dans l'image de machine virtuelle pour la science des donn\u00e9es (Data Science Virtual Machine), veuillez cliquer ici . Entrez un nom pour votre machine virtuelle ainsi qu'un nom d'utilisateur et un mot de passe que vous utiliserez pour vous connecter \u00e0 la machine virtuelle. D\u00e9cochez les cases Utiliser un secret enregistr\u00e9 et Enregistrer comme mot de passe par d\u00e9faut . Vous pouvez cliquer sur le lien Changer la taille pour modifier la taille de votre machine virtuelle si vous le souhaitez. Laissez les autres champs par d\u00e9faut et cliquez sur le bouton Cr\u00e9er .","title":"Cr\u00e9ez votre machine virtuelle"},{"location":"MachinesVirtuelle/#trouvez-votre-machine-virtuelle","text":"Faites d\u00e9filer la page Vue d'ensemble du laboratoire DevTest jusqu'\u00e0 ce que vous voyiez votre machine virtuelle sous Mes machines virtuelles . Cliquez sur votre machine virtuelle pour en afficher la Vue d'ensemble .","title":"Trouvez votre machine virtuelle"},{"location":"MachinesVirtuelle/#demarrez-votre-machine-virtuelle","text":"\u00c0 partir de la page Vue d'ensemble de votre machine virtuelle, cliquez sur le bouton D\u00e9marrer . Le demarrage de la machine virtuelle prendra quelques minutes. Surveillez l'avancement du d\u00e9marrage en cliquant sur l'ic\u00f4ne Notifications dans le coin sup\u00e9rieur droit de la fen\u00eatre.","title":"D\u00e9marrez votre machine virtuelle"},{"location":"MachinesVirtuelle/#connectez-vous-a-votre-machine-virtuelle","text":"\u00c0 partir de la page Vue d'ensemble de votre machine virtuelle, cliquez sur Connexion du navigateur (si vous ne voyez pas ce bouton, vous devrez peut-\u00eatre cliquer sur le bouton Connecter , puis choisir Bastion dans le menu d\u00e9roulant). Veuillez \u00e0 cocher la case Ouvrir dans une nouvelle fen\u00eatre , puis entrez le nom d'utilisateur et le mot de passe que vous avez utilis\u00e9 au moment de cr\u00e9er votre machine virtuelle, et cliquez sur le bouton Connecter . Votre machine virtuelle devrait s'ouvrir dans un nouvel onglet du navigateur. Note : Par d\u00e9faut, la machine virtuelle Ubuntu s'ouvre en mode Serveur Terminal. Vous pouvez acceder \u00e0 l\u2019interface graphique de votre machine Ubuntu \u00e0 l'aide de l'application X2Go , \u00e0 partir de votre machine Windows. Note : Apr\u00e8s avoir tent\u00e9 de vous connecter pour la premi\u00e8re fois, une erreur peut appara\u00eetre indiquant qu'un bloqueur de fen\u00eatres contextuelles emp\u00eache l'ouverture d'une nouvelle fen\u00eatre. Pour le d\u00e9sactiver, une ic\u00f4ne appara\u00eetra dans la barre de recherche du navigateur, s\u00e9lectionnez le bouton et cliquez sur toujours autoriser .","title":"Connectez-vous \u00e0 votre machine virtuelle"},{"location":"MachinesVirtuelle/#arretez-votre-machine-virtuelle","text":"Les machines virtuelles entra\u00eenent des co\u00fbts uniquement quand elles sont en marche. Pour \u00e9viter les d\u00e9penses inutiles, vous devez arr\u00eater votre machine virtuelle lorsque vous ne l'utilisez pas. 1. \u00c0 partir de la page Vue d'ensemble de votre machine virtuelle, cliquez sur le bouton Arr\u00eater .","title":"Arr\u00eatez votre machine virtuelle"},{"location":"PostgreSQL/","text":"English Acc\u00e8s \u00e0 une base de donn\u00e9es Azure PostgreSQL Azure Data Factory On peut cr\u00e9er un service li\u00e9 dans Azure Data Factory. Le nom d'utilisateur est votre compte cloud ou groupe AD si l'acc\u00e8s a \u00e9t\u00e9 donn\u00e9 \u00e0 un groupe auquel vous appartenez, suivi du nom du serveur. Le mot de passe est un jeton d'acc\u00e8s d'Azure AD tel que d\u00e9crit ci-dessous au point 6 de la section pgAdmin ou Veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal https://cae-eac.slack.com si vous avez besoin d'aide. Machine virtuelle Vous pouvez vous connecter \u00e0 une base de donn\u00e9es Azure PostgreSQL \u00e0 partir de votre machine virtuelle en infonuagique, en utilisant une application telle que: 1. pgAdmin 2. Azure Data Studio 3. Visual Studio Code Pr\u00e9requis Une machine virtuelle dans l'EAC. Voir la page Machines Virtuelles pour plus d'informations. pgAdmin ou un autre outil tel que Azure Data Studio and Visual Studio Code . Ces deux derniers sont disponibles par d\u00e9faut dans les images de machine virtuelle Data Science Virtual Machine . pgAdmin Ceci est l'un des outils les plus connus pour la gestion et l'administration de base de donn\u00e9es Azure PostgreSQL. Dans votre machine virtuelle installez pgAdmin disponible depuis https://www.pgadmin.org/download/ Connectez-vous \u00e0 votre machine virtuelle dans l'EAC et lancez pgAdmin . Avec le bouton droit de la souris, ajouter le serveur auquel vous devez vous connecter dans le coin sup\u00e9rieur gauche. Dans l'onglet General, entrez un nom pour le server. Vous pouvez entrer le nom de votre serveur PostgreSQL Dans l'onglet Connection, entrez le nom complet de votre serveur PostgreSQL puis entrez votre compte Cloud comme nom d'utilisateur suivi par le nom du serveur ou le nom du groupe AD auquel vous appartenez suivi par le nom du serveur si l'acc\u00e8s au serveur a \u00e9t\u00e9 accord\u00e9 \u00e0 ce groupe. Vous pouvez maintenant voir le nouveau serveur que vous venez d'ajouter dans la liste des serveurs. Cliquez dessus pour vous connecter et il vous sera demand\u00e9 d'entrer un mot de passe Le mot de passe que vous devez entrer est un jeton d'acc\u00e8s d'Azure AD qui sera g\u00e9n\u00e9r\u00e9 pour un usager authentifi\u00e9 \u00c0 l'aide de PowerShell, vous pouvez g\u00e9n\u00e9rer le jeton d'acc\u00e8s en entrant la commande suivante az account get-access-token --resource https://ossrdbms-aad.database.windows.net Le r\u00e9sultat se pr\u00e9sente comme suit o\u00f9 vous devez copier la valeur de accessToken et vous en servir comme mot de passe dans pgAdmin Azure Data Studio Connectez-vous \u00e0 votre machine virtuelle dans l'EAC et lancez Azure Data Studio . Les d\u00e9tails de connexion se pr\u00e9sentent comme suit, Avec le nom d'utilisateur qui peut \u00eatre votre compte Cloud suivi du nom du serveur: firstname.lastname@cloud.statcan.ca@servername.postgres.database.azure.com ou firstname.lastname@cloud.statcan.ca@servername Si l'acc\u00e8s au serveur a \u00e9t\u00e9 accord\u00e9 \u00e0 un groupe AD auquel vous appartenez, AD-Group@servername.postgres.database.azure.com ou AD-Group@servername","title":"Base de donn\u00e9es Azure Postgres"},{"location":"PostgreSQL/#acces-a-une-base-de-donnees-azure-postgresql","text":"","title":"Acc\u00e8s \u00e0 une base de donn\u00e9es Azure PostgreSQL"},{"location":"PostgreSQL/#azure-data-factory","text":"On peut cr\u00e9er un service li\u00e9 dans Azure Data Factory. Le nom d'utilisateur est votre compte cloud ou groupe AD si l'acc\u00e8s a \u00e9t\u00e9 donn\u00e9 \u00e0 un groupe auquel vous appartenez, suivi du nom du serveur. Le mot de passe est un jeton d'acc\u00e8s d'Azure AD tel que d\u00e9crit ci-dessous au point 6 de la section pgAdmin ou Veuillez communiquer avec l'\u00e9quipe de soutien par l'interm\u00e9diaire du canal https://cae-eac.slack.com si vous avez besoin d'aide.","title":"Azure Data Factory"},{"location":"PostgreSQL/#machine-virtuelle","text":"Vous pouvez vous connecter \u00e0 une base de donn\u00e9es Azure PostgreSQL \u00e0 partir de votre machine virtuelle en infonuagique, en utilisant une application telle que: 1. pgAdmin 2. Azure Data Studio 3. Visual Studio Code","title":"Machine virtuelle"},{"location":"PostgreSQL/#prerequis","text":"Une machine virtuelle dans l'EAC. Voir la page Machines Virtuelles pour plus d'informations. pgAdmin ou un autre outil tel que Azure Data Studio and Visual Studio Code . Ces deux derniers sont disponibles par d\u00e9faut dans les images de machine virtuelle Data Science Virtual Machine .","title":"Pr\u00e9requis"},{"location":"PostgreSQL/#pgadmin","text":"Ceci est l'un des outils les plus connus pour la gestion et l'administration de base de donn\u00e9es Azure PostgreSQL. Dans votre machine virtuelle installez pgAdmin disponible depuis https://www.pgadmin.org/download/ Connectez-vous \u00e0 votre machine virtuelle dans l'EAC et lancez pgAdmin . Avec le bouton droit de la souris, ajouter le serveur auquel vous devez vous connecter dans le coin sup\u00e9rieur gauche. Dans l'onglet General, entrez un nom pour le server. Vous pouvez entrer le nom de votre serveur PostgreSQL Dans l'onglet Connection, entrez le nom complet de votre serveur PostgreSQL puis entrez votre compte Cloud comme nom d'utilisateur suivi par le nom du serveur ou le nom du groupe AD auquel vous appartenez suivi par le nom du serveur si l'acc\u00e8s au serveur a \u00e9t\u00e9 accord\u00e9 \u00e0 ce groupe. Vous pouvez maintenant voir le nouveau serveur que vous venez d'ajouter dans la liste des serveurs. Cliquez dessus pour vous connecter et il vous sera demand\u00e9 d'entrer un mot de passe Le mot de passe que vous devez entrer est un jeton d'acc\u00e8s d'Azure AD qui sera g\u00e9n\u00e9r\u00e9 pour un usager authentifi\u00e9 \u00c0 l'aide de PowerShell, vous pouvez g\u00e9n\u00e9rer le jeton d'acc\u00e8s en entrant la commande suivante az account get-access-token --resource https://ossrdbms-aad.database.windows.net Le r\u00e9sultat se pr\u00e9sente comme suit o\u00f9 vous devez copier la valeur de accessToken et vous en servir comme mot de passe dans pgAdmin","title":"pgAdmin"},{"location":"PostgreSQL/#azure-data-studio","text":"Connectez-vous \u00e0 votre machine virtuelle dans l'EAC et lancez Azure Data Studio . Les d\u00e9tails de connexion se pr\u00e9sentent comme suit, Avec le nom d'utilisateur qui peut \u00eatre votre compte Cloud suivi du nom du serveur: firstname.lastname@cloud.statcan.ca@servername.postgres.database.azure.com ou firstname.lastname@cloud.statcan.ca@servername Si l'acc\u00e8s au serveur a \u00e9t\u00e9 accord\u00e9 \u00e0 un groupe AD auquel vous appartenez, AD-Group@servername.postgres.database.azure.com ou AD-Group@servername","title":"Azure Data Studio"},{"location":"R-Shiny/","text":"Ce document d\u00e9crit comment acc\u00e9der R-Shiny \u00e0 partir de l'application Rstudio. Commencer Afin d'utiliser R-Shiny, s.v.p. envoyer un message Slack \u00e0 l'\u00e9quipe de l'EAC pour activer RStudio dans votre cluster Databricks. Avertissement : Les clusters R-Shiny s'\u00e9teignent tous les jours \u00e0 19h . Pour r\u00e9duire vos co\u00fbts, veuillez arr\u00eater vos clusters R-Shiny lorsque vous ne les utilisez pas. Acc\u00e8s \u00e0 R-Shiny \u00c0 partir du portail Azure, lancer l'espace de travail Databricks qui vous a \u00e9t\u00e9 cr\u00e9\u00e9. \u00c0 partir de l'espace de travail Databricks, cliquez sur Cluster . \u00c0 partir de la liste de clusters disponibles, selectionnez le cluster sur lequel RStudio a \u00e9t\u00e9 install\u00e9. Note: Le cluster doit \u00eatre active pour acc\u00e9der \u00e0 l'application RStudio. Consulter la section Databricks pour plus de d\u00e9tails \u00e0 propos du d\u00e9marrage d'un cluster. Selectionnez l'onglet Apps . Cliquez sur Set up RStudio . Un mot de passe \u00e0 usage unique est g\u00e9n\u00e9r\u00e9 pour vous, cliquez sur show pour l'afficher et le copier. Cliquez sur Open RStudio . Une nouvelle fen\u00eatre s'ouvre, entrez le nom d'utilisateur et le mot de passe fournis (\u00c9tape 6) dans le formulaire de connexion afin de d\u00e9marrer RStudio. \u00c0 partir de l'interface RStudio, entrez la commande library(shiny) dans la console afin d'importer la librarie Shiny. Exemple d'une application R-Shiny Vous pouvez utiliser l'exemple Hello Shiny pour explorer la structure d'une application Shiny. Lancer l'application \u00e0 partir de votre session RStudio en entrant les commandes suivantes: library(shiny) runExample(\"01_hello\") Votre application devrait correspondre \u00e0 l'image ci-dessous. Acc\u00e8s aux fichiers \u00e0 partir du lac de donn\u00e9es Par d\u00e9faut, le r\u00e9pertoire de travail dans RStudio se trouve sur le noeud du pilote du cluster Databricks. Pour conserver votre travail, vous devrez utiliser DBFS. Pour avoir acc\u00e8s au DBFS dans l'explorateur de fichiers, cliquez sur le bouton ... \u00e0 droite et entrez /dbfs/mnt/ . Le lac de donn\u00e9es sera disponible et vous pourrez y acc\u00e9der et stocker vos fichiers. Lorsque votre cluster s'\u00e9teint \u00e0 la fin de votre session, votre travail reste disponible \u00e0 votre retour. NOTE: Voici des exemples de code pour acc\u00e9der \u00e0 vos fichiers \u00e0 partir du lac de donn\u00e9es. library(SparkR) sparkR.session() testData = as.data.frame(read.df(\"/mnt/le chemin du fichier\", source = \"l'extension du fichier\", header=\"true\", inferSchema = \"true\")) str(testData) setwd(\"/dbfs/mnt/le chemin du fichier\") testData = read.csv(\"le nom du fichier\") str(testData)","title":"R-Shiny"},{"location":"R-Shiny/#commencer","text":"Afin d'utiliser R-Shiny, s.v.p. envoyer un message Slack \u00e0 l'\u00e9quipe de l'EAC pour activer RStudio dans votre cluster Databricks. Avertissement : Les clusters R-Shiny s'\u00e9teignent tous les jours \u00e0 19h . Pour r\u00e9duire vos co\u00fbts, veuillez arr\u00eater vos clusters R-Shiny lorsque vous ne les utilisez pas.","title":"Commencer"},{"location":"R-Shiny/#acces-a-r-shiny","text":"\u00c0 partir du portail Azure, lancer l'espace de travail Databricks qui vous a \u00e9t\u00e9 cr\u00e9\u00e9. \u00c0 partir de l'espace de travail Databricks, cliquez sur Cluster . \u00c0 partir de la liste de clusters disponibles, selectionnez le cluster sur lequel RStudio a \u00e9t\u00e9 install\u00e9. Note: Le cluster doit \u00eatre active pour acc\u00e9der \u00e0 l'application RStudio. Consulter la section Databricks pour plus de d\u00e9tails \u00e0 propos du d\u00e9marrage d'un cluster. Selectionnez l'onglet Apps . Cliquez sur Set up RStudio . Un mot de passe \u00e0 usage unique est g\u00e9n\u00e9r\u00e9 pour vous, cliquez sur show pour l'afficher et le copier. Cliquez sur Open RStudio . Une nouvelle fen\u00eatre s'ouvre, entrez le nom d'utilisateur et le mot de passe fournis (\u00c9tape 6) dans le formulaire de connexion afin de d\u00e9marrer RStudio. \u00c0 partir de l'interface RStudio, entrez la commande library(shiny) dans la console afin d'importer la librarie Shiny.","title":"Acc\u00e8s \u00e0 R-Shiny"},{"location":"R-Shiny/#exemple-dune-application-r-shiny","text":"Vous pouvez utiliser l'exemple Hello Shiny pour explorer la structure d'une application Shiny. Lancer l'application \u00e0 partir de votre session RStudio en entrant les commandes suivantes: library(shiny) runExample(\"01_hello\") Votre application devrait correspondre \u00e0 l'image ci-dessous.","title":"Exemple d'une application R-Shiny"},{"location":"R-Shiny/#acces-aux-fichiers-a-partir-du-lac-de-donnees","text":"Par d\u00e9faut, le r\u00e9pertoire de travail dans RStudio se trouve sur le noeud du pilote du cluster Databricks. Pour conserver votre travail, vous devrez utiliser DBFS. Pour avoir acc\u00e8s au DBFS dans l'explorateur de fichiers, cliquez sur le bouton ... \u00e0 droite et entrez /dbfs/mnt/ . Le lac de donn\u00e9es sera disponible et vous pourrez y acc\u00e9der et stocker vos fichiers. Lorsque votre cluster s'\u00e9teint \u00e0 la fin de votre session, votre travail reste disponible \u00e0 votre retour. NOTE: Voici des exemples de code pour acc\u00e9der \u00e0 vos fichiers \u00e0 partir du lac de donn\u00e9es. library(SparkR) sparkR.session() testData = as.data.frame(read.df(\"/mnt/le chemin du fichier\", source = \"l'extension du fichier\", header=\"true\", inferSchema = \"true\")) str(testData) setwd(\"/dbfs/mnt/le chemin du fichier\") testData = read.csv(\"le nom du fichier\") str(testData)","title":"Acc\u00e8s aux fichiers \u00e0 partir du lac de donn\u00e9es"},{"location":"SeConnecter/","text":"Pr\u00e9requis Un compte de l'infonuagique StatCan ou un compte d'utilisateur invit\u00e9. L'acc\u00e8s avec un compte du r\u00e9seau de StatCan est pr\u00e9sentement seulement disponible pour le service Power BI, mais pourrait \u00eatre une option future pour les autres services de l'Environnement d'Analyse Collaborative (EAC). Notes Il est recommand\u00e9 d'utiliser Chrome, Chromium or Edge (non Internet Explorer) pour acc\u00e8der au portail Azure, les services Azure ou le service Power BI. Lors de la connexion aux services infonuagiques Azure, vous pourrez soit acc\u00e9der avec votre: Compte infonuagique de StatCan (c.-\u00e0-d. prenom.nomfamille@cloud.statcan.ca) ou Informations d\u2019identification d'autres d\u00e9partements ou chercheurs (c.-\u00e0-d. prenom.nom@dept-d\u00e9pt.gc.ca or nom@gov.prov.ca) ou Compte du r\u00e9seau de StatCan (c.-\u00e0-d. prenom.nomfamille@statcan.gc.ca) pour les employ\u00e9s de StatCan utilisant le service Power BI seulement. Suivre les instructions pour votre type de compte afin de compl\u00e9ter votre connexion. Compte infonuagique de StatCan (prenom.nomfamille@cloud.statcan.ca) S'applique \u00e0 tous les services infonuagiques Azure (Power BI, Databricks, Data Factory, machines virtuelles, etc.) En utilisant Chrome, Chromium ou Edge, ouvrir soit: Le tableau de bord Azure de l' Environnement d'Analyse Collaborative Le lien URL de l\u2019application Power BI (si fourni), ou la page de connexion au service Power BI En ouvrant le lien URL d'une application Power BI ou la page de connexion au service Power BI, vous serez dirig\u00e9 vers la page d\u2019ouverture de session de Microsoft Power BI, comme montr\u00e9 ci-dessous, disant \u201cVous disposez d\u00e9j\u00e0 d\u2019un compte?\u201c. Cliquez sur CONNEXION . On vous demandera ensuite soit d\u2019entrer ou de choisir un compte: La 1\u00e8re fois que vous vous connectez, la fen\u00eatre Se connecter de Microsoft s\u2019affichera comme montr\u00e9 ci-dessous. Entrez votre compte infonuagique (prenom.nomfamille@cloud.statcan.ca), et cliquez sur Suivant . Lors des connexions subs\u00e9quentes, la fen\u00eatre Choisir un compte de Microsoft s\u2019affichera, comme montr\u00e9 ci-dessous. Cliquez sur votre compte cloud.statcan.ca. On vous demandera ensuite d\u2019entrer le mot de passe de votre compte infonuagique. Une fois entr\u00e9, cliquez sur Connexion . Enfin, vous recevrez peut-\u00eatre une demande de Statistique Canada disant Plus d\u2019informations requises \u2013 Votre organisation a besoin de plus d\u2019information pour pr\u00e9server la s\u00e9curit\u00e9 de votre compte, comme montr\u00e9 ci-dessous, surtout si c\u2019est la premi\u00e8re fois que vous ouvrez une session sur le portail Web avec votre compte infonuagique. Cliquez sur Suivant et assurez-vous de suivre les instructions pour s\u00e9curiser votre compte par l\u2019authentification de votre courriel et la configuration de vos questions de s\u00e9curit\u00e9. Informations d\u2019identification d'autres d\u00e9partements ou chercheurs S'applique \u00e0 la majorit\u00e9 des services infonuagiques Azure (Power BI, Databricks, Data Factory, machines virtuelles, etc.) En utilisant Chrome, Chromium ou Edge, ouvrir soit: Le tableau de bord Azure de l' Environnement d'Analyse Collaborative Le lien URL de l\u2019application Power BI (si fourni), ou la page de connexion au service Power BI Azure Databricks Azure Data Factory En ouvrant le lien URL d'une application Power BI ou la page de connexion au service Power BI, vous serez dirig\u00e9 vers la page d\u2019ouverture de session de Microsoft Power BI, comme montr\u00e9 ci-dessous, disant \u201cVous disposez d\u00e9j\u00e0 d\u2019un compte?\u201c. Cliquez sur CONNEXION . On vous demandera ensuite soit d\u2019entrer ou de choisir un compte : La 1\u00e8re fois que vous vous connectez, la fen\u00eatre Se connecter de Microsoft s\u2019affichera comme montr\u00e9 ci-dessous. Connectez-vous avec vos informations d\u2019identification de messagerie officielle/O365 ou vos informations d\u2019identification GCCollaboration (p. ex., prenom.nom.departement@dept-d\u00e9pt.gc.ca, prenom.nom.departement@gccollaboration.ca or non@gov.prov.ca), et cliquez sur Suivant . Lors des connexions subs\u00e9quentes, la fen\u00eatre Choisir un compte de Microsoft s\u2019affichera, comme montr\u00e9 ci-dessous. Cliquez sur vos informations d\u2019identification de messagerie officielle/O365 ou vos informations d\u2019identification GCCollaboration (p. ex., prenom.nom.departement@dept-d\u00e9pt.gc.ca, prenom.nom.departement@gccollaboration.ca or non@gov.prov.ca). On vous demandera ensuite d\u2019entrer le mot de passe de votre compte infonuagique. Une fois entr\u00e9, cliquez sur Connexion . Si votre adresse \u00e9lectronique officielle ne prend pas en charge ce type de connexion, vous recevrez automatiquement par courrier \u00e9lectronique un code \u00e0 saisir en lieu et place d'un mot de passe, puis vous cliquerez sur Connexion . Le courriel que vous recevez ressemblera \u00e0 ce qui suit. Si vous ne le recevez pas, v\u00e9rifiez qu'il n'a pas \u00e9t\u00e9 envoy\u00e9 dans votre dossier Spam ou Junk. Accepter les autorisations de r\u00e9vision (consentement pour la premi\u00e8re fois uniquement). Vous recevrez alors le message suivant. Attendez jusqu\u2019\u00e0 la fin. Enfin, vous recevrez peut-\u00eatre une demande de Statistique Canada disant Plus d\u2019informations requises \u2013 Votre organisation a besoin de plus d\u2019information pour pr\u00e9server la s\u00e9curit\u00e9 de votre compte, comme montr\u00e9 ci-dessous, surtout si c\u2019est la premi\u00e8re fois que vous ouvrez une session sur le portail Web avec votre compte de l\u2019infonuagique. Cliquez sur Suivant et assurez-vous de suivre les instructions pour s\u00e9curiser votre compte par l\u2019authentification de votre courriel et la configuration de vos questions de s\u00e9curit\u00e9. Compte du r\u00e9seau de StatCan (prenom.nomfamille@statcan.gc.ca) S'applique seulement au service Power BI pour les employ\u00e9s de Statistique Canada. Pourrait \u00eatre une option future pour les autres services de l'Environnement d'Analyse Collaborative (EAC). Veuillez noter que l\u2019\u00e9tape 5 (mot de passe d\u2019Internet) ci-dessous peut arriver \u00e0 n\u2019importe quel moment. \u00c0 partir du r\u00e9seau B ou du OZ, et en utilisant Chrome, Chromium ou Edge, ouvrir soit: Le lien URL de l\u2019application Power BI (si fourni), ou https://powerbi.microsoft.com/fr-fr/landing/signin/ Vous serez dirig\u00e9 vers la page d\u2019ouverture de session de Microsoft Power BI, comme montr\u00e9 ci-dessous, disant \u201cVous disposez d\u00e9j\u00e0 d\u2019un compte?\u201c. Cliquez sur CONNEXION . On vous demandera ensuite soit d\u2019entrer soit de choisir un compte: La 1\u00e8re fois que vous vous connectez, la fen\u00eatre Se connecter de Microsoft s\u2019affichera comme montr\u00e9 ci-dessous. Entrez votre compte du r\u00e9seau de StatCan (prenom.nomfamille@statcan.gc.ca), et cliquez sur Suivant . Lors des connexions subs\u00e9quentes, la fen\u00eatre Choisir un compte de Microsoft s\u2019affichera, comme montr\u00e9 ci-dessous. Cliquez sur votre compte statcan.gc.ca. Vous allez ensuite recevoir le message de Microsoft \u201cVous allez \u00eatre redirig\u00e9 vers la page de connexion de votre organisation\u201c. Vous serez peut-\u00eatre amen\u00e9 \u00e0 entrer votre nom d\u2019utilisateur et votre mot de passe d\u2019 Internet comme montr\u00e9 ci-dessous. Une fois entr\u00e9s, cliquez sur Ouvrir une session . On vous demandera ensuite de vous connecter \u00e0 votre compte du r\u00e9seau de StatCan (c.-\u00e0-d. prenom.nomfamille@statcan.gc.ca) et d\u2019entrer votre mot de passe du r\u00e9seau A. Une fois entr\u00e9s, cliquez sur Ouvrir une session . Enfin, vous recevrez peut-\u00eatre une demande de Statistique Canada disant Plus d\u2019informations requises \u2013 Votre organisation a besoin de plus d\u2019information pour pr\u00e9server la s\u00e9curit\u00e9 de votre compte, comme montr\u00e9 ci-dessous, surtout si c\u2019est la premi\u00e8re fois que vous ouvrez une session sur le portail Web avec votre compte du r\u00e9seau de StatCan. Cliquez sur Suivant et assurez-vous de suivre les instructions pour s\u00e9curiser votre compte par l\u2019authentification de votre courriel et la configuration de vos questions de s\u00e9curit\u00e9. Documentation Microsoft Documentation du Portail Azure","title":"Se connecter"},{"location":"SeConnecter/#prerequis","text":"Un compte de l'infonuagique StatCan ou un compte d'utilisateur invit\u00e9. L'acc\u00e8s avec un compte du r\u00e9seau de StatCan est pr\u00e9sentement seulement disponible pour le service Power BI, mais pourrait \u00eatre une option future pour les autres services de l'Environnement d'Analyse Collaborative (EAC).","title":"Pr\u00e9requis"},{"location":"SeConnecter/#notes","text":"Il est recommand\u00e9 d'utiliser Chrome, Chromium or Edge (non Internet Explorer) pour acc\u00e8der au portail Azure, les services Azure ou le service Power BI. Lors de la connexion aux services infonuagiques Azure, vous pourrez soit acc\u00e9der avec votre: Compte infonuagique de StatCan (c.-\u00e0-d. prenom.nomfamille@cloud.statcan.ca) ou Informations d\u2019identification d'autres d\u00e9partements ou chercheurs (c.-\u00e0-d. prenom.nom@dept-d\u00e9pt.gc.ca or nom@gov.prov.ca) ou Compte du r\u00e9seau de StatCan (c.-\u00e0-d. prenom.nomfamille@statcan.gc.ca) pour les employ\u00e9s de StatCan utilisant le service Power BI seulement. Suivre les instructions pour votre type de compte afin de compl\u00e9ter votre connexion.","title":"Notes"},{"location":"SeConnecter/#compte-infonuagique-de-statcan-prenomnomfamillecloudstatcanca","text":"S'applique \u00e0 tous les services infonuagiques Azure (Power BI, Databricks, Data Factory, machines virtuelles, etc.) En utilisant Chrome, Chromium ou Edge, ouvrir soit: Le tableau de bord Azure de l' Environnement d'Analyse Collaborative Le lien URL de l\u2019application Power BI (si fourni), ou la page de connexion au service Power BI En ouvrant le lien URL d'une application Power BI ou la page de connexion au service Power BI, vous serez dirig\u00e9 vers la page d\u2019ouverture de session de Microsoft Power BI, comme montr\u00e9 ci-dessous, disant \u201cVous disposez d\u00e9j\u00e0 d\u2019un compte?\u201c. Cliquez sur CONNEXION . On vous demandera ensuite soit d\u2019entrer ou de choisir un compte: La 1\u00e8re fois que vous vous connectez, la fen\u00eatre Se connecter de Microsoft s\u2019affichera comme montr\u00e9 ci-dessous. Entrez votre compte infonuagique (prenom.nomfamille@cloud.statcan.ca), et cliquez sur Suivant . Lors des connexions subs\u00e9quentes, la fen\u00eatre Choisir un compte de Microsoft s\u2019affichera, comme montr\u00e9 ci-dessous. Cliquez sur votre compte cloud.statcan.ca. On vous demandera ensuite d\u2019entrer le mot de passe de votre compte infonuagique. Une fois entr\u00e9, cliquez sur Connexion . Enfin, vous recevrez peut-\u00eatre une demande de Statistique Canada disant Plus d\u2019informations requises \u2013 Votre organisation a besoin de plus d\u2019information pour pr\u00e9server la s\u00e9curit\u00e9 de votre compte, comme montr\u00e9 ci-dessous, surtout si c\u2019est la premi\u00e8re fois que vous ouvrez une session sur le portail Web avec votre compte infonuagique. Cliquez sur Suivant et assurez-vous de suivre les instructions pour s\u00e9curiser votre compte par l\u2019authentification de votre courriel et la configuration de vos questions de s\u00e9curit\u00e9.","title":"Compte infonuagique de StatCan (prenom.nomfamille@cloud.statcan.ca)"},{"location":"SeConnecter/#informations-didentification-dautres-departements-ou-chercheurs","text":"S'applique \u00e0 la majorit\u00e9 des services infonuagiques Azure (Power BI, Databricks, Data Factory, machines virtuelles, etc.) En utilisant Chrome, Chromium ou Edge, ouvrir soit: Le tableau de bord Azure de l' Environnement d'Analyse Collaborative Le lien URL de l\u2019application Power BI (si fourni), ou la page de connexion au service Power BI Azure Databricks Azure Data Factory En ouvrant le lien URL d'une application Power BI ou la page de connexion au service Power BI, vous serez dirig\u00e9 vers la page d\u2019ouverture de session de Microsoft Power BI, comme montr\u00e9 ci-dessous, disant \u201cVous disposez d\u00e9j\u00e0 d\u2019un compte?\u201c. Cliquez sur CONNEXION . On vous demandera ensuite soit d\u2019entrer ou de choisir un compte : La 1\u00e8re fois que vous vous connectez, la fen\u00eatre Se connecter de Microsoft s\u2019affichera comme montr\u00e9 ci-dessous. Connectez-vous avec vos informations d\u2019identification de messagerie officielle/O365 ou vos informations d\u2019identification GCCollaboration (p. ex., prenom.nom.departement@dept-d\u00e9pt.gc.ca, prenom.nom.departement@gccollaboration.ca or non@gov.prov.ca), et cliquez sur Suivant . Lors des connexions subs\u00e9quentes, la fen\u00eatre Choisir un compte de Microsoft s\u2019affichera, comme montr\u00e9 ci-dessous. Cliquez sur vos informations d\u2019identification de messagerie officielle/O365 ou vos informations d\u2019identification GCCollaboration (p. ex., prenom.nom.departement@dept-d\u00e9pt.gc.ca, prenom.nom.departement@gccollaboration.ca or non@gov.prov.ca). On vous demandera ensuite d\u2019entrer le mot de passe de votre compte infonuagique. Une fois entr\u00e9, cliquez sur Connexion . Si votre adresse \u00e9lectronique officielle ne prend pas en charge ce type de connexion, vous recevrez automatiquement par courrier \u00e9lectronique un code \u00e0 saisir en lieu et place d'un mot de passe, puis vous cliquerez sur Connexion . Le courriel que vous recevez ressemblera \u00e0 ce qui suit. Si vous ne le recevez pas, v\u00e9rifiez qu'il n'a pas \u00e9t\u00e9 envoy\u00e9 dans votre dossier Spam ou Junk. Accepter les autorisations de r\u00e9vision (consentement pour la premi\u00e8re fois uniquement). Vous recevrez alors le message suivant. Attendez jusqu\u2019\u00e0 la fin. Enfin, vous recevrez peut-\u00eatre une demande de Statistique Canada disant Plus d\u2019informations requises \u2013 Votre organisation a besoin de plus d\u2019information pour pr\u00e9server la s\u00e9curit\u00e9 de votre compte, comme montr\u00e9 ci-dessous, surtout si c\u2019est la premi\u00e8re fois que vous ouvrez une session sur le portail Web avec votre compte de l\u2019infonuagique. Cliquez sur Suivant et assurez-vous de suivre les instructions pour s\u00e9curiser votre compte par l\u2019authentification de votre courriel et la configuration de vos questions de s\u00e9curit\u00e9.","title":"Informations d\u2019identification d'autres d\u00e9partements ou chercheurs"},{"location":"SeConnecter/#compte-du-reseau-de-statcan-prenomnomfamillestatcangcca","text":"S'applique seulement au service Power BI pour les employ\u00e9s de Statistique Canada. Pourrait \u00eatre une option future pour les autres services de l'Environnement d'Analyse Collaborative (EAC). Veuillez noter que l\u2019\u00e9tape 5 (mot de passe d\u2019Internet) ci-dessous peut arriver \u00e0 n\u2019importe quel moment. \u00c0 partir du r\u00e9seau B ou du OZ, et en utilisant Chrome, Chromium ou Edge, ouvrir soit: Le lien URL de l\u2019application Power BI (si fourni), ou https://powerbi.microsoft.com/fr-fr/landing/signin/ Vous serez dirig\u00e9 vers la page d\u2019ouverture de session de Microsoft Power BI, comme montr\u00e9 ci-dessous, disant \u201cVous disposez d\u00e9j\u00e0 d\u2019un compte?\u201c. Cliquez sur CONNEXION . On vous demandera ensuite soit d\u2019entrer soit de choisir un compte: La 1\u00e8re fois que vous vous connectez, la fen\u00eatre Se connecter de Microsoft s\u2019affichera comme montr\u00e9 ci-dessous. Entrez votre compte du r\u00e9seau de StatCan (prenom.nomfamille@statcan.gc.ca), et cliquez sur Suivant . Lors des connexions subs\u00e9quentes, la fen\u00eatre Choisir un compte de Microsoft s\u2019affichera, comme montr\u00e9 ci-dessous. Cliquez sur votre compte statcan.gc.ca. Vous allez ensuite recevoir le message de Microsoft \u201cVous allez \u00eatre redirig\u00e9 vers la page de connexion de votre organisation\u201c. Vous serez peut-\u00eatre amen\u00e9 \u00e0 entrer votre nom d\u2019utilisateur et votre mot de passe d\u2019 Internet comme montr\u00e9 ci-dessous. Une fois entr\u00e9s, cliquez sur Ouvrir une session . On vous demandera ensuite de vous connecter \u00e0 votre compte du r\u00e9seau de StatCan (c.-\u00e0-d. prenom.nomfamille@statcan.gc.ca) et d\u2019entrer votre mot de passe du r\u00e9seau A. Une fois entr\u00e9s, cliquez sur Ouvrir une session . Enfin, vous recevrez peut-\u00eatre une demande de Statistique Canada disant Plus d\u2019informations requises \u2013 Votre organisation a besoin de plus d\u2019information pour pr\u00e9server la s\u00e9curit\u00e9 de votre compte, comme montr\u00e9 ci-dessous, surtout si c\u2019est la premi\u00e8re fois que vous ouvrez une session sur le portail Web avec votre compte du r\u00e9seau de StatCan. Cliquez sur Suivant et assurez-vous de suivre les instructions pour s\u00e9curiser votre compte par l\u2019authentification de votre courriel et la configuration de vos questions de s\u00e9curit\u00e9.","title":"Compte du r\u00e9seau de StatCan (prenom.nomfamille@statcan.gc.ca)"},{"location":"SeConnecter/#documentation-microsoft","text":"Documentation du Portail Azure","title":"Documentation Microsoft"},{"location":"TableauxDeBord/","text":"Les tableaux de bord affichent vos ressources infonuagiques dans le portail Azure de fa\u00e7on pr\u00e9cise et organis\u00e9e. Ils servent d'espace de travail o\u00f9 vous pouvez rapidement lancer des t\u00e2ches pour les op\u00e9rations quotidiennes et surveiller les ressources. Par exemple, vous pouvez cr\u00e9er des tableaux de bord personnalis\u00e9s en fonction des projets, des t\u00e2ches ou des r\u00f4les des utilisateurs. Le portail Azure fournit un tableau de bord par d\u00e9faut comme point de d\u00e9part. Vous pouvez modifier ce dernier, cr\u00e9er et personnaliser des tableaux de bord suppl\u00e9mentaires, ainsi qu'en publier et en partager avec d'autres utilisateurs. Acc\u00e9der au tableau de bord de l'Environnement d'analyse collaborative Dans le menu du portail Azure, s\u00e9lectionnez Tableau de bord . Votre affichage par d\u00e9faut pourrait d\u00e9j\u00e0 \u00eatre r\u00e9gl\u00e9 au tableau de bord. S\u00e9lectionnez la fl\u00e8che \u00e0 c\u00f4t\u00e9 du nom du tableau de bord. S\u00e9lectionnez le tableau de bord Environnement d'analyse collaborative dans la liste affich\u00e9e. Si ce tableau de bord n'est pas dans la liste : a. S\u00e9lectionnez Parcourir tous les tableaux de bord . b. Dans le champ Type , s\u00e9lectionnez Tableaux de bord partag\u00e9s . c. Assurez-vous que \u00ab vdl \u00bb se trouve parmi les abonnements s\u00e9lectionn\u00e9s. Vous pouvez aussi saisir du texte pour filtrer les tableaux de bord par leur nom. d. S\u00e9lectionnez le tableau de bord Environnement d'analyse collaborative dans la liste des tableaux de bord partag\u00e9s. Documentation Microsoft Cr\u00e9er un tableau de bord dans le portail Azure","title":"Tableaux de bord"},{"location":"TableauxDeBord/#acceder-au-tableau-de-bord-de-lenvironnement-danalyse-collaborative","text":"Dans le menu du portail Azure, s\u00e9lectionnez Tableau de bord . Votre affichage par d\u00e9faut pourrait d\u00e9j\u00e0 \u00eatre r\u00e9gl\u00e9 au tableau de bord. S\u00e9lectionnez la fl\u00e8che \u00e0 c\u00f4t\u00e9 du nom du tableau de bord. S\u00e9lectionnez le tableau de bord Environnement d'analyse collaborative dans la liste affich\u00e9e. Si ce tableau de bord n'est pas dans la liste : a. S\u00e9lectionnez Parcourir tous les tableaux de bord . b. Dans le champ Type , s\u00e9lectionnez Tableaux de bord partag\u00e9s . c. Assurez-vous que \u00ab vdl \u00bb se trouve parmi les abonnements s\u00e9lectionn\u00e9s. Vous pouvez aussi saisir du texte pour filtrer les tableaux de bord par leur nom. d. S\u00e9lectionnez le tableau de bord Environnement d'analyse collaborative dans la liste des tableaux de bord partag\u00e9s.","title":"Acc\u00e9der au tableau de bord de l'Environnement d'analyse collaborative"},{"location":"TableauxDeBord/#documentation-microsoft","text":"Cr\u00e9er un tableau de bord dans le portail Azure","title":"Documentation Microsoft"},{"location":"VideoTest/","text":"Vid\u00e9o Test Comment se Connecter au Portal Azure Comment se Connecter au Portal Azure - Transcription Allez sur portal.azure.com dans votre navigateur web. Entrez votre identifiant et votre mot de passe. Pour les utilisateurs internes de Statcan, utilisez votre compte cloud. Pour les utilisateurs externes, utilisez votre compte externe. La premi\u00e8re fois vous connectez, vous serez invit\u00e9 \u00e0 saisir vos questions de s\u00e9curit\u00e9. Pour l'authentification multifacteur, vous pouvez utiliser l'application Microsoft Authenticator sur votre t\u00e9l\u00e9phone portable.","title":"Vid\u00e9o Test"},{"location":"VideoTest/#video-test","text":"","title":"Vid\u00e9o Test"},{"location":"VideoTest/#comment-se-connecter-au-portal-azure","text":"Comment se Connecter au Portal Azure - Transcription Allez sur portal.azure.com dans votre navigateur web. Entrez votre identifiant et votre mot de passe. Pour les utilisateurs internes de Statcan, utilisez votre compte cloud. Pour les utilisateurs externes, utilisez votre compte externe. La premi\u00e8re fois vous connectez, vous serez invit\u00e9 \u00e0 saisir vos questions de s\u00e9curit\u00e9. Pour l'authentification multifacteur, vous pouvez utiliser l'application Microsoft Authenticator sur votre t\u00e9l\u00e9phone portable.","title":"Comment se Connecter au Portal Azure"}]}