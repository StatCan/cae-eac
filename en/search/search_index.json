{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":""},{"location":"#data-analytics-as-a-service-das-offers-the-convenience-and-familiarity-of-statistical-software","title":"Data Analytics as a Service (DAS) offers the convenience and familiarity of statistical software","text":"<p>Data Analytics as a Service, powered by Statistics Canada, provides data stewards, analysts and researchers with access to public data and microdata in its Collaborative Analytics Environment. Using this secure, cloud-based data analytics service, holds many benefits. - Upload and store data quickly and securely from an internal or external storage account. - Choose from a variety of familiar tools to prepare and transform datasets. - Analyze and present data using dynamic visualizations in order to share compelling data stories.</p> <p>Please note that at this time, some statistical software tools may not be available for use in both official languages. Statistics Canada is working to ensure that all available data products and tools in the DAaaS platform will be accessible in both official languages in the near future. For more information, visit the CAE language GitHub documentation.</p>"},{"location":"#data-storage","title":"Data storage","text":"<p>Manage, edit, share and backup</p> <p>Azure Blob Storage</p> <p>This solution includes object, file, disk, queue, and table storage. There are also services for hybrid storage solutions, and services to transfer, share, and backup data.</p> <p>Azure Data Lakes</p> <p>Leveraging the capabilities of our two existing storage services, Azure Blob storage and Azure Data Lake Storage Gen1, this tool is dedicated to big data analytics. Features such as file system semantics, directory, and file-level security and scale are combined with low-cost, tiered storage, and high availability/disaster recovery capabilities.</p> <p>Azure SQL Database</p> <p>A fully managed platform as a service (PaaS) database engine that handles most of the database management functions such as upgrading, patching, backups, and monitoring without user involvement.</p>"},{"location":"#data-preparation","title":"Data preparation","text":"<p>Clean, organize, format</p> <p>Azure Data Factory</p> <p>Create and schedule data-driven workflows (called pipelines) that can ingest data from disparate data stores, build complex ETL processes that transform data visually with data flows or by using compute services such as Azure HDInsight Hadoop, Azure Databricks, and Azure SQL Database.</p>"},{"location":"#data-analysis-and-visualization","title":"Data analysis and visualization","text":"<p>Analyze, share insights and tell data stories</p> <p>Azure Databricks</p> <p>An Apache Spark-based analytics platform optimized for the Microsoft Azure cloud services platform.</p> <p>Power BI</p> <p>A collection of software services and applications that easily connect your data sources to create coherent, visually immersive, and interactive insights.</p> <p>Azure Machine Learning</p> <p>Train, deploy, and manage machine learning models, AutoML experiments, and pipelines at scale.</p> <p>Azure Virtual Machines</p> <p>This service enables you to quickly set up an environment for your team by creating or using virtual machines using Azure DevTest Labs for day-to-day work.</p>"},{"location":"AVDStorage/","title":"Azure Storage","text":""},{"location":"AVDStorage/#azure-storage-statistics-canada-users","title":"Azure Storage - Statistics Canada Users","text":"<p>Data can be uploaded to the platform via the Azure Portal or the Azure Storage Explorer application. Once data uploaded to an external Blob storage account, it is automatically ingested into an internal Azure Data Lake Storage (ADLS) account.  Once data is in the data lake, users have their choice of tools for transformation and integration. They can use Web based tools such as Databricks and Data Factory to do their transformations or they can use desktop tools on a virtual machine (VM) to tansform &amp; analyse the data. Cleansed and transformed data can be placed into different folders (containing higher quality / processed datasets) or loaded into a database. Users can once again connect to this data with the tools they would like to use, either from their VMs or other platform services such as Databricks and Data Factory.</p> <p>Using your Azure Virtual Desktop, you can have access to your cloud storage accounts.</p>"},{"location":"AVDStorage/#azure-storage-explorer","title":"Azure Storage Explorer","text":"<ol> <li>If you don't have the Azure Storage Explorer installed in the Azure Virtual Desktop, you can request it in the Service Request Management Portal (SRM) Statistics Canada Users Only. Select the following:</li> <li>Request Type: \"Desktop Support\"</li> <li>Topic: \"Virtual Desktop (AVD, VDI)\"</li> <li>Sub-topic: \"Azure Virtual Desktop (Create, Modify)\"</li> <li> <p>Description: \"I would like to request an Azure Virtual Desktop with Azure Storage Explorer installed\"</p> </li> <li> <p>On your Azure Virtual Desktop, you will be able to access your storage account using Azure Storage Explorer</p> </li> </ol>"},{"location":"AVDStorage/#azure-portal","title":"Azure Portal","text":"<ol> <li>Navigate to the Storage Account from the Azure Portal. </li> </ol> <ol> <li>Select your subscription, then navigate the storage account.   </li> </ol> <p>Frequently Asked Questions (FAQ) 1. I get the following error message when accessing my storage accounts.</p> <p>    Please make sure you are accessing the storage account from your AVD and not the government device.</p>"},{"location":"AVDStorage/#microsoft-documentation","title":"Microsoft Documentation","text":"<ul> <li>Azure Storage Explorer Download </li> <li>Quickstart: Upload, download, and list blobs with the Azure portal</li> </ul>"},{"location":"Artifactory/","title":"Package Managment","text":""},{"location":"Artifactory/#artifactory","title":"Artifactory","text":"<p>The CAE environment uses the Artifiactory for package &amp; library management.</p>"},{"location":"Artifactory/#included-packages","title":"Included packages","text":"<p>Below are the URLs that Artifactory pulls from currently. As long as the package is available on these repositories, it can be downloaded. You can get the name of the package by searching through the repositories.</p> <ul> <li>Conda-forge</li> <li>CRAN (In the left menu, click Packages under Software, then click Table of available packages)</li> <li>Python</li> </ul> <p>The respective artifactory URLs to use are:   - https://jfrog.aaw.cloud.statcan.ca/artifactory/conda-forge-remote/   - https://jfrog.aaw.cloud.statcan.ca/artifactory/dev-cran-remote/   - https://jfrog.aaw.cloud.statcan.ca/artifactory/pypi-remote/</p> <p>Note: For any other packages, please contact the Collaborative Analytics Environment team.</p>"},{"location":"Artifactory/#azure-databricks","title":"Azure DataBricks","text":"<p>The packages can be installed from the Databricks workspace or a cluster.</p> <p>Note:   - The library sources available in Databricks are PyPI for Python packages and CRAN for CRAN packages.  - The artifactory URLs to use depending on the selected library source are:     - https://jfrog.aaw.cloud.statcan.ca/artifactory/pypi-remote/ for PyPI     - https://jfrog.aaw.cloud.statcan.ca/artifactory/dev-cran-remote/  for CRAN</p>"},{"location":"Artifactory/#from-databricks-workspace","title":"From Databricks workspace","text":"<p>1.From the main page, click Import Library.</p> <p></p> <ol> <li> <p>Under Library Source, choose either PyPI or CRAN depending of the package.</p> </li> <li> <p>Paste the name of the library under Package and the associated Artifactory URL (See Note above) under Repository.</p> </li> </ol> <p></p> <ol> <li>Click Create.</li> </ol> <p>Note: Select Install automatically on all cluster under Admin settings if you wish to install the package on all the available clusters of the workspace.</p>"},{"location":"Artifactory/#from-a-cluster","title":"From a cluster","text":"<p>1.From the cluster main page, click Install new under Libraries.</p> <p></p> <ol> <li> <p>Under Library Source, choose either PyPI or CRAN depending of the package.</p> </li> <li> <p>Paste the name of the library under Package and the associated Artifactory URL (See Note above) under Repository.</p> </li> </ol> <p></p> <ol> <li>Click Install. You should see the newly installed library under Libraries.</li> </ol> <p></p>"},{"location":"Artifactory/#azure-machine-learning","title":"Azure Machine Learning","text":"<p>The packages can be installed from the terminal using Python commands.</p> <ol> <li> <p>From the main page, click Notebooks, then click Open Terminal under Files.   </p> </li> <li> <p>Select the compute instance assigned to you from the drop-down next to Compute.</p> </li> </ol> <p>If a compute instance has not been created for you, please contact the  Collaborative Analytics Environment team.</p> <ol> <li>Run this code to setup pip to download packages from Artifactory repositories by default:</li> </ol> <p><code>pip config --user set global.index-url https://jfrog.aaw.cloud.statcan.ca/artifactory/api/pypi/pypi-remote/simple</code></p> <p></p> <ol> <li>You can now use pip3 or pip command to install packages as following:</li> </ol> <p><code>pip3 install &lt;PackageName&gt;</code> </p>"},{"location":"Artifactory/#azure-synapse","title":"Azure Synapse","text":"<p>Please contact the Collaborative Analytics Environment team to install custom packages in your Azure Synapse environment.</p>"},{"location":"Artifactory/#visual-studio-code","title":"Visual Studio Code","text":"<p>1.From the Extensions tab, click Terminal, then click New Terminal. </p> <ol> <li>Run this code to setup pip to download packages from Artifactory repositories by default:</li> </ol> <p><code>pip config --user set global.index-url https://jfrog.aaw.cloud.statcan.ca/artifactory/api/pypi/pypi-remote/simple</code></p> <p></p> <ol> <li>You can now use pip3* or pip** command to install packages as following:</li> </ol> <p><code>pip3 install &lt;PackageName&gt;</code></p> <p></p>"},{"location":"Artifactory/#rstudio","title":"RStudio","text":"<ol> <li>Run this code to configure the Rprofile.site file to use the Artifactory repository by default:</li> </ol> <pre><code>options(repos = c(artifactory = \"https://jfrog.aaw.cloud.statcan.ca/artifactory/dev-cran-remote/\"),\ndownload.file.method = \"curl\",\ndownload.file.extra = \"-k -L\") </code></pre> <ol> <li>Under Packages, click \"Install\". Choose the new repository that you added in step 1 and enter the package name to install.</li> </ol> <p>Note: You can directly use the install.packages(\"PackageName\") command to install packages</p> <ol> <li>You should see the installed package in the System Library list.</li> </ol> <p></p>"},{"location":"Artifactory/#r-console","title":"R Console","text":"<ol> <li>Run this code to configure the Rprofile.site file to use the Artifactory repository by default:</li> </ol> <pre><code>options(repos = c(artifactory = \"https://jfrog.aaw.cloud.statcan.ca/artifactory/dev-cran-remote/\"),\ndownload.file.method = \"curl\",\ndownload.file.extra = \"-k -L\")\n</code></pre> <ol> <li>Run the install.packages(\"PackageName\") command to install packages.</li> </ol>"},{"location":"Artifactory/#azure-cloud-virtual-machine","title":"Azure Cloud Virtual Machine","text":""},{"location":"Artifactory/#miniforge-prompt","title":"Miniforge Prompt","text":"<ol> <li>Run this code to setup pip to download packages from Artifactory repositories by default:</li> </ol> <p><code>pip config --user set global.index-url https://jfrog.aaw.cloud.statcan.ca/artifactory/api/pypi/pypi-remote/simple</code></p> <p></p> <ol> <li>You can now use pip3 or pip command to install packages as following:</li> </ol> <p><code>pip3 install &lt;PackageName&gt;</code></p> <p></p>"},{"location":"Artifactory/#command-prompt","title":"Command Prompt","text":"<ol> <li>Run this code to setup pip to download packages from Artifactory repositories by default:</li> </ol> <p><code>pip config --user set global.index-url https://jfrog.aaw.cloud.statcan.ca/artifactory/api/pypi/pypi-remote/simple</code></p> <p></p> <ol> <li>You can now use pip3* or pip** command to install packages as following:</li> </ol> <p><code>pip3 install &lt;PackageName&gt;</code></p> <p></p>"},{"location":"Artifactory/#python-application","title":"Python application","text":"<p>You can use the Command Prompt to install packages, then import them into the Python notebooks as the following scenario:</p> <ol> <li>Let's try to access the module\u2019s contents with the import statement.</li> </ol> <p></p> <ol> <li>Now let's install it using the Command Prompt.  See Command Prompt section of this document if the pip.ini file hasn't been configured before</li> </ol> <p></p> <ol> <li>The module can now be imported.</li> </ol> <p></p>"},{"location":"Artifactory/#download-packages-locally","title":"Download Packages locally","text":"<p>To download packages locally, you can use the Curl or pip download commands as following:</p> <p><code>curl -O \"https://jfrog.aaw.cloud.statcan.ca/artifactory/cae-generic-test/&lt;PackageName&gt;\"</code></p> <p><code>pip download &lt;package_url&gt;</code></p>"},{"location":"Avd/","title":"Azure Virtual Desktop","text":""},{"location":"Avd/#azure-virtual-desktop","title":"Azure Virtual Desktop","text":"<p>Statistics Canada employees have access to Azure Virtual Desktop on their Statistics Canada device.  Statistics Canada employees can use their statcan.gc.ca credentials to login to Azure Virtual Desktop.</p> <ol> <li> <p>To be able to access Azure Virtual Desktop, you will need your statcan account: firstname.lastname@statcan.gc.ca</p> </li> <li> <p>You will have to connect to your environment through the Azure Virtual Desktop (AVD). From the Windows Search bar, you will have to access the Remote Desktop app then select your AVD and enter your NetA credential as shown below.</p> <p> </p> </li> <li> <p>Once you are logged into your Azure Virtual Desktop,  you can access services with the CAS Azure Dashboard </p> </li> </ol>"},{"location":"AzureML/","title":"Azure Machine Learning","text":""},{"location":"AzureML/#getting-started","title":"Getting Started","text":"<ol> <li> <p>On the machine learning Overview page, click Launch studio.</p> <p> </p> </li> <li> <p>Use the drop-down to select vdl subscription and the Machine Learning workspace you want to access, then click Get started.</p> <p></p> </li> <li> <p>Once inside your Machine Learning workspace, you can train, deploy and manage machine learning models, use AutoML, and run pipelines. See Getting started quickly for more information.</p> <p> </p> </li> </ol>"},{"location":"AzureML/#using-azure-ml-notebook-standalone","title":"Using Azure ML Notebook standalone","text":""},{"location":"AzureML/#requirements","title":"Requirements","text":"<p>A compute instance in Azure ML. You should see it under Compute --&gt; Compute instances.</p> <p>Note: If a compute instance has not been created for you, please contact the support team via Slack.</p>"},{"location":"AzureML/#steps","title":"Steps","text":"<ol> <li> <p>Under Notebooks, create a new notebook in your user directory. You can then enter the code to execute.</p> <p> </p> </li> <li> <p>Select the Compute instance assigned to you.</p> <p></p> </li> <li> <p>Click the run all cells button to execute your code.</p> <p></p> </li> </ol>"},{"location":"AzureML/#using-databricks-connect-as-remote-compute","title":"Using Databricks Connect as Remote Compute","text":"<p>Disclaimer: Please note that the Databricks connect configuration shown below is under revision and will likely change in the near future.</p>"},{"location":"AzureML/#requirements_1","title":"Requirements","text":"<p>A compute instance in Azure ML. You should see it under Compute --&gt; Compute instances.</p> <p>Note: If a compute instance has not been created for you, please contact the support team via Slack.</p>"},{"location":"AzureML/#steps_1","title":"Steps","text":"<ol> <li> <p>Under Notebooks, open Terminal.</p> <p> </p> </li> <li> <p>Select your Compute instance from the drop-down next to Compute.</p> </li> <li> <p>Execute the code from Databricks Connect Setup in the terminal, while following the prompts to continue as needed. This code installs Python 3.7 and sets up a new kernel for Azure ML notebooks.</p> <p>When prompted, enter the following values to configure Databricks connect:</p> <p>Host: the URL from the Overview page for your Databricks workspace.</p> <p></p> <p>Token: the personal access token generated in your Databricks Workspace User Settings.</p> <p>Cluster ID: the value found under Cluster --&gt; Advanced Options --&gt; Tags in your Databricks workspace.</p> <p></p> <p>Org ID: the part of the Databricks URL found after .net/?o=</p> <p></p> <p>Port: keep the existing value</p> </li> <li> <p>Execute the following code in terminal to test the connectivity to Azure Databricks.     <code>databricks-connect test</code></p> </li> <li> <p>Create a new notebook with Azure ML and select the Python 3 kernel. It should now display Python 3.7.9</p> <p> </p> </li> <li> <p>Databricks connect should be setup now! Try the Databricks connect example code in a notebook, replacing public-data/incoming/1test.txt with the path to a file in your data lake container.</p> </li> </ol>"},{"location":"AzureML/#request-compute","title":"Request compute","text":"<p>Please contact the support team through the slack channel to request Azure ML compute. You will receive the following error when creating it yourself:</p> <p></p>"},{"location":"AzurePortal/","title":"Azure Portal","text":""},{"location":"AzurePortal/#dashboard","title":"Dashboard","text":"<p>See the Dashboard section of this documentation for more information. 1. Click on the Dashboard menu from the Azure Portal.  </p> <pre><code>![Dashboard](images/DataFactoryDashboard.png)\n</code></pre>"},{"location":"AzurePortal/#access-databricks-internal-users","title":"Access Databricks- Internal Users","text":"<ol> <li> <p>In the Azure Portal Search box, search for Databricks.  </p> <p> </p> </li> <li> <p>You should then see a list of the Databricks workspaces you were given permission to access.  </p> <p></p> </li> </ol>"},{"location":"AzurePortal/#access-azure-synapse-internal-users","title":"Access Azure Synapse- Internal Users","text":"<p>It is recommanded for internal user to access the cloud service through their Azure Virtual Desktop 1. Make sure that you are in your cloud virtual machine to access Azure Synapse. See Virtual Machines for information on how to create one if needed.</p> <ol> <li> <p>Inside your virtual machine, open a web browser and navigate to the Azure Portal. Sign in with your cloud account credentials.</p> </li> <li> <pre><code>a. Click on the **Azure Synapse Analytics** icon under **Azure services**. If you do not see this icon, follow step 3b instead.\n</code></pre> <p></p> <p>b. Start typing \"synapse\" into the search bar to find Azure Synapse Analytics.</p> <p> </p> </li> <li> <p>Find your Synapse workspace in the list and click on it. Then click Open Synapse Studio.</p> <p></p> </li> </ol> <p>Note: You can also acccess Synapse workspaces from the Collaborative Analytics Environment dashboard.</p>"},{"location":"AzurePortal/#accessing-azure-machine-learning-internal-users","title":"Accessing Azure Machine Learning- Internal Users","text":"<p>It is recommanded for internal user to access the cloud service through their Azure Virtual Desktop</p>"},{"location":"AzurePortal/#dashboard_1","title":"Dashboard","text":"<p>See the Dashboard section of this documentation from more information.  </p> <ol> <li> <p>Click on the Dashboard menu from the Azure Portal. Your default view might already be set to dashboard.  </p> <p></p> </li> <li> <p>Under Machine Learning , select the Machine Learning workspace that was created for you. If the workspace you want to open isn't listed, click on See more to access the complete list.</p> <p></p> </li> </ol>"},{"location":"AzurePortal/#azure-portal","title":"Azure Portal","text":"<ol> <li> <p>In the Azure Portal Search box, search for Machine Learning.</p> <p> </p> </li> <li> <p>You should see the list of the Machine Learning workspaces you were given permission to access. Select the Machine Learning workspace you want to access.</p> <p></p> </li> </ol>"},{"location":"AzurePortal/#machine-learning-url","title":"Machine Learning URL","text":"<ol> <li> <p>Navigate to https://ml.azure.com/, sign in with your cloud account credentials, and select vdl subscription and the Machine Learning workspace that was created for you.  </p> <p></p> </li> </ol>"},{"location":"AzureSQL/","title":"Azure SQL Database","text":"<p>An Azure SQL Database can be setup in advance if your project requires one.</p> <p>Reminder: The CAE Azure SQL Databases are only accessible from inside the CAE cloud environment. They are not accessible from any of the Government of Canada Data Centres.</p>"},{"location":"AzureSQL/#accessing-azure-sql-database","title":"Accessing Azure SQL Database","text":""},{"location":"AzureSQL/#azure-data-factory","title":"Azure Data Factory","text":"<p>A linked service can be setup inside Azure Data Factory. Configure the linked service to connect via the self-hosted integration runtime and use Managed Identity as the Authentication type. Please contact the support team through the Slack channel if you need assistance.</p>"},{"location":"AzureSQL/#databricks","title":"Databricks","text":"<p>Databricks notebooks can be configured to connect to the database. Because additional setup is required, please contact the the support team through the Slack channel to request this configuration as well as example notebooks.</p>"},{"location":"AzureSQL/#virtual-machine","title":"Virtual Machine","text":"<p>You can access an Azure SQL database from your cloud virtual machine, using various applications including: - SQL Server Management Studio  - Power BI Desktop - Azure Data Studio - Visual Studio or Visual Studio Code</p>"},{"location":"AzureSQL/#prerequisites","title":"Prerequisites","text":"<ul> <li>A virtual machine in the Collaborative Analytics Environment (CAE). See the VM page for more information.</li> <li>SQL Server Management Studio or another tool such as Power BI Desktop. These tools are available by default in the Windows Data Science Virtual Machine images.</li> </ul>"},{"location":"AzureSQL/#steps","title":"Steps","text":"<ol> <li> <p>Login to your CAE virtual machine.  </p> </li> <li> <p>Launch a tool such as SQL Server Management Studio. </p> </li> <li> <p>Choose Azure Active Directory - Universal with MFA as the Authentication type.</p> </li> <li> <p>Enter your Azure SQL server name and your cloud account username as User name.</p> <p> </p> </li> <li> <p>Click on Options.</p> </li> <li> <p>In the Connection Properties tab, enter your database name next to the Connect to database label and click on Connect. </p> <p></p> </li> <li> <p>Sign in with your cloud account credentials.</p> </li> </ol>"},{"location":"AzureStorage/","title":"AzureStorage","text":""},{"location":"AzureStorage/#azure-storage-external-user","title":"Azure Storage - External User","text":""},{"location":"AzureStorage/#storage-explorer","title":"Storage Explorer","text":"<ol> <li> <p>To access any of the services, you must first login to your virtual machine. See Login</p> </li> <li> <p>On your virtual machines, you will be able to access your storage account using Azure Storage Explorer</p> </li> </ol>"},{"location":"AzureStorage/#ingesting-data-files","title":"Ingesting Data Files","text":""},{"location":"AzureStorage/#electronic-file-transfert-eft","title":"Electronic File Transfert (EFT)","text":"<p>For some use cases sensitive data can be sent to the Environment using EFT. Please Contact Us for instructions if you would like to setup this service.</p>"},{"location":"AzureStorage/#download-data-to-your-storage-account","title":"Download data to your Storage Account","text":"<ol> <li>From your virtual machine, some URLs are accessible so that you can download data directly. Examples:</li> <li>statcan.gc.ca</li> <li>open.toronto.ca</li> <li>Download data to the local drive on your Virtual Machine</li> <li>Upload the data using Azure Storage Explorer into your project storage account.</li> </ol>"},{"location":"AzureStorageExplorer/","title":"Azure Storage Explorer","text":""},{"location":"AzureStorageExplorer/#storage-explorer","title":"Storage Explorer","text":"<ol> <li> <p>To access any of the services, you must first login to your virtual machine. See Login</p> </li> <li> <p>On your virtual machines, launch Azure Storage Explorer from the Start menu.</p> </li> </ol> <p></p> <ol> <li>The first time you launch the Storage explorer, you need to click the connect button </li> </ol> <p></p> <p>or click the Sign in with Azure Button</p> <p></p> <ol> <li>You then have to select your environment.  </li> </ol> <p> </p> <ol> <li>and login with your Azure Account by entering your credentials</li> </ol> <p> </p> <ol> <li> <p>Once you login, your storage account and containers will be visible to you.</p> </li> <li> <p>Files can be download and uploaded to\\from your virtual machine using your respective container </p> </li> </ol> <p> </p> <ol> <li>You must download data files to your local virtual machines so that it can be used by installed software. Storage accounts can not be mapped to virtual machines.</li> <li>You can create a personal folder in your container, to organize your personal files.</li> </ol> <p>Note: 9. Best practice is to always upload or store data to your storage account.  Virtual machines and their data are not backed up.</p>"},{"location":"AzureStorageExplorer/#microsoft-documentation","title":"Microsoft Documentation","text":"<ul> <li>Azure Storage Explorer Download </li> <li>Quickstart: Upload, download, and list blobs with the Azure portal</li> </ul>"},{"location":"AzureSynapse/","title":"Azure Synapse","text":""},{"location":"AzureSynapse/#getting-started","title":"Getting Started","text":""},{"location":"AzureSynapse/#start-and-stop-dedicated-sql-pool","title":"Start and Stop Dedicated SQL Pool","text":"<ol> <li> <p>Click the Integrate tab.</p> <p></p> </li> <li> <p>Under Pipelines, click either Start Dedicated SQL Pool or Pause Dedicated SQL Pool. Then click the trigger button to open a menu, and select Trigger now. On the next screen, click OK.</p> <p></p> </li> </ol>"},{"location":"AzureSynapse/#home","title":"Home","text":"<p>The Home tab is where you start when you first open Azure Synapse Studio. </p> <p>From here, you can access shortcuts for common tasks such as creating SQL scripts or notebooks by clicking the New dropdown menu button. Recently opened resources are also displayed.</p>"},{"location":"AzureSynapse/#data","title":"Data","text":"<p>The Data tab is where you can explore everything in your database and linked datasets.</p> <p>Under the Workspace tab, you can explore the dedicated SQL pool database and any Spark databases.</p> <p>Under the Linked tab, you can explore external objects (e.g. Data Lake accounts) and explore and create any integration datasets from external linked data (e.g. Data Lake, Blob Storage, web service, etc) to be used in pipelines.</p>"},{"location":"AzureSynapse/#how-to-bring-in-data-from-linked-services","title":"How to Bring in Data from Linked Services","text":"<p>Note: This example shows how to get data from Data Lake, although there are many source types available.</p> <ol> <li> <p>Click the \u00ab + \u00bb button the add a new resource, then click Integration Dataset.</p> <p></p> </li> <li> <p>Select Azure Data Lake Storage Gen2 (you may need to search for this), then click Continue.</p> <p></p> </li> <li> <p>Select the format type, then click Continue.</p> </li> <li> <p>Enter a name, then click the drop-down menu under Linked service and select your data lake.</p> <p></p> </li> <li> <p>Under Connect via integration runtime, ensure that interactive authoring is enabled. If it is not, click the edit button to enable it, then click Apply.</p> <p></p> </li> <li> <p>Set additional properties as appropriate, then click OK.</p> </li> </ol>"},{"location":"AzureSynapse/#how-to-explore-data-in-the-data-lake","title":"How to Explore Data in the Data Lake","text":"<p>Browse to find your dataset file (CSV, Parquet, JSON, Avro, etc) and right click it. A menu will open with options to preview the data, or create resources such as SQL scripts and notebooks.</p> <p></p>"},{"location":"AzureSynapse/#how-to-explore-the-dedicated-sql-pool","title":"How to Explore the Dedicated SQL Pool","text":"<p>Under the Workspace tab, you can explore databases similarly to SQL Server Management Studio. Right click any table, highlight New SQL script, and click Select TOP 100 rows to create a new query. You can then view the results as either a table or a chart.</p> <p></p>"},{"location":"AzureSynapse/#importing-data-to-the-dedicated-sql-pool","title":"Importing Data to the Dedicated SQL Pool","text":"<p>To import data to the dedicated SQL pool, you can either: - create a pipeline with a Copy data activity (most efficient for large datasets) - use the Bulk Load Wizard.</p>"},{"location":"AzureSynapse/#develop","title":"Develop","text":"<p>From here, you can create and save resources such as SQL scripts, notebooks, and Power BI reports.</p> <p>To add a new resource, click the \u00ab + \u00bb button. A dropdown menu will open.</p> <p></p> <p>To make your changes visible to others, you need to click the Publish button.</p>"},{"location":"AzureSynapse/#sql-scripts","title":"SQL Scripts","text":"<p>Be sure to connect to your dedicated SQL pool to run SQL scripts.</p> <p></p>"},{"location":"AzureSynapse/#notebooks","title":"Notebooks","text":"<p>To run notebook cells, you first need to select your Apache Spark pool.</p> <p></p> <p>To change languages for a single cell, you can use the following magic commands: %%pyspark, %%spark, %%csharp, %%sql. You can also change the default language using the Language dropdown menu.</p> <p></p>"},{"location":"AzureSynapse/#dataflows","title":"Dataflows","text":"<p>To add a source to a dataflow, click the \u00ab + \u00bb button under Source Settings, then select Azure Data Lake Storage Gen2 (you may need to search for this). Click Continue, select the data format, then on the next page, select your Linked Service.</p> <p></p>"},{"location":"AzureSynapse/#power-bi-reports","title":"Power BI Reports","text":"<p>You can view and create Power BI reports directly in Azure Synapse. Please contact the Collabotative Analytics Environment support team to validate that a linked service is set up.</p>"},{"location":"AzureSynapse/#integrate","title":"Integrate","text":"<p>This is where you can create pipelines for ingesting, preparing and transforming all of your data, like in Azure Data Factory. </p>"},{"location":"AzureSynapse/#example-copy-data-from-external-blob-to-data-lake","title":"Example: Copy Data from External Blob to Data Lake","text":"<ol> <li>Click the \u00ab + \u00bb button to add a new resource, then click Pipeline.</li> </ol> <ol> <li>Under Move &amp; transform, drag and drop Copy data into the window.</li> </ol> <ol> <li>Click on the Source tab, then click New to add the source dataset (where you want to copy the data from).</li> </ol> <ol> <li> <p>Select Azure Blob Storage, then select the format type (CSV, Parquet, JSON, etc). Set any additional properties if relevant, then click OK.</p> </li> <li> <p>Click Sink, then click New to set the sink dataset (where you want the data to be copied to). Choose Azure Data Lake Storage Gen2, then select the format type. Under Linked service, choose your data lake and ensure that interactive authoring is enabled (see How to Bring in Data from Linked Services under Data for more information).</p> </li> </ol>"},{"location":"AzureSynapse/#debugging-and-running-pipelines","title":"Debugging and Running Pipelines","text":"<p>To run a pipeline in debug mode, click the Debug button at the top of the pipeline window. Results will appear in the Output tab.</p> <p></p> <p>To run a pipeline without debugging, click the Add trigger button, then Trigger now.</p> <p>When you are ready to publish your pipelines, click the Validate all button, then click the Publish all button. Note that this will publish for all users to see everything that you currently have open (pipelines, SQL scripts, notebooks, etc).</p> <p></p>"},{"location":"AzureSynapse/#monitor","title":"Monitor","text":"<p>From the Monitor tab, you can monitor live pipeline runs (the inputs and outputs of each activity and any errors) and view historical pipeline runs, trigger runs, SQL requests, etc.</p>"},{"location":"AzureSynapse/#manage","title":"Manage","text":"<p>This is where you can: - Add new SQL or Apache Spark pools - Add new linked services - Grant others access to the workspace - Set up git integration</p>"},{"location":"AzureSynapse/#microsoft-documentation","title":"Microsoft Documentation","text":"<ul> <li>Azure Synapse Analytics</li> <li>What is Azure Synapse Analytics? </li> <li>Analyse Data with Dedicated SQL Pools</li> <li>Integrate with Pipelines</li> <li>Visualize Data with Power BI</li> <li>Monitor Your Synapse Workspace</li> </ul>"},{"location":"BestPractices/","title":"Best Practices","text":""},{"location":"BestPractices/#what-is-the-best-file-format-to-use-for-large-data-files","title":"What is the best file format to use for large data files?","text":"<p>Recommend using newer format like Parquet because it does save larger datesets in a smaller file in comparison to a CSV file. If only accessing certain sections of the dataset, it is also faster using Parquet as it uses columnar storage format.</p>"},{"location":"BestPractices/#do-i-need-a-sql-database","title":"Do I need a SQL database?","text":"<p>In many cases a SQL database is not needed, data can be saved in files to the datalake.</p>"},{"location":"BestPractices/#do-i-need-a-sql-database-when-using-power-bi","title":"Do I need a SQL database when using Power BI?","text":"<p>It is not needed to have an SQL datbase when using Power BI. You are able to read files from the Azure Storage. A database is only needed when you are using a more complex star-schema like system. </p> <p>Additional Information:</p> <ul> <li>How to connect to the internal data lake with Power BI Desktop.</li> </ul>"},{"location":"BestPractices/#how-should-we-structure-our-projects-data-lake-container","title":"How should we structure our projects data lake container?","text":"<p>There are 3 parts in which to structure your data lake container:</p>"},{"location":"BestPractices/#bronzeraw-zone","title":"Bronze/Raw Zone","text":"<p>This zone stores the original format of any files or files/data that is immutable. The data contained in this zone is usually locked and are only accessible to certain members or is read-only. This zone is also organised in different folders per source system, with each ingestion process having a write access to only their associated folder.</p>"},{"location":"BestPractices/#silvercleansed-zone","title":"Silver/Cleansed Zone","text":"<p>This zone is where parts of data removes unnecessary columns from the data, validates, standarizes and harmonises that data within this zone. This zone is mainly a folder per project. Any data that must be accessed within this zone is usually granted read-only access.</p>"},{"location":"BestPractices/#goldcurated-zone","title":"Gold/Curated Zone","text":"<p>This zone is mainly for analytics rather than data ingestion or processing. The data in the curated zone is stored in star schemas. The dimensional modelling is usually done using Spark or Data Factory instead of inside the database engine. But if the dimensional modelling is done outside of the lake then it is best to publish the model back to the lake. This zone is best suited to run for large-scale queries and analysis that do not have strict time-sensitive reporting needs.</p>"},{"location":"BestPractices/#laboratory-zone","title":"Laboratory Zone","text":"<p>This zone is mainly for experimentation and exploration. It is used for prototype and innovation mixing both your own data sets with data sets from production. This zone is not a replacement for a development or test data lake which is required for more careful development. Each wil data lake project would have their own laboratory area via a folder. Permissions in this zone are typically read and write for each user/project.</p> <p>For more information about structuring your projects data lake container:</p> <ul> <li>Building your Data Lake on Azure Data Lake Storage gen2</li> <li>Designing an Azure Data Lake Store Gen2</li> </ul>"},{"location":"BestPractices/#i-get-an-out-of-memory-exception-in-databricks","title":"I get an out of memory exception in Databricks?","text":""},{"location":"BestPractices/#option-1","title":"Option 1:","text":"<p>The fastest and most expensive way to fix this is to increase the size of your cluster.</p> <p>To increase the size of the cluster, please contact the CAE support team to increase the size of the cluster</p>"},{"location":"BestPractices/#option-2","title":"Option 2:","text":"<p>For a more programatic answer, if you are using pandas, it is also a suggestion to switch over and use pySpark or koalas. PySpark and koalas can run faster than pandas, it has better benefits from using data ingestion pipelines and also works efficiently as it runs parallel on different nodes in a cluster.</p> <p>More information: </p> <ul> <li>Spark Dataframe and Operations</li> </ul>"},{"location":"BestPractices/#option-3","title":"Option 3:","text":"<p>Consider to use a subset of your data when doing queries if possible. If you are working with only a certain section of the dataset but are quering through all of it, it is possible to use just the subset. </p>"},{"location":"BestPractices/#option-4","title":"Option 4:","text":"<p>Consider changing the file format to something like Parquet or Avro which uses less space than a traditional CSV file.</p> <p>Conversion from CSV to Parquet:</p> <pre><code>%python\n\ntestConvert = spark.read.format('csv').options(header='true', inferSchema='true').load('/mnt/public-data/incoming/titanic.csv')\ntestConvert.write.parquet(\"/mnt/public-data/incoming/testingFile\")\n</code></pre> <p>Conversion from CSV to Avro:</p> <pre><code>%python\n\ndiamonds = spark.read.format('csv').options(header='true', inferSchema='true').load('/mnt/public-data/incoming/titanic.csv')\ndiamonds.write.format(\"avro\").save(\"/mnt/public-data/incoming/testingFile\")\n</code></pre>"},{"location":"BestPractices/#how-can-i-easily-convert-sas-code-to-python-or-r","title":"How can i easily convert SAS code to Python or R?","text":"<p>It is not possible to easily convert SAS code to Python or R automatically, the only known way to convert is to manually do the conversion. </p>"},{"location":"BestPractices/#how-do-i-validate-that-i-am-developing-my-application-in-the-most-cost-effective-way-in-the-cloud-using-microsoft-technologies-cae","title":"How do I validate that I am developing my application in the most cost effective way in the cloud using Microsoft technologies (CAE)?","text":"<p>There are plenty of ways to validate that your development is the most cost effective it can be:</p> <ol> <li> <p>Take advantage of Spark in databricks.</p> <p>a. Spark is a great addition to databricks that runs faster and better especially for large data sets. Using Spark would cost less because it does take less time to do its task. Using spark will also</p> </li> <li> <p>Make sure you cluster is running for the minimal amout of time.</p> <p>a. If the cluster is no longer needed or not being use, ensure that it is not running and only run when it is needed.</p> </li> <li> <p>Ensure your databricks cluster is correctly sized.</p> <p>a. Make sure that you have to correct amount of workers in your cluster, too many clusters results in a higher cost.</p> </li> <li> <p>Delete data files that you are not using.</p> <p>a. Ensure that any files that are no longer needed or not in use anymore are deleted from the container.</p> </li> <li> <p>Try not to do processing on a cloud VM.</p> </li> <li> <p>Ask for a review of your architecture.</p> </li> <li> <p>Code review.</p> </li> <li> <p>If you are using Pandas, it is a good idea to switch over to Koalas.</p> </li> <li> <p>Use a file format that is optimal for your work load (i.e. Parquet, Avro)</p> </li> </ol>"},{"location":"BestPractices/#how-should-data-be-structured-if-we-plan-to-use-power-bi","title":"How should data be structured if we plan to use Power BI?","text":"<p>Data should be structured using the Star Schema.</p> <p>For more details about using Star Schema, click this link: </p> <p>Understand star schema and the importance for Power BI.</p>"},{"location":"BestPractices/#how-to-read-in-an-excel-file-from-databricks","title":"How to read in an Excel file from Databricks?","text":"<p>Here is an example of how to read an Excel file using Python: </p> <pre><code>%python\nimport pandas as pd\npd.read_excel(\"/dbfs/mnt/ccei-ccie-ext/Daily charts.xlsx\", engine='openpyxl')\n</code></pre>"},{"location":"BestPractices/#which-file-types-are-best-to-use-when","title":"Which file types are best to use when?","text":""},{"location":"BestPractices/#parquet","title":"Parquet","text":"<p>It is good to use for very large datasets. It is also good to use if only a section of the dataset is needed which reads in the data in a faster rate.</p> <p>Read:</p> <pre><code>%python\ndata = spark.read.parquet(\"/tmp/testParquet\")\ndisplay(data)\n</code></pre> <p>Write:</p> <pre><code>%python\n# Assumption that a dataframe has been created already\n\ndata.write.parquet(\"/tmp/tempFile\")\n</code></pre>"},{"location":"BestPractices/#avro","title":"Avro","text":"<p>Just as Parquet, it is great for very large datasets. To compare, it is better used for editing/writing into a dataset and for querying all columns in the dataset.</p> <p>Read:</p> <pre><code>data = spark.read.format(\"avro\").load(\"/tmp/test_dataset\")\ndisplay(data)\n</code></pre> <p>Write:</p> <pre><code>%scala\nval ex = Seq((132, \"baseball\"),\n(148, \"softball\"),\n(172, \"slow pitch\")).toDF(\"players\", \"sport\")\nex.write.format(\"avro\").save(\"/tmp/testExample\")\n</code></pre>"},{"location":"BestPractices/#csv","title":"CSV","text":"<p>It is fine to use with marginally smaller datasets as CSV files do not load well when the file size is very large. But with smaller data sets, it is simple and human-readable. For writing within a CSV file, it is also good to note that you are able to edit the file with Office. </p> <p>Read:</p> <pre><code>%python\ndata = spark.read.format('csv').options(header='true', inferSchema='true').load('/tmp/test_dataCSV.csv')\ndisplay(data)\n</code></pre>"},{"location":"BestPractices/#excel","title":"Excel","text":"<p>Please see above on how to use Excel.</p> <p>The other formats above are perferable over excel.</p>"},{"location":"BestPractices/#how-to-convert-files-csv-text-json-to-parquet-using-databricks","title":"How to convert files (CSV, Text, JSON) to parquet using databricks?","text":"<p>The rule of thumb in converting a file to parquet is to first read in the file and then write a new file into parquet</p> <p>CSV to Parquet: </p> <pre><code>%python\n\ntestConvert = spark.read.format('csv').options(header='true', inferSchema='true').load('/mnt/public-data/incoming/titanic.csv')\ntestConvert.write.parquet(\"/mnt/public-data/incoming/testingFile\")\n</code></pre> <p>JSON to Parquet:</p> <pre><code>%python\n\ntestConvert = spark.read.json('tmp/test.json')\ntestConvert.write.parquet('tmp/testingJson')\n</code></pre> <p>Text to Parquet:</p> <pre><code>%python\n\ntestConvert = spark.read.text(\"/mnt/public-data/incoming/testing.txt\")\ntestConvert.write.parquet(\"/mnt/public-data/incoming/testingFile\")\n</code></pre>"},{"location":"BestPractices/#can-i-read-word-document-in-databricks","title":"Can I read Word document in Databricks?","text":"<p>It is best practice to read Word documents via Office instead.</p>"},{"location":"BestPractices/#when-should-we-use-adf-vs-databricks-for-data-ingestion","title":"When should we use ADF vs. Databricks for data ingestion?","text":"<p>Databricks is able to do real-time streaming through the Apache Spark API that can handle the streaming analytics workloads. Databricks does not need you to wrap the python code into functions or executable modules, all the code is able to work just as is. Databricks also supports Machine Learning which makes data ingestion easier as well.</p> <p>For any code that is already in an Azure Function or is easily translated into an executable, using data factory is usable. Data factory is also good to use if it is a heavy algorithm that is not usable within Databricks. </p>"},{"location":"BestPractices/#what-is-the-difference-between-sql-database-temporal-tables-and-delta-lake","title":"What is the difference between SQL database temporal tables and Delta Lake?","text":"<p>SQL temporal tables is specific to SQL 2018 and is not currently available in Azure Synapse. On the other hand, Delta lake is available in both Azure Synapse and in Databricks. Another difference is that SQL temporal tables are only available with only SQL queries while Delta lake time travel is available in Scala, Python, and SQL. </p>"},{"location":"BestPractices/#when-to-use-power-bi-or-r-shiny","title":"When to use Power BI or R-Shiny?","text":"<p>It is recommended to use Power BI over R-shiny because less coding is required when using Power BI. There are a lot of benefits to using Power BI including the additional amount of chart types that are at hand, visualisation of data into charts is easier to use in Power BI compared to R-Shiny, the creation of a dashboard is faster within PowerBI, and the ease of connectivity with other applications within Azure.</p>"},{"location":"BestPractices/#when-is-a-good-time-to-use-azure-synapse-vs-adf-and-databricks","title":"When is a good time to use Azure Synapse vs. ADF and Databricks?","text":"<p>Azure Synapse is good to use when doing queries and data analysis via the data lake, doing SQL analyses and data warehousing, and using additional services like Power BI. It is easy to query data from the data lake using Azure Snapse and you do not have to mount the data lake to the workspace. As for data analyses and data warehousing, synapse is perferred as it allows full realtional data models, provide all SQL features and also uses Delta Lake. Synapse also includes direct services with Power BI for ease of use.</p> <p>On the other hand, Databricks is preferred when doing machine learning development and real-time transformations. Databricks includes their own machine learning development that includes popular libraries like PyTorch, manage version of MLflow.  Databricks is also preferred for real-time transformations as it uses Spark structured streaming and it gives you the ability to view changes from other users in real time.</p>"},{"location":"BestPractices/#when-should-we-use-a-sql-database-data-warehouse-vs-delta-lake","title":"When should we use a SQL database data warehouse vs. Delta Lake?","text":"<p>Best practice would be to use Delta lake over SQL server as it does not use additional SQL compute resouces and will reduce the overall cloud costs.</p>"},{"location":"BestPractices/#how-can-i-easily-convert-sas-files-to-another-format","title":"How can i easily convert SAS files to another format?","text":"<p>Statcan users can use SAS on the internal stats-can network to convert it to a supported file form. </p> <p>You are able to convert a SAS file to CSV or JSON with this method:</p> <ol> <li> <p>First open databricks and install the sas7bdat-converter within your notebook.</p> <p><code>python %pip install sas7bdat-converter</code></p> </li> <li> <p>Using python and your code editor of your choice, type in this code with the file directory that the file is in and the directory where you want the output file to be in.</p> <p>```python %python</p> <p>import sas7bdat_converter</p> <p>file_dicts = [{     'sas7bdat_file': '/dbfs/mnt/public-data/ToNetA/sas7bdat/tablea_1_10k.sas7bdat',     'export_file': '/dbfs/mnt/public-data/testFolder/testingConvert.csv', }]</p> <p>sas7bdat_converter.batch_to_csv(file_dicts) ```</p> </li> </ol> <p>You will then get the output file within the directory you have specified.</p> <p>Converter Documentation:</p> <p>sas7bdat website documentation</p>"},{"location":"BestPractices/#canhow-i-convert-a-word-document-to-a-notebook","title":"Can\\How I convert a Word document to a notebook?","text":"<p>There is no easy way to convert a word document to a notebook.</p> <p>A manual solution to convert a Word document to a notebook is by copying any of the code that is within the word document into a notebook.</p>"},{"location":"BestPractices/#how-big-of-a-dataframespark-table-can-we-store-within-the-workspace","title":"How big of a dataframe/spark table can we store within the workspace?","text":"<p>Spark tables are stored as parquet files and are stored in the internal storage account linked with the Databricks workspace, but it is best practice to delete the table if it is no longer in use.</p>"},{"location":"BestPractices/#what-is-the-best-way-to-get-data-files-into-azure-ml","title":"What is the best way to get data files into Azure ML?","text":"<p>The best way would be to upload your files to the data lake. If you need to add a new cloud storage account, contact the CAE team to add the storage account to the Azure ML studio.</p>"},{"location":"BestPractices/#whats-the-difference-to-machine-learning-in-databricks-or-in-azure-ml","title":"Whats the difference to Machine Learning in Databricks or in Azure ML?","text":"<p>The main difference between Azure ML and Databricks is the language that each application uses. Azure ML utilizes python-based libraries or R while Databricks utilizes the Apache Spark Platform and MLFlow. </p> <p>Azure ML also contains a tracking system which is able to track individual runs of the experiment and include the specific metrics of what wants to be seen. Databricks includes MLflow which also allows tracking but does not come with as many features as Azure ML. </p> <p>As a recommendation, it is best practice to use Databricks for data preperation and for large datasets but to use Azure ML for their tracking system, machine learning on normal datasets, deep learning on GPUs, and operationalization. </p>"},{"location":"BestPractices/#how-do-you-create-a-table-in-databricks","title":"How do you create a Table in Databricks?","text":""},{"location":"BestPractices/#option-1-use-create-table-function","title":"Option 1: Use Create Table function","text":"<p>In Databricks, select Data and within the Database you have selected, click on Create Table.</p> <p></p> <p>For more information about creating tables in Databricks:</p> <p>Databricks Create a Table.</p>"},{"location":"BestPractices/#option-2-create-table-from-dataframe-table","title":"Option 2: Create Table from Dataframe table","text":"<p>Python:</p> <pre><code>df.write.saveAsTable(\"Table-Name\")\n</code></pre> <p>SQL:</p> <pre><code>CREATE TABLE IF NOT EXISTS Table-Name AS SELECT * FROM df\n</code></pre>"},{"location":"BestPractices/#option-3-create-table-programatically","title":"Option 3: Create Table Programatically","text":"<p>SQL:</p> <pre><code>CREATE TABLE example (id INT, name STRING, age INT) USING CSV;\n</code></pre>"},{"location":"BestPractices/#when-to-use-spark-dataframe-or-spark-table","title":"When to use Spark Dataframe or Spark Table?","text":"<p>There are really no difference between using a Spark Dataframe or Spark Table. </p> <p>Currently with Databricks, best practice right now would be to store tables as delta tables as it is saved in parquet format and gives the tracking capabilities.</p>"},{"location":"BestPractices/#what-should-i-do-if-the-size-of-the-broadcasted-table-far-exceeds-estimates-and-exceeds-limit-of-sparkdrivermaxresultsize-_____","title":"What should I do if the size of the broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxResultSize = _____?","text":"<p>Change the Spark configuration \"spark.driver.maxResultSize\" to \"0\" (means no limit) or something larger than your needs.</p>"},{"location":"BestPractices/#what-should-i-do-if-i-cannot-broadcast-the-table-that-is-larger-than-8gb","title":"What Should I do if I cannot broadcast the table that is larger than 8GB?","text":"<p>This occurs only with BroadcastHashJoin. There are 2 options:</p> <ol> <li> <p>Change the Spark configuration \"spark.sql.autoBroadcastJoinThreshold\" to \"-1\". This forces Databricks to perform a SortMergeJoin.</p> </li> <li> <p>Steps to avoid changing configurations:</p> <p>a. Partition DataFrame A into parts.</p> <p>b. Perform joins with each parittion from DataFrame A with DataFrame B (concurrently is the fastest way but may require writing Dataframes to file for reading in next step).</p> <p>c. Perform a union on all the joined DataFrames.</p> </li> </ol>"},{"location":"BestPractices/#note-about-changing-spark-configuration","title":"Note About changing Spark Configuration","text":"<p>Warning: Changing Spark configuraitons can cause out-of-memory-errors</p> <p>Normal approach:</p> <pre><code>- spark.conf.set(\"configuration\", \"value\")\n</code></pre> <p>If you do not have permissions to change some configurations, this seems to be a work around:</p> <pre><code>- conf = spark.sparkContext._cibf,setAkk([(\"configuration\", \"value\"), (\"configuration\", \"value\")])\n</code></pre> <p>How to get Spark Configuration:</p> <pre><code>- spark.conf.get(\"configuration\")\n</code></pre>"},{"location":"ContactUs/","title":"Contact Us","text":""},{"location":"ContactUs/#data-analyitics-services-das-portal","title":"Data Analyitics Services (DAS) Portal","text":"<p>Please use the Data Analytics Services (DAS) portal Help button to submit a request for assitance .   </p>"},{"location":"ContactUs/#feedback","title":"Feedback","text":"<p>Use the Feedback form on this website:</p> <ul> <li>Feedback Form</li> </ul>"},{"location":"ContactUs/#documentation","title":"Documentation","text":"<ul> <li>Statistics Canada - Collaborative Analytics Environment (CAE)</li> <li>Frequently Asked Questions (FAQ)</li> <li>Best Practices</li> </ul>"},{"location":"ContactUs/#microsoft-documentation","title":"Microsoft Documentation","text":"<ul> <li>Azure documentation</li> </ul>"},{"location":"Dashboards/","title":"Dashboards","text":"<p>Dashboards are a focused and organized view of your cloud resources in the Azure portal. They serve as a workspace where you can quickly launch tasks for day-to-day operations and monitor resources. Build custom dashboards based on projects, tasks, or user roles, for example.</p> <p>The Azure portal provides a default dashboard as a starting point. You can edit the default dashboard. Create and customize additional dashboards, and publish and share dashboards to make them available to other users.</p>"},{"location":"Dashboards/#access-the-collaborative-analytics-environment-dashboard","title":"Access the Collaborative Analytics Environment Dashboard","text":"<ol> <li> <p>From the Azure portal menu, select Dashboard. Your default view might already be set to dashboard. </p> </li> <li> <p>Select the arrow next to the dashboard name.  </p> </li> <li> <p>Select the Collaborative Analytics Environment dashboard from the displayed list of dashboards. If this dashboard isn't listed:  </p> <p>a. Select Browse all dashboards.  </p> <p> </p> <p>b. In the Type field, select Shared dashboards.  </p> <p> </p> <p>c. Ensure the list of selected subsciptions includes the vdl subscription. You can also enter text to filter dashboards by name.  </p> <p>d. Select the Collaborative Analytics Environment dashboard from the list of shared dashboards.</p> <p></p> </li> </ol>"},{"location":"Dashboards/#accessing-databricks","title":"Accessing Databricks","text":"<ol> <li> <p>Click the on the Databricks Workspace that you have access to.     </p> </li> <li> <p>From the overview page, you can then click on the URL as shown here     </p> </li> <li> <p>Important: Do not click the Launch Workspace button,  you might receive an error message.</p> </li> </ol>"},{"location":"Dashboards/#accessing-synapse","title":"Accessing Synapse","text":"<p>Click the on the Synapse Workspace that you have access to.     </p>"},{"location":"Dashboards/#accessing-azure-machine-learning-ml","title":"Accessing Azure Machine Learning (ML)","text":"<p>Click the on the Azure Machine Learning Workspace that you have access to.     </p>"},{"location":"Dashboards/#accessing-azure-storage","title":"Accessing Azure Storage","text":"<p>Click the on the Storage account that you have access to.     </p>"},{"location":"Dashboards/#accessing-azure-devtest-labs","title":"Accessing Azure DevTest Labs","text":"<p>Click the on the DevTestLab that you have access to.     </p>"},{"location":"Dashboards/#microsoft-documentation","title":"Microsoft Documentation","text":"<ul> <li>Create a dashboard in the Azure portal</li> </ul>"},{"location":"DataBricks/","title":"Azure Databricks","text":""},{"location":"DataBricks/#getting-started","title":"Getting Started","text":"<p>Once inside Databricks you can create a new notebook or open an existing notebook. See First Access to Databricks for more information.</p>"},{"location":"DataBricks/#creating-a-cluster","title":"Creating a Cluster","text":"<p>Since you do not have permission to create a cluster, please Contact Us if a cluster has not been created for you or if you require changes to your cluster.</p> <p>Note: You must have a cluster running before you can run code inside your notebook. See below or the FAQ for information on how to start a cluster.</p>"},{"location":"DataBricks/#creating-a-notebook","title":"Creating a Notebook","text":"<ul> <li> <p>One way to create a notebook is to click on the New Notebook link from the main Databricks page. You can then provide a name for your notebook and select the default notebook language.</p> </li> <li> <p>From the available list of clusters, select the cluster to which you wish to attach your notebook.</p> <p></p> </li> <li> <p>To start or change a cluster from within a notebook, open the notebook and click on the cluster drop down found at the top right of the notebook. You can then start the cluster or detach it and attach a different one.  </p> </li> </ul>"},{"location":"DataBricks/#sharing-a-databricks-notebook","title":"Sharing a Databricks Notebook","text":"<p>To share a notebook or invite other collaborators, right-click on a specific notebook file or folder from the Workspace menu, and select Permissions. You can also do this by clicking on the Permissions button from within a notebook. Once shared, multiple authors can participate in the same notebook session and co-author at the same time.  </p> <p>Note: To add a user to the Databricks workspace, please Contact Us.  </p> <p></p>"},{"location":"DataBricks/#ingesting-data-into-databricks","title":"Ingesting Data into Databricks","text":"<p>Data can be mounted or uploaded to the Databricks File System (DBFS), which is storage specific to the Databricks workspace. You can read data from a data source or even upload a data file (e.g. CSV) directly to the DBFS.</p> <p>Note: The internal data lake container for your environment has already been mounted for you and you can work with the container directly. Please Contact Us if you don't know the name of your mounted data lake container.</p>"},{"location":"DataBricks/#adding-data-to-databricks","title":"Adding Data to Databricks","text":""},{"location":"DataBricks/#reading-mounted-files","title":"Reading Mounted Files","text":"<p>Example:</p> <pre><code>%python\ntestData = spark.read.format('csv').options(header='true', inferSchema='true').load('/mnt/mad-du/incoming/age-single-years-2018-census-csv.csv')\n\ndisplay(testData)\n</code></pre>"},{"location":"DataBricks/#changing-notebook-default-language","title":"Changing Notebook Default Language","text":""},{"location":"DataBricks/#mixing-notebook-languages","title":"Mixing Notebook Languages","text":"<p>You can override the default language by specifying the language magic command % at the beginning of a cell. The supported magic commands are: %python, %r, %scala, and %sql.</p> <p>Note: When you invoke a language magic command, the command is dispatched to the REPL in the execution context for the notebook. Variables defined in one language (and hence in the REPL for that language) are not available in the REPL of another language. REPLs can share state only through external resources such as files in DBFS or objects in object storage.    </p> <p>Notebooks also support a few auxiliary magic commands: %sh: Allows you to run shell code in your notebook. To fail the cell if the shell command has a non-zero exit status, add the -e option. This command runs only on the Apache Spark driver, and not the workers. To run a shell command on all nodes, use an init script. %fs: Allows you to use dbutils filesystem commands. %md: Allows you to include various types of documentation, including text, images, and mathematical formulas and equations.  </p>"},{"location":"DataBricks/#starting-a-databricks-cluster","title":"Starting a Databricks Cluster","text":"<ol> <li> <p>Click on the cluster drop-down list.</p> </li> <li> <p>Select a cluster from the list.</p> </li> <li> <p>Click on the Start Cluster link.  </p> <p> </p> </li> </ol>"},{"location":"DataBricks/#databricks-connect-vm-setup","title":"Databricks Connect VM Setup","text":"<p>Databricks connect is a method for accessing a Databricks environment without having to connect through the Azure Portal or the Databricks UI. It allows you to use other IDEs to work on Databricks code.</p> <p>The following are the steps for installing and testing Databricks Connect on your virtual machine (VM):</p> <ol> <li> <p>Databricks Connect conflicts with the Pyspark installation found on the Data Science Virtual Machine images. The default path for this Pyspark installation is <code>C:\\dsvm\\tools\\spark-2.4.4-bin-hadoop2.7</code>. Either delete or move this folder in order to install Databricks Connect.</p> </li> <li> <p>Before installing Databricks Connect, create a conda environment. To do this, open a command prompt and run the following commands:</p> </li> </ol> <pre><code>    conda create --name dbconnect python=3.7\nconda activate dbconnect\ntype pip install -U databricks-connect==X.Y.*\n</code></pre> <p>NOTE: Replace X and Y with the version number of the Databricks cluster. To find this value, open the Databricks workspace from the Azure portal, click on Clusters on the left of the page, and note the Runtime version for your cluster.</p> <ol> <li> <p>In a command prompt, type databricks-connect configure, then enter the following values when prompted:</p> <ul> <li> <p>Databricks Host: <code>https://canadacentral.azuredatabricks.net</code></p> </li> <li> <p>Databricks Token: a personal access token generated in your Databricks Workspace User Settings</p> </li> <li> <p>Cluster ID: the value found under Cluster --&gt; Advanced Options --&gt; Tags in your Databricks workspace.</p> <p></p> </li> <li> <p>Org ID: the part of the Databricks URL found after .net/?o=</p> <p></p> </li> <li> <p>Port: keep the existing value</p> </li> </ul> </li> <li> <p>Change the <code>SPARK_HOME</code> enviroment variable to <code>c:\\miniconda\\envs\\(conda env name))\\lib\\site-packages\\pyspark</code>, and restart your VM. (Please Contact Us if you do not know how to change environment variables.)</p> </li> <li> <p>Test the connectivity to Azure Databricks by running databricks-connect test in a command prompt. If your Databricks cluster is not running when you start this test you will receive warning messages until it has started, which can take some time.</p> </li> </ol>"},{"location":"DataBricks/#troubleshooting","title":"Troubleshooting :","text":"<ul> <li>If you are using databricks connect on windows and you get an error saying: Cannot find winutils.exe please refer to the Databricks documentation on the error.</li> </ul>"},{"location":"DataBricks/#installing-libraries","title":"Installing Libraries","text":""},{"location":"DataBricks/#databricks-cluster","title":"Databricks Cluster","text":"<p>Please Contact Us to have the support team install these libraries for you.</p>"},{"location":"DataBricks/#notebook","title":"Notebook","text":"<p>Use the following commands to install a library in a notebook session:</p> <p>Python: </p> <pre><code>dbutils.library.installPyPI(\"pypipackage\", version=\"version\", repo=\"repo\", extras=\"extras\")\ndbutils.library.restartPython() # Removes Python state, but some libraries might not work without calling this function\n</code></pre> <p>R Code:</p> <pre><code>install.packages(\"library\") </code></pre>"},{"location":"DataBricks/#microsoft-documentation","title":"Microsoft Documentation","text":"<ul> <li>First Access to Databricks </li> <li>For more information on Databricks </li> <li>Databricks Connect</li> <li>Install Libraries in Current Notebook Session </li> <li>Library Management for Admins </li> </ul>"},{"location":"DataFactory/","title":"Azure Data Factory","text":""},{"location":"DataFactory/#access-data-factory","title":"Access Data Factory","text":""},{"location":"DataFactory/#dashboard","title":"Dashboard","text":"<p>See the Dashboard section of this documentation from more information. </p> <ol> <li> <p>Click on the Dashboard menu from the Azure Portal.</p> <p> </p> </li> </ol>"},{"location":"DataFactory/#adf-url","title":"ADF URL","text":"<ol> <li> <p>Navigate to https://adf.azure.com, and select the Data Factory instance that was created for you.  </p> <p> </p> </li> </ol>"},{"location":"DataFactory/#azure-portal","title":"Azure Portal","text":"<ol> <li> <p>In the Azure Portal Search box, search for Data factories.  </p> <p> </p> </li> <li> <p>You should then see a list of the Data Factories you were given permission to access.  </p> <p> </p> </li> </ol>"},{"location":"DataFactory/#authoring","title":"Authoring","text":"<p>Click on Author and Monitor.  </p> <p> </p> <p>In Data Factory, you have the ability to author and deploy resources.     </p> <p> </p> <p>See Visual authoring in Azure Data Factory for more information.  </p> <p>You can also use some of the various wizards provided on Data Factory Overview page.</p> <p>NOTE: Configuring SSIS Integration is NOT recommended. Contact the support team through the Slack channel if you have questions.</p> <p> </p> <p>See Azure Documentation Tutorials for more details.  </p>"},{"location":"DataFactory/#access-the-data-lake-from-adf","title":"Access the Data Lake from ADF","text":"<p>A Data Lake connection has been pre-configured for your environment.   </p> <ol> <li> <p>Click on Manage.  </p> </li> <li> <p>Click on Linked Services.  </p> </li> <li> <p>The linked service with the Azure Data Lake Storage Gen2 type is your Data Lake.  </p> <p> </p> </li> </ol> <p>Note: You have been granted access to specific containers created in the Data Lake for your environment.</p>"},{"location":"DataFactory/#access-azure-sql-database","title":"Access Azure SQL Database","text":"<p>Some projects have an Azure SQL Database instance.  </p> <ol> <li> <p>Click on Manage.  </p> </li> <li> <p>Click on Linked Services.  </p> </li> <li> <p>The linked service(s) with the Azure SQL Database type is / are your Database(s) </p> <p></p> </li> </ol>"},{"location":"DataFactory/#save-publish-your-data-factory-resources","title":"Save / Publish Your Data Factory Resources","text":"<p>Azure Data Factory can be configured to save your work to the following locations: - Git repository  - Publish directly to Data Factory</p>"},{"location":"DataFactory/#git-when-supported","title":"Git (when supported)","text":"<p>When Git is enabled you can see your configuration and save your work to a specific branch.  </p> <ol> <li> <p>Click on Manage </p> </li> <li> <p>Click on Git Configuration.  </p> </li> <li> <p>See the Git configuration that was setup for you:   </p> <p> </p> </li> <li> <p>When authoring a workflow it can be saved to your branch. Click on + New branch from this branch dropdown to create a new feature branch.   </p> <p> </p> </li> <li> <p>When you are ready to merge the changes from your feature branch to your collaboration branch (master), click on the branch dropdown and select Create pull request. This action takes you to Azure DevOps Git Repo where you can create pull requests, do code reviews, and merge changes to your collaboration branch (master) after the pull request has been approved.  </p> </li> <li> <p>After you have merged changes to the collaboration branch (master), click on Publish to publish your code changes from the master branch to Azure Data Factory. Contact the support team through the Slack channel if you receive an error when trying to Publish.  </p> </li> </ol>"},{"location":"DataFactory/#data-factory-service","title":"Data Factory Service","text":"<p>When Data Factory is not integrated with source control your workflows are stored directly in the Data Factory service and you cannot save partial changes, you can only Publish all which overwrites the current state of the Data Factory with your changes, which are then visible to everyone.    </p> <p></p>"},{"location":"DataFactory/#ingest-and-transform-data-with-adf","title":"Ingest and Transform Data with ADF","text":"<ul> <li>Copy Data Wizard documentation link </li> <li>Mapping Data Flows \u2013 GUI-driven ETL documentation link</li> </ul>"},{"location":"DataFactory/#integration-runtimes","title":"Integration Runtimes","text":""},{"location":"DataFactory/#autoresolveintegrationruntime","title":"AutoResolveIntegrationRuntime","text":"<p>Do not use. Please use the canadaCentralIR-4nodesDataFlow or selfHostedCovidIaaSVnet runtimes instead.</p> <p>The auto resolve runtime is created by default with the data factory instance, and will auto resolve to the Azure Data Centre closest to the data, which may violate data residency policies.</p>"},{"location":"DataFactory/#canadacentralir-4nodesdataflow","title":"canadaCentralIR-4nodesDataFlow","text":"<p>This is shared by all users and runs all the time.</p>"},{"location":"DataFactory/#can-access","title":"Can Access:","text":"<ul> <li>Internal Data Lake </li> <li>External Storage Account</li> <li>External Data Sources (Internet)</li> </ul>"},{"location":"DataFactory/#cannot-access","title":"Cannot Access:","text":"<ul> <li>Azure SQL Database</li> </ul>"},{"location":"DataFactory/#selfhostedcovidiaasvnet","title":"selfHostedCovidIaaSVnet","text":"<p>Located inside CAE virtual network (VNet). </p>"},{"location":"DataFactory/#can-access_1","title":"Can Access:","text":"<ul> <li>Internal Data Lake</li> <li>SQL Server</li> </ul>"},{"location":"DataFactory/#cannot-access_1","title":"Cannot Access:","text":"<ul> <li>External Storage Account</li> <li>External Data Sources (Internet)</li> </ul>"},{"location":"DataFactory/#example-how-to-connect-john-hopkins-data","title":"Example: How to connect John Hopkins Data","text":"<ol> <li> <p>There is an example workflow that shows how to ingest data from GitHub using a Data Factory Pipeline.  </p> <p> </p> </li> <li> <p>Data can be filtered from within Data Factory.  </p> <p></p> </li> <li> <p>Alternatively, data can be pulled from GitHub using code in a Databricks notebook.  </p> <p></p> </li> </ol>"},{"location":"DataFactory/#microsoft-documentation","title":"Microsoft Documentation","text":"<ul> <li>Introduction to Azure Data Factory - Azure Data Factory </li> <li>Create an Azure data factory using the Azure Data Factory UI - Azure Data Factory  </li> <li>Copy data by using the Azure Copy Data tool - Azure Data Factory </li> <li>Create a mapping data flow - Azure Data Factory </li> <li>Expression functions in the mapping data flow - Azure Data Factory </li> <li>Mapping data flow Debug Mode - Azure Data Factory </li> <li>Mapping data flow Visual Monitoring - Azure Data Factory </li> </ul>"},{"location":"DataFactory/#youtube-videos","title":"YouTube Videos","text":"<ul> <li>Ingest, prepare &amp; transform using Azure Databricks &amp; Data Factory | Azure Friday </li> <li>Azure Friday | Visually build pipelines for Azure Data Factory V2 </li> <li>How to prepare data using wrangling data flows in Azure Data Factory | Azure Friday </li> <li>How to develop and debug with Azure Data Factory | Azure Friday </li> <li>Building Data Flows in Azure Data Factory</li> </ul>"},{"location":"DeltaLake/","title":"Delta Lake","text":"<p>Delta Lake is an open-source storage layer that runs on top of an existing data lake, adding the capabilities of ACID (atomicity, consistency, isolation, durability) properties and transactions. Delta Lake is fully compatible with Apache Spark in Azure Databricks and Synapse.</p> <p>Azure Data Lake is not ACID compliant, so Delta Lake should be used wherever data integrity and reliability are essential, or when there is a risk of bad data.</p>"},{"location":"DeltaLake/#microsoft-documentation","title":"Microsoft Documentation","text":"<ul> <li>What is Delta Lake</li> <li>Delta Lake on Azure</li> </ul>"},{"location":"DeltaLake/#official-documentation","title":"Official Documentation","text":"<ul> <li>Delta Lake Documentation</li> </ul>"},{"location":"DeltaLake/#how-delta-lake-works","title":"How Delta Lake Works","text":"<p>A delta lake is basically a folder inside the data lake containing log files (in the sub-folder _delta_log) and data files (stored in parquet format in the root folder) for each version of a table. As long as the log and data files exist, you can use the time travel feature to query previous versions of a delta table, and view the history of that table.</p> <p>If the log files are deleted, you will not be able to read the table at all. To fix this, you will need to empty the delta lake folder (delete everything in it), then write your original data file to it to start over.</p> <p>Delta works at the table level, so multi-table queries and joins are not supported.</p>"},{"location":"DeltaLake/#when-to-use-delta-lake","title":"When to Use Delta Lake","text":"<p>Delta lake is best to use:</p> <ul> <li>for any permanent table in Databricks;</li> <li>for large amounts of semi-structured data (10 million+ records to get the most performance benefits); or</li> <li>when you want version control and/or data access tracking (delta log files keep track of every time the data is modified and by whom).</li> </ul>"},{"location":"DeltaLake/#time-travel","title":"Time Travel","text":"<p>You can use time travel to query an older snapshot of a table, either by version number or timestamp. By default, data files are stored for 30 days.</p> <p>Example:</p> <p>SQL</p> <pre><code>SELECT * FROM example_table TIMESTAMP AS OF '2018-10-18T22:15:12.013Z'\nSELECT * FROM delta.`/delta/example_table` VERSION AS OF 12\n</code></pre> <p>Python</p> <pre><code>df1 = spark.read.format(\"delta\").option(\"timestampAsOf\", 2020-03-13).load(\"/delta/example_table\")\ndf2 = spark.read.format(\"delta\").option(\"timestampAsOf\", 2019-01-01T00:00:00.000Z).load(\"/delta/example_table\")\ndf3 = spark.read.format(\"delta\").option(\"versionAsOf\", version).load(\"/delta/example_table\")\n</code></pre>"},{"location":"DeltaLake/#removing-old-data-files","title":"Removing Old Data Files","text":"<p>To remove old data files (not log files) that are no longer referenced by a delta table, you can run the vacuum command.</p> <p>Example:</p> <p>SQL</p> <pre><code>VACUUM example_table   -- vacuum files not required by versions older than the default retention period\n\nVACUUM '/data/example_table' -- vacuum files in path-based table\n\nVACUUM delta.`/data/example_table/`\n\nVACUUM delta.`/data/example_table/` RETAIN 100 HOURS  -- vacuum files not required by versions more than 100 hours old\n\nVACUUM example_table DRY RUN    -- do dry run to get the list of files to be deleted\n</code></pre> <p>Python</p> <pre><code>from delta.tables import *\n\ndeltaTable = DeltaTable.forPath(spark, pathToTable)\n\ndeltaTable.vacuum()        # vacuum files not required by versions older than the default retention period\n\ndeltaTable.vacuum(100)     # vacuum files not required by versions more than 100 hours old\n</code></pre> <p>Scala</p> <pre><code>import io.delta.tables._\n\nval deltaTable = DeltaTable.forPath(spark, pathToTable)\n\ndeltaTable.vacuum()        // vacuum files not required by versions older than the default retention period\n\ndeltaTable.vacuum(100)     // vacuum files not required by versions more than 100 hours old\n</code></pre>"},{"location":"DeltaLake/#revert-back-to-a-previous-version","title":"Revert Back to a Previous Version","text":"<p>You can revert back to and work from a previous version of your table by using the time travel feature to read in your target version as a dataframe, then write it back to the delta lake folder. </p> <p>This will create a new version that is identical to the target version, which you can then work from. Other previous versions remain intact.</p> <p>Example:</p> <p>Python</p> <pre><code># read in old version of table\ndf = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n\n# write back to new version of table, must set mode to \"overwrite\"\ndf.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n</code></pre>"},{"location":"DeltaLake/#official-documentation_1","title":"Official Documentation","text":"<ul> <li>Query an older snapshot of a table (time travel)</li> <li>Remove files no longer referenced by a delta table</li> </ul>"},{"location":"DeltaLake/#using-delta-in-databricks","title":"Using Delta in Databricks","text":"<p>Databricks has native support for Delta Lake, and can run queries using Python, R, Scala, and SQL.</p> <ol> <li> <p>You first need to create a directory to store the delta files, and keep note of the path to this directory.</p> </li> <li> <p>Read in your data file, then write it to \"delta\" format and save it in the directory created above.     ```     # read data file     testData = spark.read.format('json').options(header='true', inferSchema='true', multiline='true').load('/mnt/public-data/incoming/covid_tracking.json')</p> </li> <li> <p>Optional (not a best practice): Create an SQL table using delta:     <code>spark.sql(\"CREATE TABLE sample_table USING DELTA LOCATION '/mnt/public-data/delta/'\")</code></p> </li> <li> <p>Now you can run SQL queries on your delta table, including querying by version number or timestamp to \"time travel\" to previous versions of your data. If you created a table in step 3, you can run queries using the table name. Otherwise (best practice), in place of the table name you can use delta.`{delta_table_path}` (replace {delta_table_path} with the actual path).     ```     %sql     SELECT * FROM sample_table VERSION AS OF 0</p> <p>SELECT * FROM delta.<code>/mnt/public-data/delta/</code> ```</p> </li> </ol>"},{"location":"DeltaLake/#write-to-delta-format","title":"write to delta format","text":"<p>testData.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/public-data/delta\") ```</p>"},{"location":"DeltaLake/#microsoft-documentation_1","title":"Microsoft Documentation","text":"<ul> <li>Delta Lake Quickstart</li> </ul>"},{"location":"DeltaLake/#using-delta-in-azure-synapse","title":"Using Delta in Azure Synapse","text":"<p>Delta Lake is compatible with Azure Synapse. Delta tables can be created and queried within Synapse notebooks similarly to Databricks, with language support for PySpark, Scala, and .NET (C#). Note that SQL is not supported with the current version.</p> <ol> <li> <p>Read in your data file.     <code>data = spark.read.format('csv').options(header='true', inferSchema='true', multiline='true').load('abfss://public-data@statsconviddsinternal.dfs.core.windows.net/incoming/data_duplicate.csv')</code></p> </li> <li> <p>Write to delta format and save to your delta table directory.     <code>data.write.format(\"delta\").save(delta_table_path)</code></p> </li> <li> <p>Optional: Create an SQL table using delta (only required if you want to run SQL queries, not necessary if you're only using Python, Scala, or C#).     <code>spark.sql(\"CREATE TABLE example USING DELTA LOCATION '{0}'\".format(delta_table_path))</code></p> </li> <li> <p>Now you can run queries on your data.</p> </li> </ol>"},{"location":"DeltaLake/#microsoft-documentation_2","title":"Microsoft Documentation","text":"<ul> <li>Work With Delta Lake</li> </ul>"},{"location":"DeltaLake/#using-delta-in-data-factory","title":"Using Delta in Data Factory","text":"<p>You can use Azure Data Factory to copy data to and from a delta lake stored in Azure Data Lake.</p>"},{"location":"DeltaLake/#example-copy-data-to-delta-lake","title":"Example: Copy Data to Delta Lake","text":"<ol> <li>Create a new dataflow and add a source.</li> <li>Under the Source Settings tab, add the dataset that you want to copy from. Configure any other relevant settings.</li> <li>Click the plus button to the right of your source and add a sink.</li> <li>Under the Sink tab, choose Inline as the Sink type, and Delta as the Inline dataset type.</li> <li>Under the Settings tab, set the folder path (the path to where your delta files will be stored).</li> </ol>"},{"location":"DeltaLake/#microsoft-documentation_3","title":"Microsoft Documentation","text":"<ul> <li>Delta Format in Azure Data Factory</li> </ul>"},{"location":"DeltaLake/#using-delta-with-power-bi","title":"Using Delta with Power BI","text":"<p>To read delta tables natively in Power BI, please see this documentation on GitHub.</p>"},{"location":"DeltaLake/#delta-in-azure-machine-learning","title":"Delta in Azure Machine Learning","text":"<p>Delta lake is not currently supported in Azure ML.</p>"},{"location":"DevTestLabs/","title":"Azure Dev Test Labs","text":""},{"location":"DevTestLabs/#azure-dev-test-labs","title":"Azure Dev Test Labs","text":"<p>Please Contact Us to request access to Azure Dev Test Labs. Statistics Canada users should use their Azure Virtual Desktop to access the environment. Alternatively, Dev Test Labs can be requested when the Azure Virtual Desktop does not have the required software.</p>"},{"location":"DevTestLabs/#find-your-devtest-lab","title":"Find Your DevTest Lab","text":"<ol> <li> <p>In your project's custom Dashboard in the Azure Portal, click on the DevTest Lab.    </p> <p> </p> </li> <li> <p>Select the DevTest Lab that was assigned.</p> </li> </ol>"},{"location":"DevTestLabs/#create-your-virtual-machine","title":"Create Your Virtual Machine","text":"<p>Note: In some instances a Virtual Machine will be pre-created for you and you will not have permission to create a virtual machine. See the FAQ if you need to make changes to your virtual machine.  </p> <ol> <li> <p>From the DevTest Lab Overview page, click on the + Add button.  </p> </li> <li> <p>Choose an appropriate base for your VM (e.g., Data Science Virtual Machine - Windows Server 2019). For more details on the software included with the Data Science Virtual Machines, please click here.  </p> </li> <li> <p>Enter a name for your VM and a User name and password that you will use to login to the VM. Be sure to deselect the Use a saved secret and Save as default password checkboxes.</p> </li> <li> <p>You may click the Change Size Link to change your VM size if you wish to do so.</p> </li> <li> <p>Leave the rest as defaults and click on the Create button.    </p> <p> </p> </li> </ol>"},{"location":"DevTestLabs/#find-your-virtual-machine","title":"Find Your Virtual Machine","text":"<ol> <li>From the DevTest Lab Overview page, scroll down until you see your VM under My virtual machines. Click on your VM to access its Overview page.  </li> </ol>"},{"location":"DevTestLabs/#start-your-virtual-machine","title":"Start Your Virtual Machine","text":"<ol> <li> <p>From the Overview page for your VM, click on the Start button.  </p> <p> </p> </li> <li> <p>It takes a few minutes for your VM to start up. Monitor its startup progress by selecting the Notifications icon at the top right of the window.   </p> <p> </p> </li> </ol>"},{"location":"DevTestLabs/#connect-to-your-virtual-machine","title":"Connect To Your Virtual Machine","text":"<ol> <li> <p>From the Overview page for your VM, click on the Browser connect button (if you do not see a Browser connect button you might have to click on the Connect button and then choose Bastion from the dropdown menu).</p> <p> </p> </li> <li> <p>Ensure the Open in new window checkbox is selected, enter the Username and Password that you used when you created your VM, and click on the Connect button. Your VM should open in a new browser tab.</p> <p>Note : By default, the Ubuntu virtual machine opens in Terminal mode. You can access the GUI of your Ubuntu machine from a Windows machine using X2Go.</p> <p>Note : After attempting to login for the first time, an error may appear that a popup blocker is preventing a new window to open. To disable it, an icon will pop up on the browser's search bar, select the button and click always allow. </p> <p></p> </li> </ol>"},{"location":"DevTestLabs/#stop-your-virtual-machine","title":"Stop Your Virtual Machine","text":"<p>Virtual machines only incur costs while they are running. You should shut down your virtual machine when not in use to prevent unneccessary charges.</p> <ol> <li> <p>From the Overview page for your VM, click on the Stop button.  </p> <p> </p> </li> </ol>"},{"location":"ExtPortal/","title":"Data Analytics Portal","text":""},{"location":"ExtPortal/#data-analytics-services-das-portal","title":"Data Analytics Services (DAS) Portal","text":""},{"location":"ExtPortal/#how-to-login","title":"How to Login","text":"<ol> <li> <p>As an external user, you will need to request an external account: firstname.lastname@ext.statcan.ca to be able to access your Virtual Machine and the resources needed. Please Contact Us to request an account.</p> </li> <li> <p>Using a modern web browser, access the link below</p> <ul> <li>https://www.statcan.gc.ca/data-analytics-services</li> </ul> </li> <li> <p>Click the sign-in button.   </p> <p></p> </li> </ol> <p>Note: When you login for the first time, you will be prompted to change your password.</p> <ol> <li> <p>Once signed in, you will be in the hub page. You can go to that page by selecting the arrow next to the user name and select Das Hub as shown here.   </p> <p></p> </li> </ol>"},{"location":"ExtPortal/#how-to-access-virtual-machines","title":"How to access Virtual Machines","text":"<p>Please use the Virtual Machines for instructions on how to login to your virtual machine.</p>"},{"location":"ExtPortal/#how-to-access-services","title":"How to access Services","text":"<p>To Access Services, you must first ensure you are:</p> <ol> <li>Login into the Data Analytics Services (DAS) Portal.</li> <li>Login into your Virtual Machines</li> </ol> <p>Note: All the services listed below will only work from your Virtual Machine.</p>"},{"location":"ExtPortal/#azure-synapse","title":"Azure Synapse","text":"<ol> <li>From your virtual machine you can login to the Data Analytics Service (DAS) portal to access the Azure Synpase URL.</li> </ol> <ol> <li>From your virtual machine you can login to the following url: https://web.azuresynapse.net/, sign in with your cloud account credentials, and select the Synapse workspace that was created for you.  </li> </ol>"},{"location":"ExtPortal/#azure-blob-storage","title":"Azure Blob Storage","text":"<p>Please see the Storage for more information on how to access storage.</p>"},{"location":"ExtPortal/#azure-data-lakes","title":"Azure Data Lakes","text":"<p>Please see the Storage for more information on how to access storage.</p>"},{"location":"ExtPortal/#azure-sql-database","title":"Azure SQL Database","text":"<p>Please see the SQL Server for more information on how to SQL Server</p>"},{"location":"ExtPortal/#azure-data-factory","title":"Azure Data Factory","text":"<p>Data Factory is available as part of Azure Synapse for External Users</p>"},{"location":"ExtPortal/#azure-databricks","title":"Azure Databricks","text":"<ol> <li>From your virtual machine you can login to the Data Analytics Service (DAS) portal to access the Azure Databricks URL.</li> </ol> <ol> <li> <p>From your virtual machine you can login to the following url: https://canadacentral.azuredatabricks.net/, sign in with your cloud account credentials, and select the Databricks workspace that was created for you.  </p> <p></p> </li> </ol>"},{"location":"ExtPortal/#microsoft-power-bi","title":"Microsoft Power BI","text":"<p>Documentation Coming Soon!</p>"},{"location":"ExtPortal/#azure-machine-learning","title":"Azure Machine Learning","text":"<ol> <li>From your virtual machine you can login to the Data Analytics Service (DAS) portal to access the Azure Machine Learning URL.</li> </ol> <ol> <li> <p>From your virtual machine you can login to the following url: https://ml.azure.com/, sign in with your cloud account credentials, and select the Machine Learning workspace that was created for you.  </p> <p></p> </li> </ol>"},{"location":"ExtPortal/#azure-virtual-machines","title":"Azure Virtual Machines","text":"<p>Use the Portal to access your Virtual Machines</p>"},{"location":"ExtPortal/#frequently-asked-questions-faq","title":"Frequently Asked Questions (FAQ)","text":"<ol> <li> <p>How do I request an account or request access?     Please Contact Us to request access and indicate that you would like to request access.</p> </li> <li> <p>How do I reset my password?     Please use the DAS portal Help button to Contact Us and submit a request for assistance and indicate that you would like to reset your password.</p> </li> </ol>"},{"location":"ExtVirtualMachines/","title":"Virtual Machines","text":""},{"location":"ExtVirtualMachines/#virtual-machine","title":"Virtual Machine","text":"<p>Before logging into your virutal machine, you must first login to the Data Analyitics Services portal </p> <ol> <li> <p>You will see at the bottom of the hub page the Virtual Machine that you can access. From the portal, you will be able to perform three different actions. You can either:</p> </li> <li> <p>Start the Virtual Machine</p> </li> <li>Stop the Virtual Machine</li> <li>Connect to the Virtual Machine</li> </ol> <p>Each action will modify the Virtual Machine status.      </p> <ol> <li> <p>Start the virtual machine: When the virtual machine status is stopped, the start button is highlighted. You can then start the virtual machine and its status will switch to Starting. The Start button is not highlighted anymore. After that, it can take a few minute for the virtual machine to be in the Running mode.  The browser connect button is then highlighted.      </p> </li> <li> <p>Connect to the virtual machine: Once the virtual machine is in its Running status, the browser connect button is highlighted. To connect to the virtual machine, you will need to hit the Browser connect button;       </p> </li> <li> <p>This will open a new page where you will be prompted to sign in using your credential provided to you by Statistics Canada. Once launched, click the Login button.</p> <p></p> </li> </ol>"},{"location":"ExtVirtualMachines/#best-practices","title":"Best Practices","text":"<ol> <li>Virtual machines only incur costs while they are running.\\    You should shut down your virtual machine when not in use to prevent unneccessary charges.</li> <li>All virtual machines are turned off every evening at 7 PM EST.</li> <li>Do not share your password with others.</li> </ol>"},{"location":"ExtVirtualMachines/#frequently-asked-questions-faq","title":"Frequently Asked Questions (FAQ)","text":"<ol> <li>I can not see any virtual machines?\\     Please use the DAS portal Help button to Contact Us and submit a request for a virtual machine.</li> <li>I have forgotten my ext.statcan.ca password?\\     Please use the DAS portal Help button to Contact Us and submit a request for assitance.</li> <li>I have forgotten my virtual machines username and password?\\     Please use the DAS portal Help button to Contact Us and submit a request for assitance.</li> </ol>"},{"location":"External/","title":"External","text":""},{"location":"External/#external-users-firstnamelastnameextstatcanca","title":"External Users (firstname.lastname@ext.statcan.ca)","text":"<p>As an external user, you will need to request an external account: firstname.lastname@ext.statcan.ca to be able to access your VM and the resources needed. Please Contact Us to request an account.</p>"},{"location":"FAQ/","title":"FAQ","text":""},{"location":"FAQ/#data-ingestion","title":"Data Ingestion","text":"<p>x</p>"},{"location":"FAQ/#how-do-i-ingest-data-including-large-files-into-the-platform","title":"How do I ingest data (including large files) into the platform?","text":""},{"location":"FAQ/#external-storage-account","title":"External Storage Account","text":"<p>Files may be uploaded to the inbox or to-vers-int container of an external storage account, as documented in Azure Storage Explorer. These files will then automatically be moved into an internal storage account (Data Lake), and made accessible from authorized services.</p> <p></p> <p>Note: The external storage accounts have the naming convention statsproject-acronymexternal.</p>"},{"location":"FAQ/#electronic-file-transfer-service-eft","title":"Electronic File Transfer Service (EFT)","text":"<p>Statistics Canada employees can use EFT to transfer files to/from on-premises (Net A or B) to/from the Azure cloud environment. Please  Contact Us for informaton about this process.</p>"},{"location":"FAQ/#platform-tools","title":"Platform Tools","text":"<p>Platform tools such as Databricks or Data Factory may be used to ingest data from public data sources.</p>"},{"location":"FAQ/#storage-explorer","title":"Storage Explorer","text":""},{"location":"FAQ/#how-do-i-configure-azure-storage-explorer-proxy-settings-on-a-network-b-vdi","title":"How do I configure Azure Storage Explorer proxy settings on a network B VDI?","text":"<p>For Statistics Canada Employees only</p> <ol> <li> <p>Proxy configuration is required if you receive the following error:  </p> <p> </p> </li> <li> <p>In Azure Storage Explorer, go to Edit --&gt; Proxy Settings. Enter the necessary proxy settings, and click on OK.  </p> <p> </p> </li> </ol>"},{"location":"FAQ/#how-do-i-request-a-new-sas-token-required-for-azure-storage-explorer-on-a-network-b-vdi","title":"How do I request a new SAS token (required for Azure Storage Explorer on a Network B VDI)?","text":"<p>For Statistics Canada Employees only</p> <p>Please Contact Us to request a temporary SAS token.</p>"},{"location":"FAQ/#why-do-i-get-an-error-message-when-accessing-the-internal-data-lake","title":"Why do I get an error message when accessing the internal Data Lake?","text":"<p>The internal Data Lake is only accessible from within a VM inside the Data Analytics Service (DAS) portal. It is not accessible from your personal or work laptop.</p>"},{"location":"FAQ/#source-code-control","title":"Source Code Control","text":""},{"location":"FAQ/#how-do-i-link-my-visual-studio-subscription-to-my-statcan-cloud-account","title":"How do I link my Visual Studio Subscription to my StatCan cloud account?","text":"<ol> <li> <p>Login to https://visualstudio.microsoft.com/subscriptions/ with your organization's email address. For StatCan employees, this will be your canada.ca email address.  </p> <p> </p> </li> <li> <p>Add your cloud account as an alternate account.  This will allow you to use your licenses for Visual Studio &amp; Azure DevOps in the CAE.  </p> <p></p> </li> </ol> <p>For Statistics Canada employees:  If you do not have a Visual Studio Subscription, please consult your supervisor. If they decide that a subscription is needed, they can then contact StatCan Software Asset Management by submitting an SRM to request a license on your behalf.</p>"},{"location":"FAQ/#virtual-machines","title":"Virtual Machines","text":""},{"location":"FAQ/#what-do-i-do-if-i-have-forgotten-the-password-for-my-virtual-machine","title":"What do I do if I have forgotten the password for my virtual machine?","text":"<p>If you forget the password of your virtual machine, please Contact Us to have it reset. You can also delete and recreate your virtual machine. Unfortunately, recreating your virtual machine means you will lose any data and software on your old machine.</p>"},{"location":"FAQ/#what-do-i-do-if-i-need-to-run-a-long-running-job-on-my-virtual-machine","title":"What do I do if I need to run a long running job on my virtual machine?","text":"<p>Machines are shut down every day at 7pm EST in order to reduce costs. If you have a long-running job, it is recommended that you use Databricks or Data Factory.</p> <p>WARNING: Disabling Auto-shutdown is not recommended as it can incur significant costs.  </p> <p>To disable Auto-shutdown:  </p> <ol> <li> <p>Navigate to your virtual machine in the Azure Portal.</p> </li> <li> <p>Disable Auto-shutdown.  </p> <p> </p> </li> </ol>"},{"location":"FAQ/#how-do-i-request-changes-to-my-virtual-machine","title":"How do I request changes to my virtual machine?","text":"<p>If the virtual machine you are currently using does not meet your requirements, please Contact Us.</p>"},{"location":"FAQ/#databricks","title":"Databricks","text":""},{"location":"FAQ/#why-am-i-unable-to-run-code-from-my-databricks-notebook","title":"Why am I unable to run code from my Databricks notebook?","text":"<p>You must first start a Databricks cluster that was previously created for you:</p> <ol> <li> <p>Click on Clusters.  </p> </li> <li> <p>Navigate to your cluster and click on the Start (arrow) button.  </p> <p></p> </li> </ol>"},{"location":"FAQ/#what-type-of-clusters-are-available-in-databricks","title":"What type of clusters are available in databricks?","text":"<p>See the following link for the different types of available clusters:  Databricks runtime releases</p> <p>Generally, LTS (long term support) clusters are supported and recommended by Databricks. If you need a different version of the databricks cluster, please contact the CAE support team.</p>"},{"location":"FAQ/#what-happens-when-clusters-are-upgraded","title":"What happens when clusters are upgraded?","text":"<p>LTS (long term support) have support for 1-2 years. They will need to be periodically updated to a newer version. When upgraded, all code should be re-run to ensure there are no issues when a cluster is updated</p>"},{"location":"FAQ/#how-do-you-read-an-excel-file-with-databrickspython","title":"How do you read an excel file with databricks/python?","text":"<p>Here is an example of how to read in an excel file:</p> <pre><code>%python\nimport pandas as pd\npd.read_excel(\"/dbfs/mnt/ccei-ccie-ext/Daily charts.xlsx\", engine='openyxl')\n</code></pre>"},{"location":"FAQ/#dashboard","title":"Dashboard","text":""},{"location":"FAQ/#how-do-i-change-my-subscription-so-that-i-can-see-my-resources","title":"How do I change my subscription so that I can see my resources?","text":"<ol> <li> <p>In the Azure Portal click the Directory + subscription icon.</p> <p></p> </li> <li> <p>Select the vdl subscription.</p> </li> </ol>"},{"location":"FAQ/#caes-sql-server-deployment-options","title":"CAE's SQL Server Deployment Options","text":"Azure SQL Database Azure SQL Database - Managed Instance SQL Server on Azure Virtual Machines Yes No No"},{"location":"FAQ/#sql-server-machine-learning-services","title":"SQL Server Machine Learning Services","text":"<p>Machine Learning Services is a feature in SQL Server that allows to run Python and R scripts and perform predictive analytics and machine learning with relational data. The scripts are executed in-database without the need to move data outside SQL Server and can use open-source packages and frameworks as well as Microsoft's enterprise packages. This feature is offered only offered in 2 of the 3 Azure Cloud SQL Server deployment options, SQL Server on Azure Virtual Machines and Azure SQL Database - Managed Instance.</p> <p>Presently, CAE has only Azure SQL Database that doesn't offer this feature.</p> <p>However, CAE users can experiment with SQL ML services on their datascience VM Azure SQL server (developer edition). </p>"},{"location":"FAQ/#other","title":"Other","text":""},{"location":"FAQ/#how-do-i-connect-to-the-internal-data-lake-with-power-bi-desktop","title":"How do I connect to the internal data lake with Power BI Desktop?","text":"<p>Prerequisites: - A VM in the Collaborative Analytics Environment (CAE). - Power BI Desktop (available by default in the Windows Data Science Virtual Machine images.)</p> <p>Steps: 1. Login to your CAE VM. 2. Launch Power BI Desktop. 3. Follow the steps in Analyze data in Azure Data Lake Storage Gen2 by using Power BI - Create a report in Power BI Desktop. Please Contact Us if you don't know the Azure Data Lake Storage Gen2 URL.</p>"},{"location":"FAQ/#how-can-statistics-canada-employees-transfer-files-from-their-data-centre","title":"How can Statistics Canada employees transfer files from their data centre?","text":"<p>For Statistics Canada Employees, they can refer to this internal documentation: Data Ingestion</p>"},{"location":"FAQ/#how-do-i-add-a-faq","title":"How do I add a FAQ?","text":"<p>Please Contact Us to send your suggestion.</p>"},{"location":"Geo/","title":"Geospatial","text":""},{"location":"Geo/#geospatial-analytical-environment-gae-cross-platform-access","title":"Geospatial Analytical Environment (GAE) - Cross Platform Access","text":""},{"location":"Geo/#getting-started","title":"Getting Started","text":"<p>IMPORTANT: Prerequisites</p> <pre><code>1. An onboarded project with access to DAS GAE ArcGIS Portal    2. An ArcGIS Portal Client Id (API Key)\n</code></pre> <p>The ArcGIS Enterprise Portal can be accessed in either the AAW or CAE using the API, from any service which leverages the Python programming language. </p> <p>For example, in AAW and the use of Jupyter Notebooks within the space, or in CAE the use of Databricks, DataFactory, etc.</p> <p>The DAS GAE ArcGIS Enterprise Portal can be accessed directly here</p> <p>For help with self-registering as a DAS Geospatial Portal user</p>"},{"location":"Geo/#using-the-arcgis-api-for-python","title":"Using the ArcGIS API for Python","text":""},{"location":"Geo/#connecting-to-arcgis-enterprise-portal-using-arcgis-api","title":"Connecting to ArcGIS Enterprise Portal using ArcGIS API","text":"<ol> <li> <p>Install packages:</p> <p><code>python conda install -c esri arcgis</code></p> <p>or using Artifactory</p> <p><code>python3333 conda install -c https://jfrog.aaw.cloud.statcan.ca/artifactory/api/conda/esri-remote arcgis</code></p> </li> <li> <p>Import the necessary libraries that you will need in the Notebook.     <code>python     from arcgis.gis import GIS     from arcgis.gis import Item</code></p> </li> <li> <p>Access the Portal     Your project group will be provided with a Client ID upon onboarding. Paste the Client ID in between the quoatations <code>client_id='######'</code>. </p> <p><code>python gis = GIS(\"https://geoanalytics.cloud.statcan.ca/portal\", client_id=' ') print(\"Successfully logged in as: \" + gis.properties.user.username)</code></p> </li> <li> <ul> <li>The output will redirect you to a login Portal.</li> <li>Use the StatCan Azure Login option, and your Cloud ID </li> <li>After successful login, you will receive a code to sign in using SAML. </li> <li>Paste this code into the output. </li> </ul> <p></p> </li> </ol>"},{"location":"Geo/#display-user-information","title":"Display user information","text":"<p>Using the 'me' function, we can display various information about the user logged in.</p> <pre><code>me = gis.users.me\nusername = me.username\ndescription = me.description\ndisplay(me)\n</code></pre>"},{"location":"Geo/#search-for-content","title":"Search for Content","text":"<p>Search for the content you have hosted on the DAaaS Geo Portal. Using the 'me' function we can search for all of the hosted content on the account. There are multiple ways to search for content. Two different methods are outlined below.</p> <p>Search all of your hosted itmes in the DAaaS Geo Portal.</p> <pre><code>my_content = me.items()\nmy_content\n</code></pre> <p>Search for specific content you own in the DAaaS Geo Portal.</p> <p>This is similar to the example above, however if you know the title of they layer you want to use, you can save it as a function.</p> <pre><code>my_items = me.items()\nfor items in my_items:\n    print(items.title, \" | \", items.type)\n    if items.title == \"Flood in Sorel-Tracy\":\n        flood_item = items\n\n    else:\n        continue\nprint(flood_item)\n</code></pre> <p>Search all content you have access to, not just your own.</p> <pre><code>flood_item = gis.content.search(\"tags: flood\", item_type =\"Feature Service\")\nflood_item\n</code></pre>"},{"location":"Geo/#get-content","title":"Get Content","text":"<p>We need to get the item from the DAaaS Geo Portal in order to use it in the Jupyter Notebook. This is done by providing the unique identification number of the item you want to use. Three examples are outlined below, all accessing the identical layer.</p> <pre><code>item1 = gis.content.get(my_content[5].id) #from searching your content above\ndisplay(item1)\n\nitem2 = gis.content.get(flood_item.id) #from example above -searching for specific content\ndisplay(item2)\n\nitem3 = gis.content.get('edebfe03764b497f90cda5f0bfe727e2') #the actual content id number\ndisplay(item3)\n</code></pre>"},{"location":"Geo/#perform-analysis","title":"Perform Analysis","text":"<p>Once the layers are brought into the Jupyter notebook, we are able to perform similar types of analysis you would expect to find in a GIS software such as ArcGIS. There are many modules containing many sub-modules of which can perform multiple types of analyses. </p> <p>Using the arcgis.features module, import the use_proximity submodule <code>from arcgis.features import use_proximity</code>. This submodule allows us to '.create_buffers' - areas of equal distance from features. Here, we specify the layer we want to use, distance, units, and output name (you may also specify other characteristics such as field, ring type, end type, and others). By specifying an output name, after running the buffer command, a new layer will be automatically uploaded into the DAaaS GEO Portal containing the new feature you just created. </p> <pre><code>buffer_lyr = use_proximity.create_buffers(item1, distances=[1], \n                                          units = \"Kilometers\", \n                                          output_name='item1_buffer')\n\ndisplay(item1_buffer)\n</code></pre> <p>Some users prefer to work with Open-Source packages.  Translating from ArcGIS to Spatial Dataframes is simple.</p> <pre><code># create a Spatially Enabled DataFrame object\nsdf = pd.DataFrame.spatial.from_layer(feature_layer)\n</code></pre>"},{"location":"Geo/#update-items","title":"Update Items","text":"<p>By getting the item as we did similar to the example above, we can use the '.update' function to update exisiting item within the DAaaS GEO Portal. We can update item properties, data, thumbnails, and metadata.</p> <pre><code>item1_buffer = gis.content.get('c60c7e57bdb846dnbd7c8226c80414d2')\nitem1_buffer.update(item_properties={'title': 'Enter Title'\n                                     'tags': 'tag1, tag2, tag3, tag4',\n                                     'description': 'Enter description of item'}\n</code></pre>"},{"location":"Geo/#visualize-your-data-on-an-interactive-map","title":"Visualize Your Data on an Interactive Map","text":"<p>Example: MatplotLib Library In the code below, we create an ax object, which is a map style plot. We then plot our data ('Population Change') change column on the axes</p> <pre><code>import matplotlib.pyplot as plt\nax = sdf.boundary.plot(figsize=(10, 5))\nshape.plot(ax=ax, column='Population Change', legend=True)\nplt.show()\n</code></pre> <p>Example: ipyleaflet Library In this example we will use the library 'ipyleaflet' to create an interactive map. This map will be centered around Toronto, ON. The data being used will be outlined below. Begin by pasting <code>conda install -c conda-forge ipyleaflet</code> allowing you to install ipyleaflet libraries in the Python environment.  Import the necessary libraries.</p> <pre><code>import ipyleaflet \nfrom ipyleaflet import *\n</code></pre> <p>Now that we have imported the ipyleaflet module, we can create a simple map by specifying the latitude and longitude of the location we want, zoom level, and basemap (more basemaps). Extra controls have been added such as layers and scale.</p> <pre><code>toronto_map = Map(center=[43.69, -79.35], zoom=11, basemap=basemaps.Esri.WorldStreetMap)\n\ntoronto_map.add_control(LayersControl(position='topright'))\ntoronto_map.add_control(ScaleControl(position='bottomleft'))\ntoronto_map\n</code></pre> <p></p>"},{"location":"Geo/#learn-more-about-the-arcgis-api-for-python","title":"Learn More about the ArcGIS API for Python","text":"<p>Full documentation for the ArGIS API can be located here</p>"},{"location":"Geo/#learn-more-about-das-geospatial-analytical-environment-gae-and-services","title":"Learn More about DAS Geospatial Analytical Environment (GAE) and Services","text":"<p>GAE Help Guide</p>"},{"location":"GitHubConfiguration/","title":"Configuration","text":"<p>GitHub.com is an online platform that is used for collaboration as well as tracking changes and versioning for a variety of project types.</p> <p>IMPORTANT: Do not store protected B data on GitHub.com</p>"},{"location":"GitHubConfiguration/#creating-a-github-account","title":"Creating a GitHub Account","text":"<p>For information on creating a GitHub account (or using your existing account), click here.</p>"},{"location":"GitHubConfiguration/#azure-data-factory","title":"Azure Data Factory","text":"<ol> <li> <p>On the Manage tab, click on Git configuration.</p> <p></p> </li> <li> <p>Click Configure. Under Repository type, select GitHub, then enter your GitHub account username. Click Continue.</p> <p></p> </li> <li> <p>A pop-up will appear. Click AuthorizeAzureDataFactory, then enter your GitHub account password.</p> </li> <li> <p>Configure a repository. You can either select a repository that you own, or enter a repository link. Specify additional settings, then click Apply.</p> <p></p> </li> <li> <p>Set your working branch. You can either create a new branch or use an existing one. Then click Save.</p> </li> </ol> <p>To remove GitHub Integration: On the Git configuration screen, click Disconnect. Enter the name of the Data Factory, then click Disconnect again to confirm.</p> <p></p>"},{"location":"GitHubConfiguration/#azure-databricks","title":"Azure Databricks","text":""},{"location":"GitHubConfiguration/#set-up-git-integration","title":"Set up Git Integration","text":"<ol> <li> <p>Go to User Settings, then click on the Git Integration tab.</p> <p></p> </li> <li> <p>Under Git provider, select GitHub. Enter your GitHub username.</p> </li> <li> <p>From your GitHub account, follow the instructions to create a personal access token, ensuring that the repo permission is checked. Note that you will have to repeat this process when your token has expired, unless you set the Expiration to never expire.</p> <p></p> </li> <li> <p>Copy the token, and paste it into Databricks. Click Save.</p> </li> </ol>"},{"location":"GitHubConfiguration/#add-a-git-repository","title":"Add a Git Repository","text":"<ol> <li> <p>On the Repos tab, click Add Repo.</p> <p></p> </li> <li> <p>With Clone remote Git repo selected, enter your GitHub repository url. The Git provider and Repo name should fill in automatically. Click Create.</p> </li> </ol>"},{"location":"GitHubConfiguration/#cae-virtual-machines","title":"CAE Virtual Machines","text":""},{"location":"GitHubConfiguration/#vs-code","title":"VS Code","text":"<p>To learn how to use GitHub with VS Code, see the GitHub - Getting Started documentation.</p>"},{"location":"GitHubConfiguration/#r-studio","title":"R-Studio","text":"<ol> <li> <p>In the File menu, click New Project..., then select Version Control.</p> <p></p> </li> <li> <p>Select Git. Enter the URL for the GitHub repository that you want to clone, choose a folder on your VM where the local files will be stored, then click Create Project.</p> <p></p> </li> </ol>"},{"location":"GitHubConfiguration/#azure-machine-learning","title":"Azure Machine Learning","text":"<ol> <li> <p>Create a compute instance, then open a terminal.</p> <p></p> </li> <li> <p>In the terminal window, enter the following (replace the example email with your own): <code>ssh-keygen -t rsa -b 4096 -C \"first.last@canada.ca\"</code></p> </li> <li> <p>Press ENTER until your key is generated.</p> <p></p> </li> <li> <p>Enter in the terminal: <code>cat ~/.ssh/id_rsa.pub</code>. Select the output and copy it to the clipboard.</p> <p></p> </li> <li> <p>Go to your GitHub account settings (on GitHub.com), click on SSH and GPG keys, then New SSH key. Paste in the key you just copied, then click Add SSH key.</p> <p></p> </li> <li> <p>In the terminal window, type: <code>git clone [url]</code> (replace [url] with the SSH url for your GitHub repository, e.g. <code>git@github.com:username/reponame.git</code>).</p> </li> <li> <p>When prompted, type <code>yes</code>.</p> </li> </ol>"},{"location":"GitHubConfiguration/#microsoft-documentation","title":"Microsoft Documentation","text":"<ul> <li>Git Integration for Azure Machine Learning</li> </ul>"},{"location":"GitHubConfiguration/#azure-synapse","title":"Azure Synapse","text":"<ol> <li> <p>On the Manage tab, click on Git configuration.</p> <p></p> </li> <li> <p>Click Configure. Under Repository type, select GitHub, then enter your GitHub account username. Click Continue.</p> </li> <li> <p>A pop-up will appear. Enter your GitHub account login info, then click AuthorizeAzureSynapse.</p> </li> <li> <p>Configure a repository. You can either select a repository that you own, or enter a repository link. Specify additional settings, then click Apply.</p> </li> <li> <p>Set your working branch. You can either create a new branch or use an existing one. Then click Save.</p> </li> </ol> <p>To remove GitHub Integration: On the Git configuration screen, click Disconnect. Enter the workspace name, then click Disconnect again to confirm.</p>"},{"location":"GitHubGettingStarted/","title":"Getting Started","text":"<p>GitHub.com is an online platform that is used for collaboration as well as tracking changes and versioning for a variety of project types.</p> <p>This document shows how to start using git with various Azure services that already have git integration set up. See GitHub - Configuration for instructions on how to set this up.</p> <p>IMPORTANT: Do not store protected B data on GitHub.</p>"},{"location":"GitHubGettingStarted/#creating-a-github-account","title":"Creating a GitHub Account","text":"<p>For information on creating a GitHub account (or using your existing account), click here.</p>"},{"location":"GitHubGettingStarted/#azure-data-factory","title":"Azure Data Factory","text":"<p>If git integration is set up for your Data Factory, whenever you save or publish changes, these changes will automatically sync with the GitHub repository.</p> <p>To change which branch you're working one (the default collaboration branch is main), click on the down arrow next to the branch name in the top left of the screen. From there, you can select a different branch or create a new one.</p> <p></p>"},{"location":"GitHubGettingStarted/#databricks","title":"Databricks","text":""},{"location":"GitHubGettingStarted/#configuring-a-personal-access-token","title":"Configuring a Personal Access Token","text":"<p>Before you can work with GitHub repositories in Databricks, you first need to configure a personal access token (this gives Databricks access to your GitHub account).</p> <ol> <li> <p>In Databricks, go to User Settings, then click on the Git Integration tab.</p> <p></p> </li> <li> <p>Under Git provider, select GitHub. Enter your GitHub username.</p> </li> <li> <p>From your GitHub account, follow the instructions to create a personal access token, ensuring that the repo permission is checked. If you set an expiration date, you will need to repeat this process to create a new token after this date.</p> <p></p> </li> <li> <p>Copy the token, and paste it into Databricks. Click Save.</p> </li> </ol>"},{"location":"GitHubGettingStarted/#creatingchanging-branches","title":"Creating/Changing Branches","text":"<p>It is a best practice to do all of your work on your own branch (not main), then merge your changes with the main branch once you are ready to publish.</p> <ol> <li> <p>To create a new branch or change to an existing branch, from the Repos tab, click the folder containing the GitHub repository to open it. Click on the down arrow next to the branch name, then click Git....</p> <p></p> </li> <li> <p>Click the down arrow to find an existing branch, or the plus to create a new one. Your branch should include your name.</p> <p></p> </li> <li> <p>Once your branch is created, find it in the drop-down menu and click on it to switch to it. Click close. All of your work will now be saved to this branch unless you change it again later.</p> </li> </ol>"},{"location":"GitHubGettingStarted/#creating-moving-and-cloning-notebooks","title":"Creating, Moving, and Cloning Notebooks","text":"<ol> <li> <p>To create a new notebook within the repository, from the drop-down menu next to your branch name, hover over Create, then click Notebook. You can also create folders this way.</p> <p></p> </li> <li> <p>To move or clone an existing notebook from your workspace, navigate to the notebook (in the Workspace tab), click on the down arrown next to the notebook name, then click Clone to create a copy of the notebook in the repo, or Move to move the notebook from the workspace to the repo. Find the repo in the pop-up menu, and navigate to the folder where you want the notebook to be cloned or moved to. Click Clone/Select.</p> </li> </ol>"},{"location":"GitHubGettingStarted/#committing-and-pushing-changes","title":"Committing and Pushing Changes","text":"<ol> <li> <p>From the Repos tab, click the folder containing the GitHub repository to open it. Click on the down arrow next to the branch name, then click Git....</p> </li> <li> <p>Make sure all the changes that you want to commit are checked, type a short summary describing what was changed, then click Commit &amp; Push.</p> <p></p> </li> </ol>"},{"location":"GitHubGettingStarted/#azure-synapse","title":"Azure Synapse","text":"<p>See Azure Data Factory above, and follow the same instructions in Synapse.</p>"},{"location":"GitHubGettingStarted/#vs-code","title":"VS Code","text":""},{"location":"GitHubGettingStarted/#how-to-clone-a-repository","title":"How to Clone a Repository","text":"<ol> <li> <p>Click on the Source Control tab. Then you can either open a folder containing a git repository (if you already have one on your cloud VM), or clone a repository from a URL.</p> <p></p> </li> <li> <p>To clone a repository, click Clone Repository. Copy the repository URL from GitHub (e.g. <code>https://github.com/username/reponame</code>), paste it in the textbox, and click Clone from URL.</p> <p></p> </li> <li> <p>Choose a folder on your cloud VM where the git repository will be stored locally. You may be promted to sign in to your GitHub account.</p> </li> <li> <p>Once the repository is cloned to your machine, you can open the local folder in VS Code.</p> </li> </ol>"},{"location":"GitHubGettingStarted/#how-to-commit-changes","title":"How to Commit Changes","text":"<ol> <li> <p>Before you can commit changes, you need to configure your user name and email. Open a terminal window (by clicking Terminal &gt; New Terminal in the menu bar). In the terminal, type the following:     <code>git config user.name \"First Last\"     git config user.email \"first.last@canada.ca\"</code> </p> <p></p> </li> <li> <p>When you are ready to publish your changes to GitHub, on the Source Control tab, type in a commit message, then click the checkmark button.</p> <p></p> </li> <li> <p>Click the source control menu button, then click Push. If you get an error message (this will happen if your local copy of the repository is not up to date with the version stored on GitHub), first click Pull, then Push to merge your changes.</p> <p></p> </li> </ol>"},{"location":"GitHubGettingStarted/#r-studio","title":"R Studio","text":"<p>Note: Instructions are the same whether you are using the desktop version of R Studio from a cloud VM or the web version through Databricks.</p>"},{"location":"GitHubGettingStarted/#set-up","title":"Set Up","text":"<ol> <li> <p>In the File menu, click New Project..., then select Version Control.</p> <p></p> </li> <li> <p>Select Git. Enter the URL for the GitHub repository that you want to clone, choose a folder where the local files will be stored, then click Create Project.</p> <p></p> </li> <li> <p>If you are prompted to sign in to your GitHub account, enter your GitHub username and a personal access token as the password.</p> </li> </ol>"},{"location":"GitHubGettingStarted/#how-to-commit-changes_1","title":"How to Commit Changes","text":"<ol> <li> <p>When you are ready to publish your changes to GitHub, on the Git tab, click Commit.</p> <p></p> </li> <li> <p>Click the checkbox for each of changes you want to commit. Enter a commit message briefly describing your changes, then click Commit. A pop-up will appear confirming that your commit was successful. Click Close.</p> <p></p> </li> <li> <p>Click the Push button to upload your changes to GitHub. A pop-up will appear confirming that the push was successful. If you get an error message, (this will happen if your local copy of the repository is not up to date with the version stored on GitHub), first click Pull, then Push to merge your changes.</p> </li> </ol>"},{"location":"GitHubGettingStarted/#faq","title":"FAQ","text":""},{"location":"GitHubGettingStarted/#i-get-an-error-when-trying-to-push-changes-whats-happening","title":"I get an error when trying to push changes. What's happening?","text":"<p>This could mean that your local copy of the repository is not up to date with the copy stored on GitHub. Try clicking pull first, then push your changes. It is a best practice to always run a pull command before starting to work with a repository to ensure you are working on the latest version, and no more than one person should be editing the same file at the same time.</p>"},{"location":"GitHubGettingStarted/#how-do-i-revert-back-to-a-previous-commit","title":"How do I revert back to a previous commit?","text":"<p>This must be done from VS Code (accessible through a cloud VM), regardless of where you primarily use git.</p> <ol> <li>In VS Code, follow the steps above to clone a repository if you haven't done so already.</li> <li>Open a terminal (by clicking Terminal -&gt; New Terminal).</li> <li>Within the terminal window, type <code>git revert HEAD~n --no-edit</code> and press enter (replace n with the number of commits you want to revert, starting from 0). Note: you can find your commit history from GitHub by clicking on x commits near the top right of your repository.</li> <li>If there are merge conflicts, click Accept incoming change, then commit and push as normal (see above for detailed instructions on how to do this in VS Code).</li> </ol>"},{"location":"Language/","title":"Language","text":"<p>This document describes how to change languages in the various service offerings.</p>"},{"location":"Language/#azure-portal","title":"Azure portal","text":"<p>To change the language settings in the Azure portal:</p> <ol> <li> <p>Click the Settings menu in the global page header.</p> </li> <li> <p>Click the Language &amp; region tab.</p> </li> <li> <p>Use the drop-downs to choose your preferred language and regional format settings.</p> </li> <li> <p>Click Apply to update your language and regional format settings.</p> <p> </p> </li> </ol>"},{"location":"Language/#dashboard","title":"Dashboard","text":"<p>To select the English dashboard of the Collaborative Analytics Environment (CAE):</p> <ol> <li> <p>From the dashboard view, click the arrow next to the dashboard name.</p> </li> <li> <p>Select the Collaborative Analytics Environment dashboard from the displayed list of dashboards. Note: If the dashboard is not listed, select Browse all dashboards to access the complete list.</p> <p> </p> </li> </ol>"},{"location":"Language/#virtual-machines","title":"Virtual Machines","text":""},{"location":"Language/#windows-server","title":"Windows Server","text":"<p>To configure the display language for a Windows virtual machine:</p> <ol> <li> <p>Go to Settings. </p> <p> </p> </li> <li> <p>Select Time &amp; Language.</p> <p> </p> </li> <li> <p>Select Language. Under Preferred languages, select Add a language. </p> <p> </p> </li> <li> <p>In the Choose a language to install dialog box, select your preferred language pack and then click Next.</p> <p> </p> </li> <li> <p>In the Install language features dialog box, click Install. </p> <p> </p> </li> <li> <p>The Windows display language box should now include the newly added language. To switch to the new language, select it from the Windows display language box, sign out of the current Windows session, and then sign back in.</p> </li> </ol>"},{"location":"Language/#ubuntu-server","title":"Ubuntu Server","text":"<p>If you are using X2GO to access the GUI of your Ubuntu machine, you might need to manually install additional language packages because the default session is available only in English.</p>"},{"location":"Language/#azure-machine-learning","title":"Azure Machine Learning","text":"<p>To change the language settings in the Microsoft Azure Machine Learning workspace:</p> <ol> <li> <p>Click the Settings menu in the global page header.</p> </li> <li> <p>Under Language and formats, use the drop-downs to choose your preferred language and the regional format settings.</p> </li> <li> <p>Click Apply to update your language and regional format settings.</p> <p> </p> </li> </ol>"},{"location":"Language/#azure-machine-learning-jupyter-lab","title":"Azure Machine Learning - Jupyter Lab","text":"<ol> <li> <p>Run in Azure ML compute instance terminal:      <code>sh     pip install jupyterlab==3</code></p> </li> <li> <p>Restart compute instance</p> </li> <li> <p>Run in Azure ML compute instance terminal:      <code>sh     pip install git+https://github.com/StatCan/jupyterlab-language-pack-fr_FR</code></p> </li> <li> <p>In JupyterLab, switch Settings - Language - French</p> </li> </ol>"},{"location":"Language/#slack","title":"Slack","text":"<p>To change the language settings in the Slack application:</p> <ol> <li> <p>Click the profile icon in the global page header.</p> </li> <li> <p>Click Preferences.</p> <p> </p> </li> <li> <p>Select the Language &amp; region tab.</p> </li> <li> <p>Under Language, use the drop-down to choose your preferred language.</p> <p> </p> </li> <li> <p>Close the Preferences window.</p> </li> </ol>"},{"location":"Language/#azure-storage-explorer","title":"Azure Storage Explorer","text":"<p>By default, the application detects your language based on the language preferences on your computer. </p> <p>To change the language settings on Microsoft Azure Storage Explorer:</p> <ol> <li> <p>Click Edit.</p> </li> <li> <p>Click Settings. </p> <p> </p> </li> <li> <p>In the Settings page, select Application. Under Regional Settings, use the drop-down to choose your preferred language.  </p> <p> </p> </li> <li> <p>To switch to the new language, close and restart the application.</p> </li> </ol>"},{"location":"Language/#power-bi","title":"Power BI","text":"<p>More information is also available in Supported languages and countries/regions for Power BI.</p>"},{"location":"Language/#power-bi-service","title":"Power BI Service","text":"<p>By default, the Power BI service detects your language based on the language preferences on your computer. The steps to access and change these preferences may vary depending on your operating system and browser.</p> <p>To switch the menu language in the Power BI service:</p> <ol> <li> <p>In the Power BI service, click the Settings icon and select Settings.</p> <p> </p> </li> <li> <p>In the General tab, select Language.</p> <p> </p> </li> <li> <p>Select your language and click Apply.</p> </li> </ol> <p>See Languages for the Power BI service for more details.</p>"},{"location":"Language/#power-bi-desktop","title":"Power BI Desktop","text":"<p>By default: - the Application language is based on the Windows language - the Model language is based on the Application language - the Query steps are based on the Application language.</p> <p>It is recommended to set the Model language to English (United States).</p> <p>The model language applies only when the report is first created and cannot be changed on existing reports. Thus, setting the language model to U.S. English is recommended, unless you have a specific need to use another language for the report model. String comparisons and internal date fields are affected by this setting.</p> <p>To switch the menu language and model language in Power BI Desktop:</p> <ol> <li> <p>Open the Options menu.</p> <p> </p> </li> <li> <p>Under GLOBAL, click Regional Settings and set the Application language and Model language to the desired language.</p> <p> </p> </li> </ol> <p>NOTE: The Import language is set separately in the CURRENT FILE section Regional Settings. You need to change this only if you import data files that have numbers and dates in a specific locale (e.g., Canada English DD/MM/YYYY, United State English MM/DD/YYYY).</p>"},{"location":"Language/#azure-databricks","title":"Azure Databricks","text":"<p>To change the language settings in Databricks:</p> <ol> <li> <p>Select the user dropdown at the top right and select User Settings</p> <p></p> </li> <li> <p>On the page, select Language Settings</p> <p></p> </li> <li> <p>Click on the dropdown and select the language of your choice.</p> </li> </ol>"},{"location":"Language/#azure-data-factory","title":"Azure Data Factory","text":"<p>To select the language:</p> <ol> <li> <p>In Azure Data Factory, go to Settings.</p> </li> <li> <p>Select English.</p> </li> <li> <p>Click on Apply.</p> <p> </p> </li> </ol>"},{"location":"Language/#jupyterlab","title":"JupyterLab","text":"<p>To change the language settings in JupyterLab:</p> <ol> <li> <p>Within JupyterLab, open up a console or terminal.</p> </li> <li> <p>Install the language example of your choice using pip.</p> <p>Example: <code>python pip install jupyterlab-language-pack-zh-CN</code> 3. Under the settings tab, highlight over language and select the language you installed.</p> <p></p> </li> <li> <p>Click on OK to refresh the page, you will see the language change.</p> </li> </ol> <p>For More information about changing languages, click the link below:</p> <p>Jupyerlab change language display</p>"},{"location":"Language/#visual-studio-code-vscode","title":"Visual Studio Code (VSCode)","text":"<p>To change the display language in VSCode:</p> <ol> <li> <p>Open VSCode and open the command Pallette (Ctrl+Shift+P).</p> </li> <li> <p>In the command pallette, type in \"display\" and select install additional languages.</p> <p></p> </li> </ol> <p>Note: If you have already installed the language you wanted, you can select the language from the dropdown.</p> <ol> <li> <p>On the left side of VSCode, languages will appear which can be installed, select the language of your choice.</p> </li> <li> <p>A pop-up may appear at the bottom right of the screen in which you can change the language and it will restart VSCode. </p> <p></p> </li> </ol>"},{"location":"Language/#visual-studio","title":"Visual Studio","text":"<p>If you have already installed language packages within Visual Studio:</p> <ol> <li> <p>On the top bar, select Tools then Options.</p> <p></p> </li> <li> <p>From the menu, under the Environment tab, select International Settings.</p> <p></p> </li> <li> <p>From the drop down under Language, select the language of your choice.</p> </li> </ol>"},{"location":"Language/#if-you-have-not-installed-other-language-packages-within-visual-studio","title":"If you have not installed other language packages within Visual Studio","text":"<ol> <li> <p>On your computer, open the Visual Studio Installer.</p> </li> <li> <p>In the installer, select the modify button.</p> <p></p> </li> <li> <p>On the new window, select Language Packs. Select all the languages you want to add and then select modify.</p> <p></p> </li> <li> <p>From here you can follow the steps on using the installed language packages within Visual Studio. </p> </li> </ol>"},{"location":"Language/#rstudio","title":"RStudio","text":"<p>To set RStudio into a different language: </p> <ol> <li> <p>Open up RStudio and open up the console.</p> </li> <li> <p>In the console, type in \"Sys.getenv(LANGUAGE = \"fr\") Note: \"fr\" is for the french language, for a list of other languages that can be used, click here.</p> </li> <li> <p>To test it, you can type \"2+x\" and it should give an error in the language inputted.</p> </li> </ol>"},{"location":"Language/#web-browsers","title":"Web browsers","text":"<ul> <li>Chrome</li> <li>Safari</li> <li>Edge</li> <li>Firefox</li> <li>Opera</li> </ul>"},{"location":"Login/","title":"How to Login","text":""},{"location":"Login/#prerequisites","title":"Prerequisites","text":"<ul> <li>An authorized StatCan account; the cloud account previously used is not required anymore.</li> </ul>"},{"location":"Login/#notes","title":"Notes","text":"<p>We recommend you use  a modern web browser such as Chrome or Edge (not Internet Explorer) to access the Azure Portal or any Azure services. When connecting to the Azure portal, you will login with your StatCan Network account (i.e. firstname.lastname@statcan.gc.ca) for StatCan employees.  </p> <p>Follow the instructions for your type of account to complete your sign-in. </p>"},{"location":"Login/#statcan-network-account-firstnamelastnamestatcangcca","title":"StatCan Network Account (firstname.lastname@statcan.gc.ca)","text":"<p>Applicable to all Azure cloud Services (Power BI, Databricks, Data Factory, Virtual Machine, etc.) for internal users only</p> <ol> <li>Using Chrome, Chromium or Edge, open either:   <ul> <li>The Azure Portal Azure Portal Dashboard</li> <li>The Power BI App URL (if provided) or Power BI Service Login Page </li> </ul> </li> </ol>"},{"location":"Login/#azure-portal-login","title":"Azure Portal Login","text":"<ol> <li> <p>Enter your statcan.gc.ca cloud account credentials</p> <p> 2. You will then be loged into the Azure Portal (show image)</p> <p> 3. See instructions on how to navigate to the Collaborative Analytics Dashboard</p> </li> </ol>"},{"location":"PostgreSQL/","title":"Azure Postgres Database","text":"<p>Fran\u00e7ais</p>"},{"location":"PostgreSQL/#accessing-azure-postgresql-database","title":"Accessing Azure PostgreSQL Database","text":""},{"location":"PostgreSQL/#azure-data-factory","title":"Azure Data Factory","text":"<p>A linked service can be setup inside Azure Data Factory. Username is Cloud account username or AD group name if access was granted to a AD group, followed by the server name. Password is an Azure AD access Token as described below in step 6 of pgAdmin section</p> <p> or </p> <p>Please contact the support team through the https://cae-eac.slack.com channel if you need assistance.</p>"},{"location":"PostgreSQL/#virtual-machine","title":"Virtual Machine","text":"<p>You can access an Azure PostgreSQL database from your cloud virtual machine, using various applications including: 1. pgAdmin  2. Azure Data Studio  3. Visual Studio Code</p>"},{"location":"PostgreSQL/#prerequisites","title":"Prerequisites","text":"<ol> <li>A virtual machine in the Collaborative Analytics Environment (CAE). See the VM page for more information.</li> <li>pgAdmin or Azure Data Studio and Visual Studio Code. Those two are available by default in the Windows Data Science Virtual Machine images.  </li> </ol>"},{"location":"PostgreSQL/#pgadmin","title":"pgAdmin","text":"<p>This is one of the common tool for PostgreSQL database administration.</p> <ol> <li> <p>In your cloud VM install pgAdmin from https://www.pgadmin.org/download/</p> </li> <li> <p>Connect to your cloud VM and launch pgAdmin.</p> </li> </ol> <p></p> <ol> <li>Add the server you need to connect to by right clicking on Servers in the top left corner</li> </ol> <p></p> <ol> <li>In the General tab, enter a name for your server. You can write the real name of the server    In the Connection tab, enter the full Server name and add your Cloud Account  as Username followed by the server name    or the active directory group you belong to followed by the server name if access to the server was granted to that active directory group.</li> </ol> <p></p> <ol> <li>You can now see in the list of server the newly added server.     Click on it to connect and you will be asked to enter a password</li> </ol> <p></p> <ol> <li>The password you need to enter is a Azure AD access Token that will be generated for an authenticated Azure AD user.     From PowerShell, you can generate this access token by entering the following command</li> </ol> <p>az account get-access-token --resource https://ossrdbms-aad.database.windows.net</p> <p>The output is as follow, where you will need to copy the value for accessToken and use it as Password in  pgAdmin</p> <p></p>"},{"location":"PostgreSQL/#azure-data-studio","title":"Azure Data Studio","text":"<ol> <li>Connectez-vous \u00e0 votre machine virtuelle dans l'EAC et lancez Azure Data Studio. </li> </ol> <ol> <li>The connexion details should be as follow    Where the user name can be the cloud account username followed by the server name:</li> </ol> <p>firstname.lastname\\ @cloud.statcan.ca@servername.postgres.database.azure.com    or    firstname.lastname@cloud.statcan.ca@servername</p> <p>if the access to the server was granted to an active directory group the user belong to</p> <p>AD-Group@servername.postgres.database.azure.com    or    AD-Group@servername</p> <p></p>"},{"location":"R-Shiny/","title":"R-Shiny","text":"<p>This document describes how to access the R-Shiny package from RStudio.</p>"},{"location":"R-Shiny/#getting-started","title":"Getting Started","text":"<p>To use R-Shiny, please send a slack message to the CEA team to enable RStudio on your Databricks cluster.</p> <p>Warning : R-Shiny clusters are shut down every day at 7pm. To save on costs, please stop your R-Shiny clusters when you are not using them.</p>"},{"location":"R-Shiny/#accessing-r-shiny-from-databricks","title":"Accessing R-Shiny from Databricks","text":"<ol> <li> <p>From the Azure portal, launch the Databricks workspace that was created for you.</p> </li> <li> <p>From the Databricks workspace, click on Clusters.</p> <p> </p> </li> <li> <p>From the list of available clusters, select the cluster with RStudio installed.</p> <p> </p> <p>Note: You must have the cluster running before you can access RStudio. See the Databricks section for information on how to start a cluster.</p> </li> <li> <p>Select the Apps tab.</p> <p></p> </li> <li> <p>Click on Set up RStudio.</p> <p></p> </li> <li> <p>A one-time password is generated for you, click on show to display and copy it.</p> <p></p> </li> <li> <p>Click on Open RStudio.</p> <p></p> </li> <li> <p>A new tab opens, enter the username and password provided (step 6) in the login form and sign in to RStudio.</p> <p></p> </li> <li> <p>From the RStudio UI, enter the library(shiny) command in the console to import the Shiny package.</p> <p></p> </li> </ol>"},{"location":"R-Shiny/#rshiny-app-example","title":"RShiny App Example","text":"<p>You can use use the Hello Shiny example to explore the structure of a Shiny app.</p> <ol> <li> <p>Launch the app from your R session by running:</p> <p><code>library(shiny) runExample(\"01_hello\")</code></p> </li> <li> <p>Your app should match the image below.</p> <p></p> </li> </ol>"},{"location":"R-Shiny/#accessing-files-from-the-datalake","title":"Accessing files from the datalake","text":"<p>By default, the working directory in RStudio will be on the driver node of the Databricks cluster. To persist your work, you'll need to use DBFS.</p> <ol> <li> <p>To access DBFS in the File Explorer, click on the ... to the right and enter <code>/dbfs/mnt/</code> at the prompt.</p> <p></p> </li> <li> <p>The data lake will be available and you will be able to access and store your files. When your cluster is terminated at the end of your session, your work will be there for you when you return.</p> </li> </ol> <p>NOTE: Here are some code samples to access your files from the datalake.</p> <pre><code>library(SparkR)\nsparkR.session()\ntestData = as.data.frame(read.df(\"/mnt/the file path\", source = \"the file extension\", header=\"true\", inferSchema = \"true\"))\nstr(testData)\n</code></pre> <pre><code>setwd(\"/dbfs/mnt/the file path\")\ntestData = read.csv(\"the filename\")\nstr(testData)\n</code></pre>"},{"location":"RegisterProject/","title":"Register your Project","text":"<p>Register here to be part of the Data Analytics as a Service (DAaaS) Early Adopter Community. Go to the Data Analyitics Services (DAS) Portal and click on \"Get Started\".</p> <p>https://www.statcan.gc.ca/data-analytics-services</p> <p>If you have already been given access to the platform, see How to login for more information.</p> <p>Before you sign in you may see the Get started button. If you click that button you will be requested to login.             </p> <p>Once on the hub page, you have the option to register a new project by clicking the Start a New project button             </p>"},{"location":"StartHere/","title":"StartHere","text":""},{"location":"StartHere/#collaborative-analytics-environment","title":"Collaborative Analytics Environment","text":""},{"location":"StartHere/#data-storage","title":"Data Storage","text":"Scenario Best Choice Other Choices Relational data from SQL Server or mysql  Total size less than 4 TB with largest fact table &lt; 60 millions Azure SQL Database Synapse SQL Pools or Data Lake (Parquet) Binary Files (images or similar) Azure Data Lake Relational Data with lots of GeoSpatial queries  Requires builtin GeoSpatial functions  Less than 4TB and largest table &lt; 60 million records Azure Database for PostgreSQL Tabular files (CSV, Parquet...) to be used in ML training Azure Data Lake Azure SQL Database SQL Data warehouse  &gt;10TB Storage with fact tables &gt; 100 million records Synapse SQL Pools"},{"location":"StartHere/#notebooks","title":"Notebooks","text":"Scenario Best Choice Other Choices Data manipulation using python with Pandas  Migrating from individual-machine-experience  Dataset &lt; 10 GB (per dataframe) Azure Machine Learning Must use Jupyter notebooks or Jupyter lab  Need access to the terminal of the VM Azure Machine Learning Large dataset  Using Pyspark  Performance is the biggest concern  No dependancy on SQL Pools in Synapse Azure Databricks Azure Synapse Spark Use R on Spark Azure Databricks Need to use Spark on single machine cluster Azure Databricks Use .NET for Spark Azure Synapse Spark Project that has SQL Pools, Pipelines and Spark  Prefer one UI Azure Synapse Spark"},{"location":"StartHere/#data-movement","title":"Data movement","text":"Scenario Best Choice Other Choices Prefer no code or low code. Use relational database Data Factory Synapse Pipelines Prefer no code or low code. Use data lake Synapse Pipelines Data Factory Prefer Python, R Azure Databricks Prefer SQL Synapse Serverlerss SQL Pools"},{"location":"StartHere/#ui-apps","title":"UI apps","text":"Scenario Best Choice Other Choices Desktop software (SQL Server Management Studio, VSCode, SAS Desktop) Virtual Machines  / Azure Virtual Desktop R-Shiny Apps Azure Machine Learning compute + App service Azure Databricks"},{"location":"TestVideo/","title":"TestVideo","text":""},{"location":"TestVideo/#test-video","title":"Test Video","text":""},{"location":"TestVideo/#how-to-login-to-the-azure-portal","title":"How to Login to the Azure Portal","text":"How to Login to Azure Portal - Transcript   Go to portal.azure.com in your web browser. Enter your credential and password. For internal Statcan users, use your cloud account. For external users, use your external account. The first time you login you will be prompted to enter your security questions. For multifactor authentication, you can use the Microsoft Authenticator app on your cell phone."},{"location":"VirtualMachines/","title":"VirtualMachines","text":""},{"location":"VirtualMachines/#azure-dev-test-labs","title":"Azure Dev Test Labs","text":"<p>Please Contact Us to request access to Azure Dev Test Labs. Statistics Canada users should use thier Azure Virtual Desktop to access the environment. Alternatively, Dev Test Labs can be requested when the Azure Virtual Desktop does not have the required software.</p>"},{"location":"VirtualMachines/#find-your-devtest-lab","title":"Find Your DevTest Lab","text":"<ol> <li> <p>In your project's custom Dashboard in the Azure Portal, click on the DevTest Lab.    </p> <p> </p> </li> <li> <p>Select the DevTest Lab that was assigned.</p> </li> </ol>"},{"location":"VirtualMachines/#create-your-virtual-machine","title":"Create Your Virtual Machine","text":"<p>Note: In some instances a Virtual Machine will be pre-created for you and you will not have permission to create a virtual machine. See the FAQ if you need to make changes to your virtual machine.  </p> <ol> <li> <p>From the DevTest Lab Overview page, click on the + Add button.  </p> </li> <li> <p>Choose an appropriate base for your VM (e.g., Data Science Virtual Machine - Windows Server 2019). For more details on the software included with the Data Science Virtual Machines, please click here.  </p> </li> <li> <p>Enter a name for your VM and a User name and password that you will use to login to the VM. Be sure to deselect the Use a saved secret and Save as default password checkboxes.</p> </li> <li> <p>You may click the Change Size Link to change your VM size if you wish to do so.</p> </li> <li> <p>Leave the rest as defaults and click on the Create button.    </p> <p> </p> </li> </ol>"},{"location":"VirtualMachines/#find-your-virtual-machine","title":"Find Your Virtual Machine","text":"<ol> <li>From the DevTest Lab Overview page, scroll down until you see your VM under My virtual machines. Click on your VM to access its Overview page.  </li> </ol>"},{"location":"VirtualMachines/#start-your-virtual-machine","title":"Start Your Virtual Machine","text":"<ol> <li> <p>From the Overview page for your VM, click on the Start button.  </p> <p> </p> </li> <li> <p>It takes a few minutes for your VM to start up. Monitor its startup progress by selecting the Notifications icon at the top right of the window.   </p> <p> </p> </li> </ol>"},{"location":"VirtualMachines/#connect-to-your-virtual-machine","title":"Connect To Your Virtual Machine","text":"<ol> <li> <p>From the Overview page for your VM, click on the Browser connect button (if you do not see a Browser connect button you might have to click on the Connect button and then choose Bastion from the dropdown menu).</p> <p> </p> </li> <li> <p>Ensure the Open in new window checkbox is selected, enter the Username and Password that you used when you created your VM, and click on the Connect button. Your VM should open in a new browser tab.</p> <p>Note : By default, the Ubuntu virtual machine opens in Terminal mode. You can access the GUI of your Ubuntu machine from a Windows machine using X2Go.</p> <p>Note : After attempting to login for the first time, an error may appear that a popup blocker is preventing a new window to open. To disable it, an icon will pop up on the browser's search bar, select the button and click always allow. </p> <p></p> </li> </ol>"},{"location":"VirtualMachines/#stop-your-virtual-machine","title":"Stop Your Virtual Machine","text":"<p>Virtual machines only incur costs while they are running. You should shut down your virtual machine when not in use to prevent unneccessary charges.</p> <ol> <li> <p>From the Overview page for your VM, click on the Stop button.  </p> <p> </p> </li> </ol>"},{"location":"Archive/VirtualMachineAccess/","title":"VirtualMachineAccess","text":""},{"location":"Archive/VirtualMachineAccess/#find-your-virtual-machine","title":"Find Your Virtual Machine","text":"<ol> <li>From your project's DevTest Lab's Overview page scroll down until you see your VM under My virtual machines. Click this to access the Overview page for your VM.</li> </ol>"},{"location":"Archive/VirtualMachineAccess/#start-your-virtual-machine","title":"Start Your Virtual Machine","text":"<ol> <li>From the Overview page for your VM, click the Start button. </li> <li>It takes a few minutes for your VM to start up, monitor its startup progress by selecting the Notifications icon at the top right of the window.  </li> </ol>"},{"location":"Archive/VirtualMachineAccess/#connect-to-your-virtual-machine","title":"Connect To Your Virtual Machine","text":"<ol> <li>From your project's DevTest Lab's Overview page scroll down until you see your VM under My virtual machines. Click this and then from the Overview page for your VM click the Browser connect button (if you do not see a Browser connect button you might have to hit the Connect button and then choose Bastion from the dropdown menu). </li> <li>Ensure the Open in new window checkbox is selected, type in the Username and Password that you used when you created your VM, and click the Connect button. Your VM should open in a new browser tab.</li> </ol>"},{"location":"Archive/VirtualMachineAccess/#stop-your-virtual-machine","title":"Stop Your Virtual Machine","text":"<ol> <li>From the Overview page for your VM, click the Stop button. </li> </ol>"},{"location":"Archive/VirtualMachineCreate/","title":"VirtualMachineCreate","text":""},{"location":"Archive/VirtualMachineCreate/#create-your-virtual-machine","title":"Create Your Virtual Machine","text":"<ol> <li>From your project's DevTest Lab's Overview page press the + Add button.</li> <li>Choose an appropriate base for your VM (eg. Data Science Virtual Machine - Windows Server 2019). For more details on software included with the Data Science Virtual Machines please click here. </li> <li>Enter a name for your VM and a User name and password that you wish to use to log into it. Be sure to deselect Save as default password checkbox. Leave the rest as defaults and click the Create button </li> </ol>"},{"location":"Archive/VirtualMachineDevTestLab/","title":"VirtualMachineDevTestLab","text":""},{"location":"Archive/VirtualMachineDevTestLab/#find-your-devtest-lab","title":"Find Your DevTest Lab","text":"<ol> <li>Go to your project's DevTest Lab by clicking it from within your project's custom Dashboard. </li> </ol>"}]}