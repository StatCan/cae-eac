{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Collaborative Analytics Environment (CAE) The Collaborative Analytics Environment (CAE) provides cloud services for data ingestion, transformation and preparation, as well as data exploration and computation. It includes tools for collaborative analytics, machine learning environments, and data visualization capabilities. Notebook environments and virtual machines provide analytical capabilties using a variety of statistical software such as R, Python, SAS, etc. The CAE leverages Microsoft Azure Platform as a Service (PaaS) and Software as a Service (SaaS) offerings. Environment Overview We are currently testing different use cases against the platform. Each use case can be onboarded into the main or a new private environment can be created. Main (Shared) Environment Shared with users from several use cases. When granted access to this environment, users can view / share data across use cases. Private Environment A private environment configured upon request so only named users can access workspace files. Data Ingestion Data enters the platform via an external storage account. Once inside the platform, the data is stored an internal storage account (Data Lake). Publicly available data sources may be ingested directly via one of the platform tools. External Storage Account Users will be able to access the external storage account from the Internet, and use it to upload / download data in and out the environment. In some private environments, restrictions or additonal vetting processes may be implemented for data upload / download. Internal Storage Account (Data Lake) Files that are uploaded into the external storage account are automatically moved to an internal Data Lake. This Data Lake is located in a secure virtual network, and is only accessible from platform services and virtual machines.","title":"About"},{"location":"#collaborative-analytics-environment-cae","text":"The Collaborative Analytics Environment (CAE) provides cloud services for data ingestion, transformation and preparation, as well as data exploration and computation. It includes tools for collaborative analytics, machine learning environments, and data visualization capabilities. Notebook environments and virtual machines provide analytical capabilties using a variety of statistical software such as R, Python, SAS, etc. The CAE leverages Microsoft Azure Platform as a Service (PaaS) and Software as a Service (SaaS) offerings.","title":"Collaborative Analytics Environment (CAE)"},{"location":"#environment-overview","text":"We are currently testing different use cases against the platform. Each use case can be onboarded into the main or a new private environment can be created.","title":"Environment Overview"},{"location":"#main-shared-environment","text":"Shared with users from several use cases. When granted access to this environment, users can view / share data across use cases.","title":"Main (Shared) Environment"},{"location":"#private-environment","text":"A private environment configured upon request so only named users can access workspace files.","title":"Private Environment"},{"location":"#data-ingestion","text":"Data enters the platform via an external storage account. Once inside the platform, the data is stored an internal storage account (Data Lake). Publicly available data sources may be ingested directly via one of the platform tools.","title":"Data Ingestion"},{"location":"#external-storage-account","text":"Users will be able to access the external storage account from the Internet, and use it to upload / download data in and out the environment. In some private environments, restrictions or additonal vetting processes may be implemented for data upload / download.","title":"External Storage Account"},{"location":"#internal-storage-account-data-lake","text":"Files that are uploaded into the external storage account are automatically moved to an internal Data Lake. This Data Lake is located in a secure virtual network, and is only accessible from platform services and virtual machines.","title":"Internal Storage Account (Data Lake)"},{"location":"Artifactory/","text":"Artifactory The CAE environment uses the Artifiactory for package & Library management. Included packages Below are the URLs that Artifactory pulls from currently. As long as the package is available on these repositories, it can be downloaded. You can get the name of the package by searching through the repositories. Conda-forge CRAN (In the left menu, click Packages under Software , then click Table of available packages ) Python The respective artifactory URLs to use are: - https://jfrog.aaw.cloud.statcan.ca/artifactory/conda-forge-remote/ - https://jfrog.aaw.cloud.statcan.ca/artifactory/dev-cran-remote/ - https://jfrog.aaw.cloud.statcan.ca/artifactory/pypi-remote/ Note : For any other packages, please contact the Collaborative Analytics Environment team. Azure DataBricks The packages can be installed from the Databricks workspace or a cluster. From Databricks workspace 1.From the main page, click Import Library . Under Library Source, choose either PyPI or CRAN . Paste the name of the library under Package and the Artifactory URL under Repository . Click Create . Note : Select Install automatically on all cluster under Admin settings if you wish to install the package on all the available clusters of the workspace. From a cluster 1.From the cluster main page, click Install new under Libraries . Under Library Source , choose either PyPI or CRAN . Paste the name of the library under Package and the Artifactory URL under Repository . Click Install . You should see the installed librarie under Libraries . Azure Machine Learning To install packages on a compute instance, you can use the Curl command in the Azure Machine Learning terminal as following: curl -O \"https://jfrog.aaw.cloud.statcan.ca/artifactory/cae-generic-test/<PackageName>\" Azure Synapse Please contact the Collaborative Analytics Environment team to install custom packages in your Azure Synapse environment. Azure Cloud VM Visual Studio Code 1.On the Extensions tab, click Terminal , then click New Terminal . 2.To install the package, use the pip install command in the terminal as following: pip install <PackageName> -i https://jfrog.aaw.cloud.statcan.ca/artifactory/pypi-remote/","title":"Artifactory"},{"location":"Artifactory/#artifactory","text":"The CAE environment uses the Artifiactory for package & Library management.","title":"Artifactory"},{"location":"Artifactory/#included-packages","text":"Below are the URLs that Artifactory pulls from currently. As long as the package is available on these repositories, it can be downloaded. You can get the name of the package by searching through the repositories. Conda-forge CRAN (In the left menu, click Packages under Software , then click Table of available packages ) Python The respective artifactory URLs to use are: - https://jfrog.aaw.cloud.statcan.ca/artifactory/conda-forge-remote/ - https://jfrog.aaw.cloud.statcan.ca/artifactory/dev-cran-remote/ - https://jfrog.aaw.cloud.statcan.ca/artifactory/pypi-remote/ Note : For any other packages, please contact the Collaborative Analytics Environment team.","title":"Included packages"},{"location":"Artifactory/#azure-databricks","text":"The packages can be installed from the Databricks workspace or a cluster.","title":"Azure DataBricks"},{"location":"Artifactory/#from-databricks-workspace","text":"1.From the main page, click Import Library . Under Library Source, choose either PyPI or CRAN . Paste the name of the library under Package and the Artifactory URL under Repository . Click Create . Note : Select Install automatically on all cluster under Admin settings if you wish to install the package on all the available clusters of the workspace.","title":"From Databricks workspace"},{"location":"Artifactory/#from-a-cluster","text":"1.From the cluster main page, click Install new under Libraries . Under Library Source , choose either PyPI or CRAN . Paste the name of the library under Package and the Artifactory URL under Repository . Click Install . You should see the installed librarie under Libraries .","title":"From a cluster"},{"location":"Artifactory/#azure-machine-learning","text":"To install packages on a compute instance, you can use the Curl command in the Azure Machine Learning terminal as following: curl -O \"https://jfrog.aaw.cloud.statcan.ca/artifactory/cae-generic-test/<PackageName>\"","title":"Azure Machine Learning"},{"location":"Artifactory/#azure-synapse","text":"Please contact the Collaborative Analytics Environment team to install custom packages in your Azure Synapse environment.","title":"Azure Synapse"},{"location":"Artifactory/#azure-cloud-vm","text":"","title":"Azure Cloud VM"},{"location":"Artifactory/#visual-studio-code","text":"1.On the Extensions tab, click Terminal , then click New Terminal . 2.To install the package, use the pip install command in the terminal as following: pip install <PackageName> -i https://jfrog.aaw.cloud.statcan.ca/artifactory/pypi-remote/","title":"Visual Studio Code"},{"location":"AzureML/","text":"New: Please access Azure ML form your CAE Virtual Machine . Accessing Azure Machine Learning Dashboard See the Dashboard section of this documentation from more information. Click on the Dashboard menu from the Azure Portal. Your default view might already be set to dashboard. Under Machine Learning , select the Machine Learning workspace that was created for you. If the workspace you want to open isn't listed, click on See more to access the complete list. Azure Portal In the Azure Portal Search box, search for Machine Learning . You should see the list of the Machine Learning workspaces you were given permission to access. Select the Machine Learning workspace you want to access. Machine Learning URL Navigate to https://ml.azure.com/, sign in with your cloud account credentials, and select vdl subscription and the Machine Learning workspace that was created for you. Getting Started On the machine learning Overview page, click Launch studio . Use the drop-down to select vdl subscription and the Machine Learning workspace you want to access, then click Get started . Once inside your Machine Learning workspace, you can train, deploy and manage machine learning models, use AutoML, and run pipelines. See Getting started quickly for more information. Using Azure ML Notebook standalone Requirements A compute instance in Azure ML. You should see it under Compute --> Compute instances . Note : If a compute instance has not been created for you, please contact the support team via Slack . Steps Under Notebooks , create a new notebook in your user directory. You can then enter the code to execute. Select the Compute instance assigned to you. Click the run all cells button to execute your code. Using Databricks Connect as Remote Compute Disclaimer: Please note that the Databricks connect configuration shown below is under revision and will likely change in the near future. Requirements A compute instance in Azure ML. You should see it under Compute --> Compute instances . Note : If a compute instance has not been created for you, please contact the support team via Slack . Steps Under Notebooks , open Terminal . Select your Compute instance from the drop-down next to Compute . Execute the code from Databricks Connect Setup in the terminal, while following the prompts to continue as needed. This code installs Python 3.7 and sets up a new kernel for Azure ML notebooks. When prompted, enter the following values to configure Databricks connect: Host: the URL from the Overview page for your Databricks workspace. Token: the personal access token generated in your Databricks Workspace User Settings. Cluster ID: the value found under Cluster --> Advanced Options --> Tags in your Databricks workspace. Org ID: the part of the Databricks URL found after .net/?o= Port: keep the existing value Execute the following code in terminal to test the connectivity to Azure Databricks. databricks-connect test Create a new notebook with Azure ML and select the Python 3 kernel . It should now display Python 3.7.9 Databricks connect should be setup now! Try the Databricks connect example code in a notebook, replacing public-data/incoming/1test.txt with the path to a file in your data lake container. Request compute Please contact the support team through the slack channel to request Azure ML compute. You will receive the following error when creating it yourself:","title":"Azure Machine Learning"},{"location":"AzureML/#accessing-azure-machine-learning","text":"","title":"Accessing Azure Machine Learning"},{"location":"AzureML/#dashboard","text":"See the Dashboard section of this documentation from more information. Click on the Dashboard menu from the Azure Portal. Your default view might already be set to dashboard. Under Machine Learning , select the Machine Learning workspace that was created for you. If the workspace you want to open isn't listed, click on See more to access the complete list.","title":"Dashboard"},{"location":"AzureML/#azure-portal","text":"In the Azure Portal Search box, search for Machine Learning . You should see the list of the Machine Learning workspaces you were given permission to access. Select the Machine Learning workspace you want to access.","title":"Azure Portal"},{"location":"AzureML/#machine-learning-url","text":"Navigate to https://ml.azure.com/, sign in with your cloud account credentials, and select vdl subscription and the Machine Learning workspace that was created for you.","title":"Machine Learning URL"},{"location":"AzureML/#getting-started","text":"On the machine learning Overview page, click Launch studio . Use the drop-down to select vdl subscription and the Machine Learning workspace you want to access, then click Get started . Once inside your Machine Learning workspace, you can train, deploy and manage machine learning models, use AutoML, and run pipelines. See Getting started quickly for more information.","title":"Getting Started"},{"location":"AzureML/#using-azure-ml-notebook-standalone","text":"","title":"Using Azure ML Notebook standalone"},{"location":"AzureML/#requirements","text":"A compute instance in Azure ML. You should see it under Compute --> Compute instances . Note : If a compute instance has not been created for you, please contact the support team via Slack .","title":"Requirements"},{"location":"AzureML/#steps","text":"Under Notebooks , create a new notebook in your user directory. You can then enter the code to execute. Select the Compute instance assigned to you. Click the run all cells button to execute your code.","title":"Steps"},{"location":"AzureML/#using-databricks-connect-as-remote-compute","text":"Disclaimer: Please note that the Databricks connect configuration shown below is under revision and will likely change in the near future.","title":"Using Databricks Connect as Remote Compute"},{"location":"AzureML/#requirements_1","text":"A compute instance in Azure ML. You should see it under Compute --> Compute instances . Note : If a compute instance has not been created for you, please contact the support team via Slack .","title":"Requirements"},{"location":"AzureML/#steps_1","text":"Under Notebooks , open Terminal . Select your Compute instance from the drop-down next to Compute . Execute the code from Databricks Connect Setup in the terminal, while following the prompts to continue as needed. This code installs Python 3.7 and sets up a new kernel for Azure ML notebooks. When prompted, enter the following values to configure Databricks connect: Host: the URL from the Overview page for your Databricks workspace. Token: the personal access token generated in your Databricks Workspace User Settings. Cluster ID: the value found under Cluster --> Advanced Options --> Tags in your Databricks workspace. Org ID: the part of the Databricks URL found after .net/?o= Port: keep the existing value Execute the following code in terminal to test the connectivity to Azure Databricks. databricks-connect test Create a new notebook with Azure ML and select the Python 3 kernel . It should now display Python 3.7.9 Databricks connect should be setup now! Try the Databricks connect example code in a notebook, replacing public-data/incoming/1test.txt with the path to a file in your data lake container.","title":"Steps"},{"location":"AzureML/#request-compute","text":"Please contact the support team through the slack channel to request Azure ML compute. You will receive the following error when creating it yourself:","title":"Request compute"},{"location":"AzureSQL/","text":"An Azure SQL Database can be setup in advance if your project requires one. Reminder : The CAE Azure SQL Databases are only accessible from inside the CAE cloud environment. They are not accessible from any of the Government of Canada Data Centres. Accessing Azure SQL Database Azure Data Factory A linked service can be setup inside Azure Data Factory. Configure the linked service to connect via the self-hosted integration runtime and use Managed Identity as the Authentication type . Please contact the support team through the Slack channel if you need assistance. Databricks Databricks notebooks can be configured to connect to the database. Because additional setup is required, please contact the the support team through the Slack channel to request this configuration as well as example notebooks. Virtual Machine You can access an Azure SQL database from your cloud virtual machine, using various applications including: - SQL Server Management Studio - Power BI Desktop - Azure Data Studio - Visual Studio or Visual Studio Code Prerequisites A virtual machine in the Collaborative Analytics Environment (CAE). See the VM page for more information. SQL Server Management Studio or another tool such as Power BI Desktop . These tools are available by default in the Windows Data Science Virtual Machine images. Steps Login to your CAE virtual machine . Launch a tool such as SQL Server Management Studio . Choose Azure Active Directory - Universal with MFA as the Authentication type. Enter your Azure SQL server name and your cloud account username as User name. Click on Options . In the Connection Properties tab, enter your database name next to the Connect to database label and click on Connect . Sign in with your cloud account credentials.","title":"Azure SQL Database"},{"location":"AzureSQL/#accessing-azure-sql-database","text":"","title":"Accessing Azure SQL Database"},{"location":"AzureSQL/#azure-data-factory","text":"A linked service can be setup inside Azure Data Factory. Configure the linked service to connect via the self-hosted integration runtime and use Managed Identity as the Authentication type . Please contact the support team through the Slack channel if you need assistance.","title":"Azure Data Factory"},{"location":"AzureSQL/#databricks","text":"Databricks notebooks can be configured to connect to the database. Because additional setup is required, please contact the the support team through the Slack channel to request this configuration as well as example notebooks.","title":"Databricks"},{"location":"AzureSQL/#virtual-machine","text":"You can access an Azure SQL database from your cloud virtual machine, using various applications including: - SQL Server Management Studio - Power BI Desktop - Azure Data Studio - Visual Studio or Visual Studio Code","title":"Virtual Machine"},{"location":"AzureSQL/#prerequisites","text":"A virtual machine in the Collaborative Analytics Environment (CAE). See the VM page for more information. SQL Server Management Studio or another tool such as Power BI Desktop . These tools are available by default in the Windows Data Science Virtual Machine images.","title":"Prerequisites"},{"location":"AzureSQL/#steps","text":"Login to your CAE virtual machine . Launch a tool such as SQL Server Management Studio . Choose Azure Active Directory - Universal with MFA as the Authentication type. Enter your Azure SQL server name and your cloud account username as User name. Click on Options . In the Connection Properties tab, enter your database name next to the Connect to database label and click on Connect . Sign in with your cloud account credentials.","title":"Steps"},{"location":"AzureStorage/","text":"Data can be uploaded to the platform via the Azure Portal or the Azure Storage Explorer application. Once data uploaded to an external Blob storage account, it is automatically ingested into an internal Azure Data Lake Storage (ADLS) account. Once data is in the data lake, users have their choice of tools for transformation and integration. They can use Web based tools such as Databricks and Data Factory to do their transformations or they can use desktop tools on a virtual machine (VM) to tansform & analyse the data. Cleansed and transformed data can be placed into different folders (containing higher quality / processed datasets) or loaded into a database. Users can once again connect to this data with the tools they would like to use, either from their VMs or other platform services such as Databricks and Data Factory. Reminder: Internal Storage Accounts can only be accessed from a VM in the Collaborative Analytics Environment (CAE) - See the FAQ Storage Explorer - Azure Portal Navigate to the Storage Account (Preview) from the Azure Portal. Select your subscription, then navigate the storage account. Storage Explorer - Personal Workstation or Cloud Virtual Machine Download the Azure Storage Explorer application, and install it on your workstation or VM. Launch Azure Storage Explorer from the Start menu. Login with your Azure Account. Enter your Cloud credentials Storage Explorer - Network B VDI This section is for Statistics Canada employees who need to upload data from Network B. 1. Download the Azure Storage Explorer application, and install it on your Network B VDI. Launch Azure Storage Explorer from the Start menu. On a Network B VDI, you can only access your storage account with a temporary SAS token. Please contact the support team through the Slack channel to obtain one. Note: See the FAQ for information on configuring Network B proxy settings. Microsoft Documentation Azure Storage Explorer Download Quickstart: Upload, download, and list blobs with the Azure portal","title":"Azure Storage"},{"location":"AzureStorage/#storage-explorer-azure-portal","text":"Navigate to the Storage Account (Preview) from the Azure Portal. Select your subscription, then navigate the storage account.","title":"Storage Explorer - Azure Portal"},{"location":"AzureStorage/#storage-explorer-personal-workstation-or-cloud-virtual-machine","text":"Download the Azure Storage Explorer application, and install it on your workstation or VM. Launch Azure Storage Explorer from the Start menu. Login with your Azure Account. Enter your Cloud credentials","title":"Storage Explorer - Personal Workstation or Cloud Virtual Machine"},{"location":"AzureStorage/#storage-explorer-network-b-vdi","text":"This section is for Statistics Canada employees who need to upload data from Network B. 1. Download the Azure Storage Explorer application, and install it on your Network B VDI. Launch Azure Storage Explorer from the Start menu. On a Network B VDI, you can only access your storage account with a temporary SAS token. Please contact the support team through the Slack channel to obtain one. Note: See the FAQ for information on configuring Network B proxy settings.","title":"Storage Explorer - Network B VDI"},{"location":"AzureStorage/#microsoft-documentation","text":"Azure Storage Explorer Download Quickstart: Upload, download, and list blobs with the Azure portal","title":"Microsoft Documentation"},{"location":"AzureSynapse/","text":"Getting Started Access Azure Synapse Make sure that you are in your cloud virtual machine to access Azure Synapse. See Virtual Machines for information on how to create one if needed. Inside your virtual machine, open a web browser and navigate to the Azure Portal . Sign in with your cloud account credentials. a . Click on the ** Azure Synapse Analytics ** icon under ** Azure services ** . If you do not see this icon , follow step 3 b instead . b. Start typing \"synapse\" into the search bar to find Azure Synapse Analytics . Find your Synapse workspace in the list and click on it. Then click Open Synapse Studio . Note: You can also acccess Synapse workspaces from the Collaborative Analytics Environment dashboard. Start and Stop Dedicated SQL Pool Click the Integrate tab. Under Pipelines , click either Start Dedicated SQL Pool or Pause Dedicated SQL Pool . Then click the trigger button to open a menu, and select Trigger now . On the next screen, click OK . Home The Home tab is where you start when you first open Azure Synapse Studio. From here, you can access shortcuts for common tasks such as creating SQL scripts or notebooks by clicking the New dropdown menu button. Recently opened resources are also displayed. Data The Data tab is where you can explore everything in your database and linked datasets. Under the Workspace tab, you can explore the dedicated SQL pool database and any Spark databases. Under the Linked tab, you can explore external objects (e.g. Data Lake accounts) and explore and create any integration datasets from external linked data (e.g. Data Lake, Blob Storage, web service, etc) to be used in pipelines. How to Bring in Data from Linked Services Note : This example shows how to get data from Data Lake, although there are many source types available. Click the \u00ab + \u00bb button the add a new resource, then click Integration Dataset . Select Azure Data Lake Storage Gen2 (you may need to search for this), then click Continue . Select the format type, then click Continue . Enter a name, then click the drop-down menu under Linked service and select your data lake. Under Connect via integration runtime , ensure that interactive authoring is enabled. If it is not, click the edit button to enable it, then click Apply . Set additional properties as appropriate, then click OK . How to Explore Data in the Data Lake Browse to find your dataset file (CSV, Parquet, JSON, Avro, etc) and right click it. A menu will open with options to preview the data, or create resources such as SQL scripts and notebooks. How to Explore the Dedicated SQL Pool Under the Workspace tab, you can explore databases similarly to SQL Server Management Studio. Right click any table, highlight New SQL script , and click Select TOP 100 rows to create a new query. You can then view the results as either a table or a chart. Importing Data to the Dedicated SQL Pool To import data to the dedicated SQL pool, you can either: - create a pipeline with a Copy data activity (most efficient for large datasets) - use the Bulk Load Wizard . Develop From here, you can create and save resources such as SQL scripts, notebooks, and Power BI reports. To add a new resource, click the \u00ab + \u00bb button. A dropdown menu will open. To make your changes visible to others, you need to click the Publish button. SQL Scripts Be sure to connect to your dedicated SQL pool to run SQL scripts. Notebooks To run notebook cells, you first need to select your Apache Spark pool. To change languages for a single cell, you can use the following magic commands: %%pyspark, %%spark, %%csharp, %%sql. You can also change the default language using the Language dropdown menu. Dataflows To add a source to a dataflow, click the \u00ab + \u00bb button under Source Settings , then select Azure Data Lake Storage Gen2 (you may need to search for this). Click Continue , select the data format, then on the next page, select your Linked Service. Power BI Reports You can view and create Power BI reports directly in Azure Synapse. Please contact the Collabotative Analytics Environment support team to validate that a linked service is set up. Integrate This is where you can create pipelines for ingesting, preparing and transforming all of your data, like in Azure Data Factory . Example: Copy Data from External Blob to Data Lake Click the \u00ab + \u00bb button to add a new resource, then click Pipeline . Under Move & transform , drag and drop Copy data into the window. Click on the Source tab, then click New to add the source dataset (where you want to copy the data from). Select Azure Blob Storage , then select the format type (CSV, Parquet, JSON, etc). Set any additional properties if relevant, then click OK . Click Sink , then click New to set the sink dataset (where you want the data to be copied to). Choose Azure Data Lake Storage Gen2 , then select the format type. Under Linked service , choose your data lake and ensure that interactive authoring is enabled (see How to Bring in Data from Linked Services under Data for more information). Debugging and Running Pipelines To run a pipeline in debug mode, click the Debug button at the top of the pipeline window. Results will appear in the Output tab. To run a pipeline without debugging, click the Add trigger button, then Trigger now . When you are ready to publish your pipelines, click the Validate all button, then click the Publish all button. Note that this will publish for all users to see everything that you currently have open (pipelines, SQL scripts, notebooks, etc). Monitor From the Monitor tab, you can monitor live pipeline runs (the inputs and outputs of each activity and any errors) and view historical pipeline runs, trigger runs, SQL requests, etc. Manage This is where you can: - Add new SQL or Apache Spark pools - Add new linked services - Grant others access to the workspace - Set up git integration Microsoft Documentation Azure Synapse Analytics What is Azure Synapse Analytics? Analyse Data with Dedicated SQL Pools Integrate with Pipelines Visualize Data with Power BI Monitor Your Synapse Workspace","title":"Azure Synapse"},{"location":"AzureSynapse/#getting-started","text":"","title":"Getting Started"},{"location":"AzureSynapse/#access-azure-synapse","text":"Make sure that you are in your cloud virtual machine to access Azure Synapse. See Virtual Machines for information on how to create one if needed. Inside your virtual machine, open a web browser and navigate to the Azure Portal . Sign in with your cloud account credentials. a . Click on the ** Azure Synapse Analytics ** icon under ** Azure services ** . If you do not see this icon , follow step 3 b instead . b. Start typing \"synapse\" into the search bar to find Azure Synapse Analytics . Find your Synapse workspace in the list and click on it. Then click Open Synapse Studio . Note: You can also acccess Synapse workspaces from the Collaborative Analytics Environment dashboard.","title":"Access Azure Synapse"},{"location":"AzureSynapse/#start-and-stop-dedicated-sql-pool","text":"Click the Integrate tab. Under Pipelines , click either Start Dedicated SQL Pool or Pause Dedicated SQL Pool . Then click the trigger button to open a menu, and select Trigger now . On the next screen, click OK .","title":"Start and Stop Dedicated SQL Pool"},{"location":"AzureSynapse/#home","text":"The Home tab is where you start when you first open Azure Synapse Studio. From here, you can access shortcuts for common tasks such as creating SQL scripts or notebooks by clicking the New dropdown menu button. Recently opened resources are also displayed.","title":"Home"},{"location":"AzureSynapse/#data","text":"The Data tab is where you can explore everything in your database and linked datasets. Under the Workspace tab, you can explore the dedicated SQL pool database and any Spark databases. Under the Linked tab, you can explore external objects (e.g. Data Lake accounts) and explore and create any integration datasets from external linked data (e.g. Data Lake, Blob Storage, web service, etc) to be used in pipelines.","title":"Data"},{"location":"AzureSynapse/#how-to-bring-in-data-from-linked-services","text":"Note : This example shows how to get data from Data Lake, although there are many source types available. Click the \u00ab + \u00bb button the add a new resource, then click Integration Dataset . Select Azure Data Lake Storage Gen2 (you may need to search for this), then click Continue . Select the format type, then click Continue . Enter a name, then click the drop-down menu under Linked service and select your data lake. Under Connect via integration runtime , ensure that interactive authoring is enabled. If it is not, click the edit button to enable it, then click Apply . Set additional properties as appropriate, then click OK .","title":"How to Bring in Data from Linked Services"},{"location":"AzureSynapse/#how-to-explore-data-in-the-data-lake","text":"Browse to find your dataset file (CSV, Parquet, JSON, Avro, etc) and right click it. A menu will open with options to preview the data, or create resources such as SQL scripts and notebooks.","title":"How to Explore Data in the Data Lake"},{"location":"AzureSynapse/#how-to-explore-the-dedicated-sql-pool","text":"Under the Workspace tab, you can explore databases similarly to SQL Server Management Studio. Right click any table, highlight New SQL script , and click Select TOP 100 rows to create a new query. You can then view the results as either a table or a chart.","title":"How to Explore the Dedicated SQL Pool"},{"location":"AzureSynapse/#importing-data-to-the-dedicated-sql-pool","text":"To import data to the dedicated SQL pool, you can either: - create a pipeline with a Copy data activity (most efficient for large datasets) - use the Bulk Load Wizard .","title":"Importing Data to the Dedicated SQL Pool"},{"location":"AzureSynapse/#develop","text":"From here, you can create and save resources such as SQL scripts, notebooks, and Power BI reports. To add a new resource, click the \u00ab + \u00bb button. A dropdown menu will open. To make your changes visible to others, you need to click the Publish button.","title":"Develop"},{"location":"AzureSynapse/#sql-scripts","text":"Be sure to connect to your dedicated SQL pool to run SQL scripts.","title":"SQL Scripts"},{"location":"AzureSynapse/#notebooks","text":"To run notebook cells, you first need to select your Apache Spark pool. To change languages for a single cell, you can use the following magic commands: %%pyspark, %%spark, %%csharp, %%sql. You can also change the default language using the Language dropdown menu.","title":"Notebooks"},{"location":"AzureSynapse/#dataflows","text":"To add a source to a dataflow, click the \u00ab + \u00bb button under Source Settings , then select Azure Data Lake Storage Gen2 (you may need to search for this). Click Continue , select the data format, then on the next page, select your Linked Service.","title":"Dataflows"},{"location":"AzureSynapse/#power-bi-reports","text":"You can view and create Power BI reports directly in Azure Synapse. Please contact the Collabotative Analytics Environment support team to validate that a linked service is set up.","title":"Power BI Reports"},{"location":"AzureSynapse/#integrate","text":"This is where you can create pipelines for ingesting, preparing and transforming all of your data, like in Azure Data Factory .","title":"Integrate"},{"location":"AzureSynapse/#example-copy-data-from-external-blob-to-data-lake","text":"Click the \u00ab + \u00bb button to add a new resource, then click Pipeline . Under Move & transform , drag and drop Copy data into the window. Click on the Source tab, then click New to add the source dataset (where you want to copy the data from). Select Azure Blob Storage , then select the format type (CSV, Parquet, JSON, etc). Set any additional properties if relevant, then click OK . Click Sink , then click New to set the sink dataset (where you want the data to be copied to). Choose Azure Data Lake Storage Gen2 , then select the format type. Under Linked service , choose your data lake and ensure that interactive authoring is enabled (see How to Bring in Data from Linked Services under Data for more information).","title":"Example: Copy Data from External Blob to Data Lake"},{"location":"AzureSynapse/#debugging-and-running-pipelines","text":"To run a pipeline in debug mode, click the Debug button at the top of the pipeline window. Results will appear in the Output tab. To run a pipeline without debugging, click the Add trigger button, then Trigger now . When you are ready to publish your pipelines, click the Validate all button, then click the Publish all button. Note that this will publish for all users to see everything that you currently have open (pipelines, SQL scripts, notebooks, etc).","title":"Debugging and Running Pipelines"},{"location":"AzureSynapse/#monitor","text":"From the Monitor tab, you can monitor live pipeline runs (the inputs and outputs of each activity and any errors) and view historical pipeline runs, trigger runs, SQL requests, etc.","title":"Monitor"},{"location":"AzureSynapse/#manage","text":"This is where you can: - Add new SQL or Apache Spark pools - Add new linked services - Grant others access to the workspace - Set up git integration","title":"Manage"},{"location":"AzureSynapse/#microsoft-documentation","text":"Azure Synapse Analytics What is Azure Synapse Analytics? Analyse Data with Dedicated SQL Pools Integrate with Pipelines Visualize Data with Power BI Monitor Your Synapse Workspace","title":"Microsoft Documentation"},{"location":"BestPractices/","text":"What is the best file format to use for large data files? Recommend using newer format like Parquet because it does save larger datesets in a smaller file in comparison to a CSV file. If only accessing certain sections of the dataset, it is also faster using Parquet as it uses columnar storage format. Do I need a SQL database? In many cases a SQL database is not needed, data can be saved in files to the datalake. Do I need a SQL database when using Power BI? It is not needed to have an SQL datbase when using Power BI. You are able to read files from the Azure Storage. A database is only needed when you are using a more complex star-schema like system. Additional Information: How to connect to the internal data lake with Power BI Desktop . How should we structure our projects data lake container? There are 3 parts in which to structure your data lake container: Bronze/Raw Zone This zone stores the original format of any files or files/data that is immutable. The data contained in this zone is usually locked and are only accessible to certain members or is read-only. This zone is also organised in different folders per source system, with each ingestion process having a write access to only their associated folder. Silver/Cleansed Zone This zone is where parts of data removes unnecessary columns from the data, validates, standarizes and harmonises that data within this zone. This zone is mainly a folder per project. Any data that must be accessed within this zone is usually granted read-only access. Gold/Curated Zone This zone is mainly for analytics rather than data ingestion or processing. The data in the curated zone is stored in star schemas. The dimensional modelling is usually done using Spark or Data Factory instead of inside the database engine. But if the dimensional modelling is done outside of the lake then it is best to publish the model back to the lake. This zone is best suited to run for large-scale queries and analysis that do not have strict time-sensitive reporting needs. Laboratory Zone This zone is mainly for experimentation and exploration. It is used for prototype and innovation mixing both your own data sets with data sets from production. This zone is not a replacement for a development or test data lake which is required for more careful development. Each wil data lake project would have their own laboratory area via a folder. Permissions in this zone are typically read and write for each user/project. For more information about structuring your projects data lake container: Building your Data Lake on Azure Data Lake Storage gen2 Designing an Azure Data Lake Store Gen2 I get an out of memory exception in Databricks? Option 1: The fastest and most expensive way to fix this is to increase the size of your cluster. To increase the size of the cluster, please contact the CAE support team to increase the size of the cluster Option 2: For a more programatic answer, if you are using pandas, it is also a suggestion to switch over and use pySpark or koalas. PySpark and koalas can run faster than pandas, it has better benefits from using data ingestion pipelines and also works efficiently as it runs parallel on different nodes in a cluster. More information: Spark Dataframe and Operations Option 3: Consider to use a subset of your data when doing queries if possible. If you are working with only a certain section of the dataset but are quering through all of it, it is possible to use just the subset. Option 4: Consider changing the file format to something like Parquet or Avro which uses less space than a traditional CSV file. Conversion from CSV to Parquet: % python testConvert = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/mnt/public-data/incoming/titanic.csv' ) testConvert . write . parquet ( \"/mnt/public-data/incoming/testingFile\" ) Conversion from CSV to Avro: % python diamonds = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/mnt/public-data/incoming/titanic.csv' ) diamonds . write . format ( \"avro\" ) . save ( \"/mnt/public-data/incoming/testingFile\" ) How can i easily convert SAS code to Python or R? It is not possible to easily convert SAS code to Python or R automatically, the only known way to convert is to manually do the conversion. How do I validate that I am developing my application in the most cost effective way in the cloud using Microsoft technologies (CAE)? There are plenty of ways to validate that your development is the most cost effective it can be: Take advantage of Spark in databricks. a. Spark is a great addition to databricks that runs faster and better especially for large data sets. Using Spark would cost less because it does take less time to do its task. Using spark will also Make sure you cluster is running for the minimal amout of time. a. If the cluster is no longer needed or not being use, ensure that it is not running and only run when it is needed. Ensure your databricks cluster is correctly sized. a. Make sure that you have to correct amount of workers in your cluster, too many clusters results in a higher cost. Delete data files that you are not using. a. Ensure that any files that are no longer needed or not in use anymore are deleted from the container. Try not to do processing on a cloud VM. Ask for a review of your architecture. Code review. If you are using Pandas, it is a good idea to switch over to Koalas. Use a file format that is optimal for your work load (i.e. Parquet, Avro) How should data be structured if we plan to use Power BI? Data should be structured using the Star Schema. For more details about using Star Schema, click this link: Understand star schema and the importance for Power BI . How to read in an Excel file from Databricks? Here is an example of how to read an Excel file using Python: % python import pandas as pd pd . read_excel ( \"/dbfs/mnt/ccei-ccie-ext/Daily charts.xlsx\" , engine = 'openpyxl' ) Which file types are best to use when? Parquet It is good to use for very large datasets. It is also good to use if only a section of the dataset is needed which reads in the data in a faster rate. Read: % python data = spark . read . parquet ( \"/tmp/testParquet\" ) display ( data ) Write: % python # Assumption that a dataframe has been created already data . write . parquet ( \"/tmp/tempFile\" ) Avro Just as Parquet, it is great for very large datasets. To compare, it is better used for editing/writing into a dataset and for querying all columns in the dataset. Read: data = spark . read . format ( \"avro\" ) . load ( \"/tmp/test_dataset\" ) display ( data ) Write: % scala val ex = Seq (( 132 , \"baseball\" ), ( 148 , \"softball\" ), ( 172 , \"slow pitch\" )). toDF ( \"players\" , \"sport\" ) ex . write . format ( \"avro\" ). save ( \"/tmp/testExample\" ) CSV It is fine to use with marginally smaller datasets as CSV files do not load well when the file size is very large. But with smaller data sets, it is simple and human-readable. For writing within a CSV file, it is also good to note that you are able to edit the file with Office. Read: % python data = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/tmp/test_dataCSV.csv' ) display ( data ) Excel Please see above on how to use Excel. The other formats above are perferable over excel. How to convert files (CSV, Text, JSON) to parquet using databricks? The rule of thumb in converting a file to parquet is to first read in the file and then write a new file into parquet CSV to Parquet: % python testConvert = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/mnt/public-data/incoming/titanic.csv' ) testConvert . write . parquet ( \"/mnt/public-data/incoming/testingFile\" ) JSON to Parquet: % python testConvert = spark . read . json ( 'tmp/test.json' ) testConvert . write . parquet ( 'tmp/testingJson' ) Text to Parquet: % python testConvert = spark . read . text ( \"/mnt/public-data/incoming/testing.txt\" ) testConvert . write . parquet ( \"/mnt/public-data/incoming/testingFile\" ) Can I read Word document in Databricks? It is best practice to read Word documents via Office instead. When should we use ADF vs. Databricks for data ingestion? Databricks is able to do real-time streaming through the Apache Spark API that can handle the streaming analytics workloads. Databricks does not need you to wrap the python code into functions or executable modules, all the code is able to work just as is. Databricks also supports Machine Learning which makes data ingestion easier as well. For any code that is already in an Azure Function or is easily translated into an executable, using data factory is usable. Data factory is also good to use if it is a heavy algorithm that is not usable within Databricks. What is the difference between SQL database temporal tables and Delta Lake? SQL temporal tables is specific to SQL 2018 and is not currently available in Azure Synapse. On the other hand, Delta lake is available in both Azure Synapse and in Databricks. Another difference is that SQL temporal tables are only available with only SQL queries while Delta lake time travel is available in Scala, Python, and SQL. When to use Power BI or R-Shiny? It is recommended to use Power BI over R-shiny because less coding is required when using Power BI. There are a lot of benefits to using Power BI including the additional amount of chart types that are at hand, visualisation of data into charts is easier to use in Power BI compared to R-Shiny, the creation of a dashboard is faster within PowerBI, and the ease of connectivity with other applications within Azure. When is a good time to use Azure Synapse vs. ADF and Databricks? Azure Synapse is good to use when doing queries and data analysis via the data lake, doing SQL analyses and data warehousing, and using additional services like Power BI. It is easy to query data from the data lake using Azure Snapse and you do not have to mount the data lake to the workspace. As for data analyses and data warehousing, synapse is perferred as it allows full realtional data models, provide all SQL features and also uses Delta Lake. Synapse also includes direct services with Power BI for ease of use. On the other hand, Databricks is preferred when doing machine learning development and real-time transformations. Databricks includes their own machine learning development that includes popular libraries like PyTorch, manage version of MLflow. Databricks is also preferred for real-time transformations as it uses Spark structured streaming and it gives you the ability to view changes from other users in real time. When should we use a SQL database data warehouse vs. Delta Lake? Best practice would be to use Delta lake over SQL server as it does not use additional SQL compute resouces and will reduce the overall cloud costs. How can i easily convert SAS files to another format? Statcan users can use SAS on the internal stats-can network to convert it to a supported file form. You are able to convert a SAS file to CSV or JSON with this method: First open databricks and install the sas7bdat-converter within your notebook. python %pip install sas7bdat-converter Using python and your code editor of your choice, type in this code with the file directory that the file is in and the directory where you want the output file to be in. ```python %python import sas7bdat_converter file_dicts = [{ 'sas7bdat_file': '/dbfs/mnt/public-data/ToNetA/sas7bdat/tablea_1_10k.sas7bdat', 'export_file': '/dbfs/mnt/public-data/testFolder/testingConvert.csv', }] sas7bdat_converter.batch_to_csv(file_dicts) ``` You will then get the output file within the directory you have specified. Converter Documentation: sas7bdat website documentation Can\\How I convert a Word document to a notebook? There is no easy way to convert a word document to a notebook. A manual solution to convert a Word document to a notebook is by copying any of the code that is within the word document into a notebook. How big of a dataframe/spark table can we store within the workspace? Spark tables are stored as parquet files and are stored in the internal storage account linked with the Databricks workspace, but it is best practice to delete the table if it is no longer in use. What is the best way to get data files into Azure ML? The best way would be to upload your files to the data lake. If you need to add a new cloud storage account, contact the CAE team to add the storage account to the Azure ML studio. Whats the difference to Machine Learning in Databricks or in Azure ML? The main difference between Azure ML and Databricks is the language that each application uses. Azure ML utilizes python-based libraries or R while Databricks utilizes the Apache Spark Platform and MLFlow. Azure ML also contains a tracking system which is able to track individual runs of the experiment and include the specific metrics of what wants to be seen. Databricks includes MLflow which also allows tracking but does not come with as many features as Azure ML. As a recommendation, it is best practice to use Databricks for data preperation and for large datasets but to use Azure ML for their tracking system, machine learning on normal datasets, deep learning on GPUs, and operationalization. How do you create a Table in Databricks? Option 1: Use Create Table function In Databricks, select Data and within the Database you have selected, click on Create Table. For more information about creating tables in Databricks: Databricks Create a Table . Option 2: Create Table from Dataframe table Python: df . write . saveAsTable ( \"Table-Name\" ) SQL: CREATE TABLE IF NOT EXISTS Table - Name AS SELECT * FROM df Option 3: Create Table Programatically SQL: CREATE TABLE example (id INT, name STRING, age INT) USING CSV; When to use Spark Dataframe or Spark Table? There are really no difference between using a Spark Dataframe or Spark Table. Currently with Databricks, best practice right now would be to store tables as delta tables as it is saved in parquet format and gives the tracking capabilities. What should I do if the size of the broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxResultSize = _____? Change the Spark configuration \"spark.driver.maxResultSize\" to \"0\" (means no limit) or something larger than your needs. What Should I do if I cannot broadcast the table that is larger than 8GB? This occurs only with BroadcastHashJoin. There are 2 options: Change the Spark configuration \"spark.sql.autoBroadcastJoinThreshold\" to \"-1\". This forces Databricks to perform a SortMergeJoin. Note About changing Spark Configuration Warning: Changing Spark configuraitons can cause out-of-memory-errors Normal approach: - spark.conf.set(\"configuration\", \"value\") If you do not have permissions to change some configurations, this seems to be a work around: - conf = spark.sparkContext._cibf,setAkk([(\"configuration\", \"value\"), (\"configuration\", \"value\")]) How to get Spark Configuration: - spark.conf.get(\"configuration\") Steps to avoid changing configurations: a. Partition DataFrame A into parts. b. Perform joins with each parittion from DataFrame A with DataFrame B (concurrently is the fastest way but may require writing Dataframes to file for reading in next step). c. Perform a union on all the joined DataFrames.","title":"Best Practices"},{"location":"BestPractices/#what-is-the-best-file-format-to-use-for-large-data-files","text":"Recommend using newer format like Parquet because it does save larger datesets in a smaller file in comparison to a CSV file. If only accessing certain sections of the dataset, it is also faster using Parquet as it uses columnar storage format.","title":"What is the best file format to use for large data files?"},{"location":"BestPractices/#do-i-need-a-sql-database","text":"In many cases a SQL database is not needed, data can be saved in files to the datalake.","title":"Do I need a SQL database?"},{"location":"BestPractices/#do-i-need-a-sql-database-when-using-power-bi","text":"It is not needed to have an SQL datbase when using Power BI. You are able to read files from the Azure Storage. A database is only needed when you are using a more complex star-schema like system. Additional Information: How to connect to the internal data lake with Power BI Desktop .","title":"Do I need a SQL database when using Power BI?"},{"location":"BestPractices/#how-should-we-structure-our-projects-data-lake-container","text":"There are 3 parts in which to structure your data lake container:","title":"How should we structure our projects data lake container?"},{"location":"BestPractices/#bronzeraw-zone","text":"This zone stores the original format of any files or files/data that is immutable. The data contained in this zone is usually locked and are only accessible to certain members or is read-only. This zone is also organised in different folders per source system, with each ingestion process having a write access to only their associated folder.","title":"Bronze/Raw Zone"},{"location":"BestPractices/#silvercleansed-zone","text":"This zone is where parts of data removes unnecessary columns from the data, validates, standarizes and harmonises that data within this zone. This zone is mainly a folder per project. Any data that must be accessed within this zone is usually granted read-only access.","title":"Silver/Cleansed Zone"},{"location":"BestPractices/#goldcurated-zone","text":"This zone is mainly for analytics rather than data ingestion or processing. The data in the curated zone is stored in star schemas. The dimensional modelling is usually done using Spark or Data Factory instead of inside the database engine. But if the dimensional modelling is done outside of the lake then it is best to publish the model back to the lake. This zone is best suited to run for large-scale queries and analysis that do not have strict time-sensitive reporting needs.","title":"Gold/Curated Zone"},{"location":"BestPractices/#laboratory-zone","text":"This zone is mainly for experimentation and exploration. It is used for prototype and innovation mixing both your own data sets with data sets from production. This zone is not a replacement for a development or test data lake which is required for more careful development. Each wil data lake project would have their own laboratory area via a folder. Permissions in this zone are typically read and write for each user/project. For more information about structuring your projects data lake container: Building your Data Lake on Azure Data Lake Storage gen2 Designing an Azure Data Lake Store Gen2","title":"Laboratory Zone"},{"location":"BestPractices/#i-get-an-out-of-memory-exception-in-databricks","text":"","title":"I get an out of memory exception in Databricks?"},{"location":"BestPractices/#option-1","text":"The fastest and most expensive way to fix this is to increase the size of your cluster. To increase the size of the cluster, please contact the CAE support team to increase the size of the cluster","title":"Option 1:"},{"location":"BestPractices/#option-2","text":"For a more programatic answer, if you are using pandas, it is also a suggestion to switch over and use pySpark or koalas. PySpark and koalas can run faster than pandas, it has better benefits from using data ingestion pipelines and also works efficiently as it runs parallel on different nodes in a cluster. More information: Spark Dataframe and Operations","title":"Option 2:"},{"location":"BestPractices/#option-3","text":"Consider to use a subset of your data when doing queries if possible. If you are working with only a certain section of the dataset but are quering through all of it, it is possible to use just the subset.","title":"Option 3:"},{"location":"BestPractices/#option-4","text":"Consider changing the file format to something like Parquet or Avro which uses less space than a traditional CSV file. Conversion from CSV to Parquet: % python testConvert = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/mnt/public-data/incoming/titanic.csv' ) testConvert . write . parquet ( \"/mnt/public-data/incoming/testingFile\" ) Conversion from CSV to Avro: % python diamonds = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/mnt/public-data/incoming/titanic.csv' ) diamonds . write . format ( \"avro\" ) . save ( \"/mnt/public-data/incoming/testingFile\" )","title":"Option 4:"},{"location":"BestPractices/#how-can-i-easily-convert-sas-code-to-python-or-r","text":"It is not possible to easily convert SAS code to Python or R automatically, the only known way to convert is to manually do the conversion.","title":"How can i easily convert SAS code to Python or R?"},{"location":"BestPractices/#how-do-i-validate-that-i-am-developing-my-application-in-the-most-cost-effective-way-in-the-cloud-using-microsoft-technologies-cae","text":"There are plenty of ways to validate that your development is the most cost effective it can be: Take advantage of Spark in databricks. a. Spark is a great addition to databricks that runs faster and better especially for large data sets. Using Spark would cost less because it does take less time to do its task. Using spark will also Make sure you cluster is running for the minimal amout of time. a. If the cluster is no longer needed or not being use, ensure that it is not running and only run when it is needed. Ensure your databricks cluster is correctly sized. a. Make sure that you have to correct amount of workers in your cluster, too many clusters results in a higher cost. Delete data files that you are not using. a. Ensure that any files that are no longer needed or not in use anymore are deleted from the container. Try not to do processing on a cloud VM. Ask for a review of your architecture. Code review. If you are using Pandas, it is a good idea to switch over to Koalas. Use a file format that is optimal for your work load (i.e. Parquet, Avro)","title":"How do I validate that I am developing my application in the most cost effective way in the cloud using Microsoft technologies (CAE)?"},{"location":"BestPractices/#how-should-data-be-structured-if-we-plan-to-use-power-bi","text":"Data should be structured using the Star Schema. For more details about using Star Schema, click this link: Understand star schema and the importance for Power BI .","title":"How should data be structured if we plan to use Power BI?"},{"location":"BestPractices/#how-to-read-in-an-excel-file-from-databricks","text":"Here is an example of how to read an Excel file using Python: % python import pandas as pd pd . read_excel ( \"/dbfs/mnt/ccei-ccie-ext/Daily charts.xlsx\" , engine = 'openpyxl' )","title":"How to read in an Excel file from Databricks?"},{"location":"BestPractices/#which-file-types-are-best-to-use-when","text":"","title":"Which file types are best to use when?"},{"location":"BestPractices/#parquet","text":"It is good to use for very large datasets. It is also good to use if only a section of the dataset is needed which reads in the data in a faster rate. Read: % python data = spark . read . parquet ( \"/tmp/testParquet\" ) display ( data ) Write: % python # Assumption that a dataframe has been created already data . write . parquet ( \"/tmp/tempFile\" )","title":"Parquet"},{"location":"BestPractices/#avro","text":"Just as Parquet, it is great for very large datasets. To compare, it is better used for editing/writing into a dataset and for querying all columns in the dataset. Read: data = spark . read . format ( \"avro\" ) . load ( \"/tmp/test_dataset\" ) display ( data ) Write: % scala val ex = Seq (( 132 , \"baseball\" ), ( 148 , \"softball\" ), ( 172 , \"slow pitch\" )). toDF ( \"players\" , \"sport\" ) ex . write . format ( \"avro\" ). save ( \"/tmp/testExample\" )","title":"Avro"},{"location":"BestPractices/#csv","text":"It is fine to use with marginally smaller datasets as CSV files do not load well when the file size is very large. But with smaller data sets, it is simple and human-readable. For writing within a CSV file, it is also good to note that you are able to edit the file with Office. Read: % python data = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/tmp/test_dataCSV.csv' ) display ( data )","title":"CSV"},{"location":"BestPractices/#excel","text":"Please see above on how to use Excel. The other formats above are perferable over excel.","title":"Excel"},{"location":"BestPractices/#how-to-convert-files-csv-text-json-to-parquet-using-databricks","text":"The rule of thumb in converting a file to parquet is to first read in the file and then write a new file into parquet CSV to Parquet: % python testConvert = spark . read . format ( 'csv' ) . options ( header = 'true' , inferSchema = 'true' ) . load ( '/mnt/public-data/incoming/titanic.csv' ) testConvert . write . parquet ( \"/mnt/public-data/incoming/testingFile\" ) JSON to Parquet: % python testConvert = spark . read . json ( 'tmp/test.json' ) testConvert . write . parquet ( 'tmp/testingJson' ) Text to Parquet: % python testConvert = spark . read . text ( \"/mnt/public-data/incoming/testing.txt\" ) testConvert . write . parquet ( \"/mnt/public-data/incoming/testingFile\" )","title":"How to convert files (CSV, Text, JSON) to parquet using databricks?"},{"location":"BestPractices/#can-i-read-word-document-in-databricks","text":"It is best practice to read Word documents via Office instead.","title":"Can I read Word document in Databricks?"},{"location":"BestPractices/#when-should-we-use-adf-vs-databricks-for-data-ingestion","text":"Databricks is able to do real-time streaming through the Apache Spark API that can handle the streaming analytics workloads. Databricks does not need you to wrap the python code into functions or executable modules, all the code is able to work just as is. Databricks also supports Machine Learning which makes data ingestion easier as well. For any code that is already in an Azure Function or is easily translated into an executable, using data factory is usable. Data factory is also good to use if it is a heavy algorithm that is not usable within Databricks.","title":"When should we use ADF vs. Databricks for data ingestion?"},{"location":"BestPractices/#what-is-the-difference-between-sql-database-temporal-tables-and-delta-lake","text":"SQL temporal tables is specific to SQL 2018 and is not currently available in Azure Synapse. On the other hand, Delta lake is available in both Azure Synapse and in Databricks. Another difference is that SQL temporal tables are only available with only SQL queries while Delta lake time travel is available in Scala, Python, and SQL.","title":"What is the difference between SQL database temporal tables and Delta Lake?"},{"location":"BestPractices/#when-to-use-power-bi-or-r-shiny","text":"It is recommended to use Power BI over R-shiny because less coding is required when using Power BI. There are a lot of benefits to using Power BI including the additional amount of chart types that are at hand, visualisation of data into charts is easier to use in Power BI compared to R-Shiny, the creation of a dashboard is faster within PowerBI, and the ease of connectivity with other applications within Azure.","title":"When to use Power BI or R-Shiny?"},{"location":"BestPractices/#when-is-a-good-time-to-use-azure-synapse-vs-adf-and-databricks","text":"Azure Synapse is good to use when doing queries and data analysis via the data lake, doing SQL analyses and data warehousing, and using additional services like Power BI. It is easy to query data from the data lake using Azure Snapse and you do not have to mount the data lake to the workspace. As for data analyses and data warehousing, synapse is perferred as it allows full realtional data models, provide all SQL features and also uses Delta Lake. Synapse also includes direct services with Power BI for ease of use. On the other hand, Databricks is preferred when doing machine learning development and real-time transformations. Databricks includes their own machine learning development that includes popular libraries like PyTorch, manage version of MLflow. Databricks is also preferred for real-time transformations as it uses Spark structured streaming and it gives you the ability to view changes from other users in real time.","title":"When is a good time to use Azure Synapse vs. ADF and Databricks?"},{"location":"BestPractices/#when-should-we-use-a-sql-database-data-warehouse-vs-delta-lake","text":"Best practice would be to use Delta lake over SQL server as it does not use additional SQL compute resouces and will reduce the overall cloud costs.","title":"When should we use a SQL database data warehouse vs. Delta Lake?"},{"location":"BestPractices/#how-can-i-easily-convert-sas-files-to-another-format","text":"Statcan users can use SAS on the internal stats-can network to convert it to a supported file form. You are able to convert a SAS file to CSV or JSON with this method: First open databricks and install the sas7bdat-converter within your notebook. python %pip install sas7bdat-converter Using python and your code editor of your choice, type in this code with the file directory that the file is in and the directory where you want the output file to be in. ```python %python import sas7bdat_converter file_dicts = [{ 'sas7bdat_file': '/dbfs/mnt/public-data/ToNetA/sas7bdat/tablea_1_10k.sas7bdat', 'export_file': '/dbfs/mnt/public-data/testFolder/testingConvert.csv', }] sas7bdat_converter.batch_to_csv(file_dicts) ``` You will then get the output file within the directory you have specified. Converter Documentation: sas7bdat website documentation","title":"How can i easily convert SAS files to another format?"},{"location":"BestPractices/#canhow-i-convert-a-word-document-to-a-notebook","text":"There is no easy way to convert a word document to a notebook. A manual solution to convert a Word document to a notebook is by copying any of the code that is within the word document into a notebook.","title":"Can\\How I convert a Word document to a notebook?"},{"location":"BestPractices/#how-big-of-a-dataframespark-table-can-we-store-within-the-workspace","text":"Spark tables are stored as parquet files and are stored in the internal storage account linked with the Databricks workspace, but it is best practice to delete the table if it is no longer in use.","title":"How big of a dataframe/spark table can we store within the workspace?"},{"location":"BestPractices/#what-is-the-best-way-to-get-data-files-into-azure-ml","text":"The best way would be to upload your files to the data lake. If you need to add a new cloud storage account, contact the CAE team to add the storage account to the Azure ML studio.","title":"What is the best way to get data files into Azure ML?"},{"location":"BestPractices/#whats-the-difference-to-machine-learning-in-databricks-or-in-azure-ml","text":"The main difference between Azure ML and Databricks is the language that each application uses. Azure ML utilizes python-based libraries or R while Databricks utilizes the Apache Spark Platform and MLFlow. Azure ML also contains a tracking system which is able to track individual runs of the experiment and include the specific metrics of what wants to be seen. Databricks includes MLflow which also allows tracking but does not come with as many features as Azure ML. As a recommendation, it is best practice to use Databricks for data preperation and for large datasets but to use Azure ML for their tracking system, machine learning on normal datasets, deep learning on GPUs, and operationalization.","title":"Whats the difference to Machine Learning in Databricks or in Azure ML?"},{"location":"BestPractices/#how-do-you-create-a-table-in-databricks","text":"","title":"How do you create a Table in Databricks?"},{"location":"BestPractices/#option-1-use-create-table-function","text":"In Databricks, select Data and within the Database you have selected, click on Create Table. For more information about creating tables in Databricks: Databricks Create a Table .","title":"Option 1: Use Create Table function"},{"location":"BestPractices/#option-2-create-table-from-dataframe-table","text":"Python: df . write . saveAsTable ( \"Table-Name\" ) SQL: CREATE TABLE IF NOT EXISTS Table - Name AS SELECT * FROM df","title":"Option 2: Create Table from Dataframe table"},{"location":"BestPractices/#option-3-create-table-programatically","text":"SQL: CREATE TABLE example (id INT, name STRING, age INT) USING CSV;","title":"Option 3: Create Table Programatically"},{"location":"BestPractices/#when-to-use-spark-dataframe-or-spark-table","text":"There are really no difference between using a Spark Dataframe or Spark Table. Currently with Databricks, best practice right now would be to store tables as delta tables as it is saved in parquet format and gives the tracking capabilities.","title":"When to use Spark Dataframe or Spark Table?"},{"location":"BestPractices/#what-should-i-do-if-the-size-of-the-broadcasted-table-far-exceeds-estimates-and-exceeds-limit-of-sparkdrivermaxresultsize-_____","text":"Change the Spark configuration \"spark.driver.maxResultSize\" to \"0\" (means no limit) or something larger than your needs.","title":"What should I do if the size of the broadcasted table far exceeds estimates and exceeds limit of spark.driver.maxResultSize = _____?"},{"location":"BestPractices/#what-should-i-do-if-i-cannot-broadcast-the-table-that-is-larger-than-8gb","text":"This occurs only with BroadcastHashJoin. There are 2 options: Change the Spark configuration \"spark.sql.autoBroadcastJoinThreshold\" to \"-1\". This forces Databricks to perform a SortMergeJoin.","title":"What Should I do if I cannot broadcast the table that is larger than 8GB?"},{"location":"BestPractices/#note-about-changing-spark-configuration","text":"Warning: Changing Spark configuraitons can cause out-of-memory-errors Normal approach: - spark.conf.set(\"configuration\", \"value\") If you do not have permissions to change some configurations, this seems to be a work around: - conf = spark.sparkContext._cibf,setAkk([(\"configuration\", \"value\"), (\"configuration\", \"value\")]) How to get Spark Configuration: - spark.conf.get(\"configuration\") Steps to avoid changing configurations: a. Partition DataFrame A into parts. b. Perform joins with each parittion from DataFrame A with DataFrame B (concurrently is the fastest way but may require writing Dataframes to file for reading in next step). c. Perform a union on all the joined DataFrames.","title":"Note About changing Spark Configuration"},{"location":"ContactUs/","text":"Technical Support Subscribe to the following slack channel: cae-eac.slack.com Feedback Use the Feedback form on this website: Feedback Form Documentation Statistics Canada - Collaborative Analytics Environment (CAE) Frequently Asked Questions (FAQ) Best Practices Microsoft Documentation Azure documentation","title":"Contact Us"},{"location":"ContactUs/#technical-support","text":"Subscribe to the following slack channel: cae-eac.slack.com","title":"Technical Support"},{"location":"ContactUs/#feedback","text":"Use the Feedback form on this website: Feedback Form","title":"Feedback"},{"location":"ContactUs/#documentation","text":"Statistics Canada - Collaborative Analytics Environment (CAE) Frequently Asked Questions (FAQ) Best Practices","title":"Documentation"},{"location":"ContactUs/#microsoft-documentation","text":"Azure documentation","title":"Microsoft Documentation"},{"location":"Dashboards/","text":"Dashboards are a focused and organized view of your cloud resources in the Azure portal. They serve as a workspace where you can quickly launch tasks for day-to-day operations and monitor resources. Build custom dashboards based on projects, tasks, or user roles, for example. The Azure portal provides a default dashboard as a starting point. You can edit the default dashboard. Create and customize additional dashboards, and publish and share dashboards to make them available to other users. Access the Collaborative Analytics Environment Dashboard From the Azure portal menu, select Dashboard . Your default view might already be set to dashboard. Select the arrow next to the dashboard name. Select the Collaborative Analytics Environment dashboard from the displayed list of dashboards. If this dashboard isn't listed: a. Select Browse all dashboards . b. In the Type field, select Shared dashboards . c. Ensure the list of selected subsciptions includes the vdl subscription. You can also enter text to filter dashboards by name. d. Select the Collaborative Analytics Environment dashboard from the list of shared dashboards. Microsoft Documentation Create a dashboard in the Azure portal","title":"Dashboards"},{"location":"Dashboards/#access-the-collaborative-analytics-environment-dashboard","text":"From the Azure portal menu, select Dashboard . Your default view might already be set to dashboard. Select the arrow next to the dashboard name. Select the Collaborative Analytics Environment dashboard from the displayed list of dashboards. If this dashboard isn't listed: a. Select Browse all dashboards . b. In the Type field, select Shared dashboards . c. Ensure the list of selected subsciptions includes the vdl subscription. You can also enter text to filter dashboards by name. d. Select the Collaborative Analytics Environment dashboard from the list of shared dashboards.","title":"Access the Collaborative Analytics Environment Dashboard"},{"location":"Dashboards/#microsoft-documentation","text":"Create a dashboard in the Azure portal","title":"Microsoft Documentation"},{"location":"DataBricks/","text":"Accessing Databricks Dashboard See the Dashboard section of this documentation from more information. 1. Click on the Dashboard menu from the Azure Portal. ! [ Dashboard ] ( images / DataFactoryDashboard . png ) Databricks URL Navigate to https://canadacentral.azuredatabricks.net/, sign in with your cloud account credentials, and select the Databricks workspace that was created for you. Azure Portal In the Azure Portal Search box, search for Databricks . You should then see a list of the Databricks workspaces you were given permission to access. Getting Started Once inside Databricks you can create a new notebook or open an existing notebook. See First Access to Databricks for more information. Creating a Cluster Since you do not have permission to create a cluster, please send a slack message if a cluster has not been created for you or if you require changes to your cluster. Note: You must have a cluster running before you can run code inside your notebook. See below or the FAQ for information on how to start a cluster. Creating a Notebook One way to create a notebook is to click on the New Notebook link from the main Databricks page. You can then provide a name for your notebook and select the default notebook language. From the available list of clusters, select the cluster to which you wish to attach your notebook. To start or change a cluster from within a notebook, open the notebook and click on the cluster drop down found at the top right of the notebook. You can then start the cluster or detach it and attach a different one. Sharing a Databricks Notebook To share a notebook or invite other collaborators, right-click on a specific notebook file or folder from the Workspace menu, and select Permissions . You can also do this by clicking on the Permissions button from within a notebook. Once shared, multiple authors can participate in the same notebook session and co-author at the same time. Note: To add a user to the Databricks workspace, please send a Slack message. Ingesting Data into Databricks Data can be mounted or uploaded to the Databricks File System (DBFS), which is storage specific to the Databricks workspace. You can read data from a data source or even upload a data file (e.g. CSV) directly to the DBFS. Note: The internal data lake container for your environment has already been mounted for you and you can work with the container directly. Please send a Slack message if you don't know the name of your mounted data lake container. Adding Data to Databricks Reading Mounted Files Example: %python testData = spark . read . format ( ' csv ' ). options ( header = ' true ' , inferSchema = ' true ' ). load ( ' / mnt / mad - du / incoming / age - single - years -2018 - census - csv . csv ' ) display ( testData ) Changing Notebook Default Language Mixing Notebook Languages You can override the default language by specifying the language magic command % at the beginning of a cell. The supported magic commands are: %python, %r, %scala, and %sql. Note: When you invoke a language magic command, the command is dispatched to the REPL in the execution context for the notebook. Variables defined in one language (and hence in the REPL for that language) are not available in the REPL of another language. REPLs can share state only through external resources such as files in DBFS or objects in object storage. Notebooks also support a few auxiliary magic commands: %sh: Allows you to run shell code in your notebook. To fail the cell if the shell command has a non-zero exit status, add the -e option. This command runs only on the Apache Spark driver, and not the workers. To run a shell command on all nodes, use an init script. %fs: Allows you to use dbutils filesystem commands. %md: Allows you to include various types of documentation, including text, images, and mathematical formulas and equations. Starting a Databricks Cluster Click on the cluster drop-down list. Select a cluster from the list. Click on the Start Cluster link. Databricks Connect VM Setup Databricks connect is a method for accessing a Databricks environment without having to connect through the Azure Portal or the Databricks UI. It allows you to use other IDEs to work on Databricks code. The following are the steps for installing and testing Databricks Connect on your virtual machine (VM): Databricks Connect conflicts with the Pyspark installation found on the Data Science Virtual Machine images. The default path for this Pyspark installation is C:\\dsvm\\tools\\spark-2.4.4-bin-hadoop2.7 . Either delete or move this folder in order to install Databricks Connect. Before installing Databricks Connect, create a conda environment. To do this, open a command prompt and run the following commands: conda create -- name dbconnect python = 3 . 7 conda activate dbconnect type pip install - U databricks - connect == X . Y . * NOTE: Replace X and Y with the version number of the Databricks cluster. To find this value, open the Databricks workspace from the Azure portal, click on Clusters on the left of the page, and note the Runtime version for your cluster. In a command prompt, type databricks-connect configure , then enter the following values when prompted: Databricks Host: https://canadacentral.azuredatabricks.net Databricks Token: a personal access token generated in your Databricks Workspace User Settings Cluster ID: the value found under Cluster --> Advanced Options --> Tags in your Databricks workspace. Org ID: the part of the Databricks URL found after .net/?o= Port: keep the existing value Change the SPARK_HOME enviroment variable to c:\\miniconda\\envs\\(conda env name))\\lib\\site-packages\\pyspark , and restart your VM. (Please ask for help via a Slack message if you do not know how to change environment variables.) Test the connectivity to Azure Databricks by running databricks-connect test in a command prompt. If your Databricks cluster is not running when you start this test you will receive warning messages until it has started, which can take some time. Troubleshooting : If you are using databricks connect on windows and you get an error saying: Cannot find winutils.exe please refer to the Databricks documentation on the error . Installing Libraries Databricks Cluster Please contact the slack channel to have the support team install these libraries for you. Notebook Use the following commands to install a library in a notebook session: Python: dbutils . library . installPyPI ( \"pypipackage\" , version = \"version\" , repo = \"repo\" , extras = \"extras\" ) dbutils . library . restartPython () # Removes Python state, but some libraries might not work without calling this function R Code: install.packages ( \"library\" ) Microsoft Documentation First Access to Databricks For more information on Databricks Databricks Connect Install Libraries in Current Notebook Session Library Management for Admins","title":"Azure Databricks"},{"location":"DataBricks/#accessing-databricks","text":"","title":"Accessing Databricks"},{"location":"DataBricks/#dashboard","text":"See the Dashboard section of this documentation from more information. 1. Click on the Dashboard menu from the Azure Portal. ! [ Dashboard ] ( images / DataFactoryDashboard . png )","title":"Dashboard"},{"location":"DataBricks/#databricks-url","text":"Navigate to https://canadacentral.azuredatabricks.net/, sign in with your cloud account credentials, and select the Databricks workspace that was created for you.","title":"Databricks URL"},{"location":"DataBricks/#azure-portal","text":"In the Azure Portal Search box, search for Databricks . You should then see a list of the Databricks workspaces you were given permission to access.","title":"Azure Portal"},{"location":"DataBricks/#getting-started","text":"Once inside Databricks you can create a new notebook or open an existing notebook. See First Access to Databricks for more information.","title":"Getting Started"},{"location":"DataBricks/#creating-a-cluster","text":"Since you do not have permission to create a cluster, please send a slack message if a cluster has not been created for you or if you require changes to your cluster. Note: You must have a cluster running before you can run code inside your notebook. See below or the FAQ for information on how to start a cluster.","title":"Creating a Cluster"},{"location":"DataBricks/#creating-a-notebook","text":"One way to create a notebook is to click on the New Notebook link from the main Databricks page. You can then provide a name for your notebook and select the default notebook language. From the available list of clusters, select the cluster to which you wish to attach your notebook. To start or change a cluster from within a notebook, open the notebook and click on the cluster drop down found at the top right of the notebook. You can then start the cluster or detach it and attach a different one.","title":"Creating a Notebook"},{"location":"DataBricks/#sharing-a-databricks-notebook","text":"To share a notebook or invite other collaborators, right-click on a specific notebook file or folder from the Workspace menu, and select Permissions . You can also do this by clicking on the Permissions button from within a notebook. Once shared, multiple authors can participate in the same notebook session and co-author at the same time. Note: To add a user to the Databricks workspace, please send a Slack message.","title":"Sharing a Databricks Notebook"},{"location":"DataBricks/#ingesting-data-into-databricks","text":"Data can be mounted or uploaded to the Databricks File System (DBFS), which is storage specific to the Databricks workspace. You can read data from a data source or even upload a data file (e.g. CSV) directly to the DBFS. Note: The internal data lake container for your environment has already been mounted for you and you can work with the container directly. Please send a Slack message if you don't know the name of your mounted data lake container.","title":"Ingesting Data into Databricks"},{"location":"DataBricks/#adding-data-to-databricks","text":"","title":"Adding Data to Databricks"},{"location":"DataBricks/#reading-mounted-files","text":"Example: %python testData = spark . read . format ( ' csv ' ). options ( header = ' true ' , inferSchema = ' true ' ). load ( ' / mnt / mad - du / incoming / age - single - years -2018 - census - csv . csv ' ) display ( testData )","title":"Reading Mounted Files"},{"location":"DataBricks/#changing-notebook-default-language","text":"","title":"Changing Notebook Default Language"},{"location":"DataBricks/#mixing-notebook-languages","text":"You can override the default language by specifying the language magic command % at the beginning of a cell. The supported magic commands are: %python, %r, %scala, and %sql. Note: When you invoke a language magic command, the command is dispatched to the REPL in the execution context for the notebook. Variables defined in one language (and hence in the REPL for that language) are not available in the REPL of another language. REPLs can share state only through external resources such as files in DBFS or objects in object storage. Notebooks also support a few auxiliary magic commands: %sh: Allows you to run shell code in your notebook. To fail the cell if the shell command has a non-zero exit status, add the -e option. This command runs only on the Apache Spark driver, and not the workers. To run a shell command on all nodes, use an init script. %fs: Allows you to use dbutils filesystem commands. %md: Allows you to include various types of documentation, including text, images, and mathematical formulas and equations.","title":"Mixing Notebook Languages"},{"location":"DataBricks/#starting-a-databricks-cluster","text":"Click on the cluster drop-down list. Select a cluster from the list. Click on the Start Cluster link.","title":"Starting a Databricks Cluster"},{"location":"DataBricks/#databricks-connect-vm-setup","text":"Databricks connect is a method for accessing a Databricks environment without having to connect through the Azure Portal or the Databricks UI. It allows you to use other IDEs to work on Databricks code. The following are the steps for installing and testing Databricks Connect on your virtual machine (VM): Databricks Connect conflicts with the Pyspark installation found on the Data Science Virtual Machine images. The default path for this Pyspark installation is C:\\dsvm\\tools\\spark-2.4.4-bin-hadoop2.7 . Either delete or move this folder in order to install Databricks Connect. Before installing Databricks Connect, create a conda environment. To do this, open a command prompt and run the following commands: conda create -- name dbconnect python = 3 . 7 conda activate dbconnect type pip install - U databricks - connect == X . Y . * NOTE: Replace X and Y with the version number of the Databricks cluster. To find this value, open the Databricks workspace from the Azure portal, click on Clusters on the left of the page, and note the Runtime version for your cluster. In a command prompt, type databricks-connect configure , then enter the following values when prompted: Databricks Host: https://canadacentral.azuredatabricks.net Databricks Token: a personal access token generated in your Databricks Workspace User Settings Cluster ID: the value found under Cluster --> Advanced Options --> Tags in your Databricks workspace. Org ID: the part of the Databricks URL found after .net/?o= Port: keep the existing value Change the SPARK_HOME enviroment variable to c:\\miniconda\\envs\\(conda env name))\\lib\\site-packages\\pyspark , and restart your VM. (Please ask for help via a Slack message if you do not know how to change environment variables.) Test the connectivity to Azure Databricks by running databricks-connect test in a command prompt. If your Databricks cluster is not running when you start this test you will receive warning messages until it has started, which can take some time.","title":"Databricks Connect VM Setup"},{"location":"DataBricks/#troubleshooting","text":"If you are using databricks connect on windows and you get an error saying: Cannot find winutils.exe please refer to the Databricks documentation on the error .","title":"Troubleshooting :"},{"location":"DataBricks/#installing-libraries","text":"","title":"Installing Libraries"},{"location":"DataBricks/#databricks-cluster","text":"Please contact the slack channel to have the support team install these libraries for you.","title":"Databricks Cluster"},{"location":"DataBricks/#notebook","text":"Use the following commands to install a library in a notebook session: Python: dbutils . library . installPyPI ( \"pypipackage\" , version = \"version\" , repo = \"repo\" , extras = \"extras\" ) dbutils . library . restartPython () # Removes Python state, but some libraries might not work without calling this function R Code: install.packages ( \"library\" )","title":"Notebook"},{"location":"DataBricks/#microsoft-documentation","text":"First Access to Databricks For more information on Databricks Databricks Connect Install Libraries in Current Notebook Session Library Management for Admins","title":"Microsoft Documentation"},{"location":"DataFactory/","text":"Access Data Factory Dashboard See the Dashboard section of this documentation from more information. Click on the Dashboard menu from the Azure Portal. ADF URL Navigate to https://adf.azure.com, and select the Data Factory instance that was created for you. Azure Portal In the Azure Portal Search box, search for Data factories . You should then see a list of the Data Factories you were given permission to access. Authoring Click on Author and Monitor . In Data Factory, you have the ability to author and deploy resources. See Visual authoring in Azure Data Factory for more information. You can also use some of the various wizards provided on Data Factory Overview page. NOTE: Configuring SSIS Integration is NOT recommended. Contact the support team through the Slack channel if you have questions. See Azure Documentation Tutorials for more details. Access the Data Lake from ADF A Data Lake connection has been pre-configured for your environment. Click on Manage . Click on Linked Services . The linked service with the Azure Data Lake Storage Gen2 type is your Data Lake . Note: You have been granted access to specific containers created in the Data Lake for your environment. Access Azure SQL Database Some projects have an Azure SQL Database instance. Click on Manage . Click on Linked Services . The linked service(s) with the Azure SQL Database type is / are your Database(s) Save / Publish Your Data Factory Resources Azure Data Factory can be configured to save your work to the following locations: - Git repository - Publish directly to Data Factory Git (when supported) When Git is enabled you can see your configuration and save your work to a specific branch. Click on Manage Click on Git Configuration . See the Git configuration that was setup for you: When authoring a workflow it can be saved to your branch. Click on + New branch from this branch dropdown to create a new feature branch. When you are ready to merge the changes from your feature branch to your collaboration branch (master), click on the branch dropdown and select Create pull request . This action takes you to Azure DevOps Git Repo where you can create pull requests, do code reviews, and merge changes to your collaboration branch (master) after the pull request has been approved. After you have merged changes to the collaboration branch (master), click on Publish to publish your code changes from the master branch to Azure Data Factory. Contact the support team through the Slack channel if you receive an error when trying to Publish. Data Factory Service When Data Factory is not integrated with source control your workflows are stored directly in the Data Factory service and you cannot save partial changes, you can only Publish all which overwrites the current state of the Data Factory with your changes, which are then visible to everyone. Ingest and Transform Data with ADF Copy Data Wizard documentation link Mapping Data Flows \u2013 GUI-driven ETL documentation link Integration Runtimes AutoResolveIntegrationRuntime Do not use. Please use the canadaCentralIR-4nodesDataFlow or selfHostedCovidIaaSVnet runtimes instead. The auto resolve runtime is created by default with the data factory instance, and will auto resolve to the Azure Data Centre closest to the data, which may violate data residency policies. canadaCentralIR-4nodesDataFlow This is shared by all users and runs all the time. Can Access: Internal Data Lake External Storage Account External Data Sources (Internet) Cannot Access: Azure SQL Database selfHostedCovidIaaSVnet Located inside CAE virtual network (VNet). Can Access: Internal Data Lake SQL Server Cannot Access: External Storage Account External Data Sources (Internet) Example: How to connect John Hopkins Data There is an example workflow that shows how to ingest data from GitHub using a Data Factory Pipeline. Data can be filtered from within Data Factory. Alternatively, data can be pulled from GitHub using code in a Databricks notebook. Microsoft Documentation Introduction to Azure Data Factory - Azure Data Factory Create an Azure data factory using the Azure Data Factory UI - Azure Data Factory Copy data by using the Azure Copy Data tool - Azure Data Factory Create a mapping data flow - Azure Data Factory Expression functions in the mapping data flow - Azure Data Factory Mapping data flow Debug Mode - Azure Data Factory Mapping data flow Visual Monitoring - Azure Data Factory YouTube Videos Ingest, prepare & transform using Azure Databricks & Data Factory | Azure Friday Azure Friday | Visually build pipelines for Azure Data Factory V2 How to prepare data using wrangling data flows in Azure Data Factory | Azure Friday How to develop and debug with Azure Data Factory | Azure Friday Building Data Flows in Azure Data Factory","title":"Azure Data Factory"},{"location":"DataFactory/#access-data-factory","text":"","title":"Access Data Factory"},{"location":"DataFactory/#dashboard","text":"See the Dashboard section of this documentation from more information. Click on the Dashboard menu from the Azure Portal.","title":"Dashboard"},{"location":"DataFactory/#adf-url","text":"Navigate to https://adf.azure.com, and select the Data Factory instance that was created for you.","title":"ADF URL"},{"location":"DataFactory/#azure-portal","text":"In the Azure Portal Search box, search for Data factories . You should then see a list of the Data Factories you were given permission to access.","title":"Azure Portal"},{"location":"DataFactory/#authoring","text":"Click on Author and Monitor . In Data Factory, you have the ability to author and deploy resources. See Visual authoring in Azure Data Factory for more information. You can also use some of the various wizards provided on Data Factory Overview page. NOTE: Configuring SSIS Integration is NOT recommended. Contact the support team through the Slack channel if you have questions. See Azure Documentation Tutorials for more details.","title":"Authoring"},{"location":"DataFactory/#access-the-data-lake-from-adf","text":"A Data Lake connection has been pre-configured for your environment. Click on Manage . Click on Linked Services . The linked service with the Azure Data Lake Storage Gen2 type is your Data Lake . Note: You have been granted access to specific containers created in the Data Lake for your environment.","title":"Access the Data Lake from ADF"},{"location":"DataFactory/#access-azure-sql-database","text":"Some projects have an Azure SQL Database instance. Click on Manage . Click on Linked Services . The linked service(s) with the Azure SQL Database type is / are your Database(s)","title":"Access Azure SQL Database"},{"location":"DataFactory/#save-publish-your-data-factory-resources","text":"Azure Data Factory can be configured to save your work to the following locations: - Git repository - Publish directly to Data Factory","title":"Save / Publish Your Data Factory Resources"},{"location":"DataFactory/#git-when-supported","text":"When Git is enabled you can see your configuration and save your work to a specific branch. Click on Manage Click on Git Configuration . See the Git configuration that was setup for you: When authoring a workflow it can be saved to your branch. Click on + New branch from this branch dropdown to create a new feature branch. When you are ready to merge the changes from your feature branch to your collaboration branch (master), click on the branch dropdown and select Create pull request . This action takes you to Azure DevOps Git Repo where you can create pull requests, do code reviews, and merge changes to your collaboration branch (master) after the pull request has been approved. After you have merged changes to the collaboration branch (master), click on Publish to publish your code changes from the master branch to Azure Data Factory. Contact the support team through the Slack channel if you receive an error when trying to Publish.","title":"Git (when supported)"},{"location":"DataFactory/#data-factory-service","text":"When Data Factory is not integrated with source control your workflows are stored directly in the Data Factory service and you cannot save partial changes, you can only Publish all which overwrites the current state of the Data Factory with your changes, which are then visible to everyone.","title":"Data Factory Service"},{"location":"DataFactory/#ingest-and-transform-data-with-adf","text":"Copy Data Wizard documentation link Mapping Data Flows \u2013 GUI-driven ETL documentation link","title":"Ingest and Transform Data with ADF"},{"location":"DataFactory/#integration-runtimes","text":"","title":"Integration Runtimes"},{"location":"DataFactory/#autoresolveintegrationruntime","text":"Do not use. Please use the canadaCentralIR-4nodesDataFlow or selfHostedCovidIaaSVnet runtimes instead. The auto resolve runtime is created by default with the data factory instance, and will auto resolve to the Azure Data Centre closest to the data, which may violate data residency policies.","title":"AutoResolveIntegrationRuntime"},{"location":"DataFactory/#canadacentralir-4nodesdataflow","text":"This is shared by all users and runs all the time.","title":"canadaCentralIR-4nodesDataFlow"},{"location":"DataFactory/#can-access","text":"Internal Data Lake External Storage Account External Data Sources (Internet)","title":"Can Access:"},{"location":"DataFactory/#cannot-access","text":"Azure SQL Database","title":"Cannot Access:"},{"location":"DataFactory/#selfhostedcovidiaasvnet","text":"Located inside CAE virtual network (VNet).","title":"selfHostedCovidIaaSVnet"},{"location":"DataFactory/#can-access_1","text":"Internal Data Lake SQL Server","title":"Can Access:"},{"location":"DataFactory/#cannot-access_1","text":"External Storage Account External Data Sources (Internet)","title":"Cannot Access:"},{"location":"DataFactory/#example-how-to-connect-john-hopkins-data","text":"There is an example workflow that shows how to ingest data from GitHub using a Data Factory Pipeline. Data can be filtered from within Data Factory. Alternatively, data can be pulled from GitHub using code in a Databricks notebook.","title":"Example: How to connect John Hopkins Data"},{"location":"DataFactory/#microsoft-documentation","text":"Introduction to Azure Data Factory - Azure Data Factory Create an Azure data factory using the Azure Data Factory UI - Azure Data Factory Copy data by using the Azure Copy Data tool - Azure Data Factory Create a mapping data flow - Azure Data Factory Expression functions in the mapping data flow - Azure Data Factory Mapping data flow Debug Mode - Azure Data Factory Mapping data flow Visual Monitoring - Azure Data Factory","title":"Microsoft Documentation"},{"location":"DataFactory/#youtube-videos","text":"Ingest, prepare & transform using Azure Databricks & Data Factory | Azure Friday Azure Friday | Visually build pipelines for Azure Data Factory V2 How to prepare data using wrangling data flows in Azure Data Factory | Azure Friday How to develop and debug with Azure Data Factory | Azure Friday Building Data Flows in Azure Data Factory","title":"YouTube Videos"},{"location":"DeltaLake/","text":"Delta Lake is an open-source storage layer that runs on top of an existing data lake, adding the capabilities of ACID (atomicity, consistency, isolation, durability) properties and transactions. Delta Lake is fully compatible with Apache Spark in Azure Databricks and Synapse. Azure Data Lake is not ACID compliant, so Delta Lake should be used wherever data integrity and reliability are essential, or when there is a risk of bad data. Microsoft Documentation What is Delta Lake Delta Lake on Azure Official Documentation Delta Lake Documentation How Delta Lake Works A delta lake is basically a folder inside the data lake containing log files (in the sub-folder _delta_log ) and data files (stored in parquet format in the root folder) for each version of a table. As long as the log and data files exist, you can use the time travel feature to query previous versions of a delta table, and view the history of that table. If the log files are deleted , you will not be able to read the table at all. To fix this, you will need to empty the delta lake folder (delete everything in it), then write your original data file to it to start over. Delta works at the table level, so multi-table queries and joins are not supported. When to Use Delta Lake Delta lake is best to use: for any permanent table in Databricks; for large amounts of semi-structured data (10 million+ records to get the most performance benefits); or when you want version control and/or data access tracking (delta log files keep track of every time the data is modified and by whom). Time Travel You can use time travel to query an older snapshot of a table, either by version number or timestamp. By default, data files are stored for 30 days. Example: SQL SELECT * FROM example_table TIMESTAMP AS OF '2018-10-18T22:15:12.013Z' SELECT * FROM delta.`/delta/example_table` VERSION AS OF 12 Python df1 = spark . read . format ( \"delta\" ) . option ( \"timestampAsOf\" , 2020 - 03 - 13 ) . load ( \"/delta/example_table\" ) df2 = spark . read . format ( \"delta\" ) . option ( \"timestampAsOf\" , 2019 - 01 - 01 T00 : 00 : 00.000 Z ) . load ( \"/delta/example_table\" ) df3 = spark . read . format ( \"delta\" ) . option ( \"versionAsOf\" , version ) . load ( \"/delta/example_table\" ) Removing Old Data Files To remove old data files (not log files) that are no longer referenced by a delta table, you can run the vacuum command. Example: SQL VACUUM example_table -- vacuum files not required by versions older than the default retention period VACUUM '/data/example_table' -- vacuum files in path - based table VACUUM delta .` / data / example_table / ` VACUUM delta .` / data / example_table / ` RETAIN 100 HOURS -- vacuum files not required by versions more than 100 hours old VACUUM example_table DRY RUN -- do dry run to get the list of files to be deleted Python from delta.tables import * deltaTable = DeltaTable . forPath ( spark , pathToTable ) deltaTable . vacuum () # vacuum files not required by versions older than the default retention period deltaTable . vacuum ( 100 ) # vacuum files not required by versions more than 100 hours old Scala import io.delta.tables._ val deltaTable = DeltaTable . forPath ( spark , pathToTable ) deltaTable . vacuum () // vacuum files not required by versions older than the default retention period deltaTable . vacuum ( 100 ) // vacuum files not required by versions more than 100 hours old Revert Back to a Previous Version You can revert back to and work from a previous version of your table by using the time travel feature to read in your target version as a dataframe, then write it back to the delta lake folder. This will create a new version that is identical to the target version, which you can then work from. Other previous versions remain intact. Example: Python # read in old version of table df = spark . read . format ( \"delta\" ) . option ( \"versionAsOf\" , 0 ) . load ( delta_table_path ) # write back to new version of table, must set mode to \"overwrite\" df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( delta_table_path ) Official Documentation Query an older snapshot of a table (time travel) Remove files no longer referenced by a delta table Using Delta in Databricks Databricks has native support for Delta Lake, and can run queries using Python, R, Scala, and SQL. You first need to create a directory to store the delta files, and keep note of the path to this directory. Read in your data file, then write it to \"delta\" format and save it in the directory created above. ``` # read data file testData = spark.read.format('json').options(header='true', inferSchema='true', multiline='true').load('/mnt/public-data/incoming/covid_tracking.json') write to delta format testData.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/public-data/delta\") ``` Optional ( not a best practice ): Create an SQL table using delta: spark.sql(\"CREATE TABLE sample_table USING DELTA LOCATION '/mnt/public-data/delta/'\") Now you can run SQL queries on your delta table, including querying by version number or timestamp to \"time travel\" to previous versions of your data. If you created a table in step 3 , you can run queries using the table name. Otherwise (best practice) , in place of the table name you can use delta.`{delta_table_path}` (replace {delta_table_path} with the actual path). ``` %sql SELECT * FROM sample_table VERSION AS OF 0 SELECT * FROM delta. /mnt/public-data/delta/ ``` Microsoft Documentation Delta Lake Quickstart Using Delta in Azure Synapse Delta Lake is compatible with Azure Synapse. Delta tables can be created and queried within Synapse notebooks similarly to Databricks, with language support for PySpark, Scala, and .NET (C#). Note that SQL is not supported with the current version. Read in your data file. data = spark.read.format('csv').options(header='true', inferSchema='true', multiline='true').load('abfss://public-data@statsconviddsinternal.dfs.core.windows.net/incoming/data_duplicate.csv') Write to delta format and save to your delta table directory. data.write.format(\"delta\").save(delta_table_path) Optional: Create an SQL table using delta (only required if you want to run SQL queries, not necessary if you're only using Python, Scala, or C# ). spark.sql(\"CREATE TABLE example USING DELTA LOCATION '{0}'\".format(delta_table_path)) Now you can run queries on your data. Microsoft Documentation Work With Delta Lake Using Delta in Data Factory You can use Azure Data Factory to copy data to and from a delta lake stored in Azure Data Lake. Example: Copy Data to Delta Lake Create a new dataflow and add a source. Under the Source Settings tab, add the dataset that you want to copy from. Configure any other relevant settings. Click the plus button to the right of your source and add a sink. Under the Sink tab, choose Inline as the Sink type, and Delta as the Inline dataset type. Under the Settings tab, set the folder path (the path to where your delta files will be stored). Microsoft Documentation Delta Format in Azure Data Factory Using Delta with Power BI To read delta tables natively in Power BI, please see this documentation on GitHub . Delta in Azure Machine Learning Delta lake is not currently supported in Azure ML.","title":"Delta Lake"},{"location":"DeltaLake/#microsoft-documentation","text":"What is Delta Lake Delta Lake on Azure","title":"Microsoft Documentation"},{"location":"DeltaLake/#official-documentation","text":"Delta Lake Documentation","title":"Official Documentation"},{"location":"DeltaLake/#how-delta-lake-works","text":"A delta lake is basically a folder inside the data lake containing log files (in the sub-folder _delta_log ) and data files (stored in parquet format in the root folder) for each version of a table. As long as the log and data files exist, you can use the time travel feature to query previous versions of a delta table, and view the history of that table. If the log files are deleted , you will not be able to read the table at all. To fix this, you will need to empty the delta lake folder (delete everything in it), then write your original data file to it to start over. Delta works at the table level, so multi-table queries and joins are not supported.","title":"How Delta Lake Works"},{"location":"DeltaLake/#when-to-use-delta-lake","text":"Delta lake is best to use: for any permanent table in Databricks; for large amounts of semi-structured data (10 million+ records to get the most performance benefits); or when you want version control and/or data access tracking (delta log files keep track of every time the data is modified and by whom).","title":"When to Use Delta Lake"},{"location":"DeltaLake/#time-travel","text":"You can use time travel to query an older snapshot of a table, either by version number or timestamp. By default, data files are stored for 30 days. Example: SQL SELECT * FROM example_table TIMESTAMP AS OF '2018-10-18T22:15:12.013Z' SELECT * FROM delta.`/delta/example_table` VERSION AS OF 12 Python df1 = spark . read . format ( \"delta\" ) . option ( \"timestampAsOf\" , 2020 - 03 - 13 ) . load ( \"/delta/example_table\" ) df2 = spark . read . format ( \"delta\" ) . option ( \"timestampAsOf\" , 2019 - 01 - 01 T00 : 00 : 00.000 Z ) . load ( \"/delta/example_table\" ) df3 = spark . read . format ( \"delta\" ) . option ( \"versionAsOf\" , version ) . load ( \"/delta/example_table\" )","title":"Time Travel"},{"location":"DeltaLake/#removing-old-data-files","text":"To remove old data files (not log files) that are no longer referenced by a delta table, you can run the vacuum command. Example: SQL VACUUM example_table -- vacuum files not required by versions older than the default retention period VACUUM '/data/example_table' -- vacuum files in path - based table VACUUM delta .` / data / example_table / ` VACUUM delta .` / data / example_table / ` RETAIN 100 HOURS -- vacuum files not required by versions more than 100 hours old VACUUM example_table DRY RUN -- do dry run to get the list of files to be deleted Python from delta.tables import * deltaTable = DeltaTable . forPath ( spark , pathToTable ) deltaTable . vacuum () # vacuum files not required by versions older than the default retention period deltaTable . vacuum ( 100 ) # vacuum files not required by versions more than 100 hours old Scala import io.delta.tables._ val deltaTable = DeltaTable . forPath ( spark , pathToTable ) deltaTable . vacuum () // vacuum files not required by versions older than the default retention period deltaTable . vacuum ( 100 ) // vacuum files not required by versions more than 100 hours old","title":"Removing Old Data Files"},{"location":"DeltaLake/#revert-back-to-a-previous-version","text":"You can revert back to and work from a previous version of your table by using the time travel feature to read in your target version as a dataframe, then write it back to the delta lake folder. This will create a new version that is identical to the target version, which you can then work from. Other previous versions remain intact. Example: Python # read in old version of table df = spark . read . format ( \"delta\" ) . option ( \"versionAsOf\" , 0 ) . load ( delta_table_path ) # write back to new version of table, must set mode to \"overwrite\" df . write . format ( \"delta\" ) . mode ( \"overwrite\" ) . save ( delta_table_path )","title":"Revert Back to a Previous Version"},{"location":"DeltaLake/#official-documentation_1","text":"Query an older snapshot of a table (time travel) Remove files no longer referenced by a delta table","title":"Official Documentation"},{"location":"DeltaLake/#using-delta-in-databricks","text":"Databricks has native support for Delta Lake, and can run queries using Python, R, Scala, and SQL. You first need to create a directory to store the delta files, and keep note of the path to this directory. Read in your data file, then write it to \"delta\" format and save it in the directory created above. ``` # read data file testData = spark.read.format('json').options(header='true', inferSchema='true', multiline='true').load('/mnt/public-data/incoming/covid_tracking.json')","title":"Using Delta in Databricks"},{"location":"DeltaLake/#write-to-delta-format","text":"testData.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/public-data/delta\") ``` Optional ( not a best practice ): Create an SQL table using delta: spark.sql(\"CREATE TABLE sample_table USING DELTA LOCATION '/mnt/public-data/delta/'\") Now you can run SQL queries on your delta table, including querying by version number or timestamp to \"time travel\" to previous versions of your data. If you created a table in step 3 , you can run queries using the table name. Otherwise (best practice) , in place of the table name you can use delta.`{delta_table_path}` (replace {delta_table_path} with the actual path). ``` %sql SELECT * FROM sample_table VERSION AS OF 0 SELECT * FROM delta. /mnt/public-data/delta/ ```","title":"write to delta format"},{"location":"DeltaLake/#microsoft-documentation_1","text":"Delta Lake Quickstart","title":"Microsoft Documentation"},{"location":"DeltaLake/#using-delta-in-azure-synapse","text":"Delta Lake is compatible with Azure Synapse. Delta tables can be created and queried within Synapse notebooks similarly to Databricks, with language support for PySpark, Scala, and .NET (C#). Note that SQL is not supported with the current version. Read in your data file. data = spark.read.format('csv').options(header='true', inferSchema='true', multiline='true').load('abfss://public-data@statsconviddsinternal.dfs.core.windows.net/incoming/data_duplicate.csv') Write to delta format and save to your delta table directory. data.write.format(\"delta\").save(delta_table_path) Optional: Create an SQL table using delta (only required if you want to run SQL queries, not necessary if you're only using Python, Scala, or C# ). spark.sql(\"CREATE TABLE example USING DELTA LOCATION '{0}'\".format(delta_table_path)) Now you can run queries on your data.","title":"Using Delta in Azure Synapse"},{"location":"DeltaLake/#microsoft-documentation_2","text":"Work With Delta Lake","title":"Microsoft Documentation"},{"location":"DeltaLake/#using-delta-in-data-factory","text":"You can use Azure Data Factory to copy data to and from a delta lake stored in Azure Data Lake.","title":"Using Delta in Data Factory"},{"location":"DeltaLake/#example-copy-data-to-delta-lake","text":"Create a new dataflow and add a source. Under the Source Settings tab, add the dataset that you want to copy from. Configure any other relevant settings. Click the plus button to the right of your source and add a sink. Under the Sink tab, choose Inline as the Sink type, and Delta as the Inline dataset type. Under the Settings tab, set the folder path (the path to where your delta files will be stored).","title":"Example: Copy Data to Delta Lake"},{"location":"DeltaLake/#microsoft-documentation_3","text":"Delta Format in Azure Data Factory","title":"Microsoft Documentation"},{"location":"DeltaLake/#using-delta-with-power-bi","text":"To read delta tables natively in Power BI, please see this documentation on GitHub .","title":"Using Delta with Power BI"},{"location":"DeltaLake/#delta-in-azure-machine-learning","text":"Delta lake is not currently supported in Azure ML.","title":"Delta in Azure Machine Learning"},{"location":"FAQ/","text":"Data Ingestion How do I ingest data (including large files) into the platform? External Storage Account Files may be uploaded to the inbox or to-vers-int container of an external storage account, as documented in Azure Storage Explorer . These files will then automatically be moved into an internal storage account (Data Lake), and made accessible from authorized services. Note: The external storage accounts have the naming convention stats project-acronym external . Electronic File Transfer Service (EFT) Statistics Canada employees can use EFT to transfer files to/from on-premises (Net A or B) to/from the Azure cloud environment. Please contact the support team through the https://cae-eac.slack.com channel for informaton about this process. Platform Tools Platform tools such as Databricks or Data Factory may be used to ingest data from public data sources. Storage Explorer How do I configure Azure Storage Explorer proxy settings on a network B VDI? For Statistics Canada Employees only Proxy configuration is required if you receive the following error: In Azure Storage Explorer, go to Edit --> Proxy Settings . Enter the necessary proxy settings, and click on OK . How do I request a new SAS token (required for Azure Storage Explorer on a Network B VDI)? For Statistics Canada Employees only Please contact the support team through the https://cae-eac.slack.com channel to request a temporary SAS token. Why do I get an error message when accessing the internal Data Lake? The internal Data Lake is only accessible from within a VM in the Collaborative Analytics Environment (CAE). It is not accessible from your personal or work laptop, Network B VDI or other cloud VM. Source Code Control How do I link my Visual Studio Subscription to my StatCan cloud account? Login to https://visualstudio.microsoft.com/subscriptions/ with your organization's email address. For StatCan employees, this will be your canada.ca email address. Add your cloud account as an alternate account. This will allow you to use your licenses for Visual Studio & Azure DevOps in the CAE. For Statistics Canada employees: If you do not have a Visual Studio Subscription, please consult your supervisor. If they decide that a subscription is needed, they can then contact StatCan Software Asset Management by submitting an SRM to request a license on your behalf. Virtual Machines What do I do if I have forgotten the password for my virtual machine? If you forget the password of your virtual machine, please contact the support team through the https://cae-eac.slack.com channel to have it reset. You can also delete and recreate your virtual machine. Unfortunately, recreating your virtual machine means you will lose any data and software on your old machine. What do I do if I need to run a long running job on my virtual machine? Machines are shut down every day at 7pm EST in order to reduce costs. If you have a long-running job, it is recommended that you use Databricks or Data Factory. WARNING: Disabling Auto-shutdown is not recommended as it can incur significant costs. To disable Auto-shutdown: Navigate to your virtual machine in the Azure Portal. Disable Auto-shutdown. How do I request changes to my virtual machine? If the virtual machine you are currently using does not meet your requirements, please contact the support team through the https://cae-eac.slack.com channel. Databricks Why am I unable to run code from my Databricks notebook? You must first start a Databricks cluster that was previously created for you: Click on Clusters. Navigate to your cluster and click on the Start (arrow) button. What type of clusters are available in databricks? See the following link for the different types of available clusters: Databricks runtime releases Generally, LTS (long term support) clusters are supported and recommended by Databricks. If you need a different version of the databricks cluster, please contact the CAE support team. What happens when clusters are upgraded? LTS (long term support) have support for 1-2 years. They will need to be periodically updated to a newer version. When upgraded, all code should be re-run to ensure there are no issues when a cluster is updated How do you read an excel file with databricks/python? Here is an example of how to read in an excel file: % python import pandas as pd pd . read_excel ( \"/dbfs/mnt/ccei-ccie-ext/Daily charts.xlsx\" , engine = 'openyxl' ) Dashboard How do I change my subscription so that I can see my resources? In the Azure Portal click the Directory + subscription icon. Select the vdl subscription. CAE's SQL Server Deployment Options Azure SQL Database Azure SQL Database - Managed Instance SQL Server on Azure Virtual Machines Yes No No SQL Server Machine Learning Services Machine Learning Services is a feature in SQL Server that allows to run Python and R scripts and perform predictive analytics and machine learning with relational data. The scripts are executed in-database without the need to move data outside SQL Server and can use open-source packages and frameworks as well as Microsoft's enterprise packages. This feature is offered only offered in 2 of the 3 Azure Cloud SQL Server deployment options, SQL Server on Azure Virtual Machines and Azure SQL Database - Managed Instance. Presently, CAE has only Azure SQL Database that doesn't offer this feature. However, CAE users can experiment with SQL ML services on their datascience VM Azure SQL server (developer edition). Other How do I connect to the internal data lake with Power BI Desktop? Prerequisites: - A VM in the Collaborative Analytics Environment (CAE). - Power BI Desktop (available by default in the Windows Data Science Virtual Machine images.) Steps: 1. Login to your CAE VM. 2. Launch Power BI Desktop. 3. Follow the steps in Analyze data in Azure Data Lake Storage Gen2 by using Power BI - Create a report in Power BI Desktop . Please send a slack message to [Slack]https://cae-eac.slack.com if you don't know the Azure Data Lake Storage Gen2 URL. How can Statistics Canada employees transfer files from their data centre? For Statistics Canada Employees, they can refer to this internal documentation: Data Ingestion How do I add a FAQ? Please send your suggestion through the Slack channel.","title":"FAQ"},{"location":"FAQ/#data-ingestion","text":"","title":"Data Ingestion"},{"location":"FAQ/#how-do-i-ingest-data-including-large-files-into-the-platform","text":"","title":"How do I ingest data (including large files) into the platform?"},{"location":"FAQ/#external-storage-account","text":"Files may be uploaded to the inbox or to-vers-int container of an external storage account, as documented in Azure Storage Explorer . These files will then automatically be moved into an internal storage account (Data Lake), and made accessible from authorized services. Note: The external storage accounts have the naming convention stats project-acronym external .","title":"External Storage Account"},{"location":"FAQ/#electronic-file-transfer-service-eft","text":"Statistics Canada employees can use EFT to transfer files to/from on-premises (Net A or B) to/from the Azure cloud environment. Please contact the support team through the https://cae-eac.slack.com channel for informaton about this process.","title":"Electronic File Transfer Service (EFT)"},{"location":"FAQ/#platform-tools","text":"Platform tools such as Databricks or Data Factory may be used to ingest data from public data sources.","title":"Platform Tools"},{"location":"FAQ/#storage-explorer","text":"","title":"Storage Explorer"},{"location":"FAQ/#how-do-i-configure-azure-storage-explorer-proxy-settings-on-a-network-b-vdi","text":"For Statistics Canada Employees only Proxy configuration is required if you receive the following error: In Azure Storage Explorer, go to Edit --> Proxy Settings . Enter the necessary proxy settings, and click on OK .","title":"How do I configure Azure Storage Explorer proxy settings on a network B VDI?"},{"location":"FAQ/#how-do-i-request-a-new-sas-token-required-for-azure-storage-explorer-on-a-network-b-vdi","text":"For Statistics Canada Employees only Please contact the support team through the https://cae-eac.slack.com channel to request a temporary SAS token.","title":"How do I request a new SAS token (required for Azure Storage Explorer on a Network B VDI)?"},{"location":"FAQ/#why-do-i-get-an-error-message-when-accessing-the-internal-data-lake","text":"The internal Data Lake is only accessible from within a VM in the Collaborative Analytics Environment (CAE). It is not accessible from your personal or work laptop, Network B VDI or other cloud VM.","title":"Why do I get an error message when accessing the internal Data Lake?"},{"location":"FAQ/#source-code-control","text":"","title":"Source Code Control"},{"location":"FAQ/#how-do-i-link-my-visual-studio-subscription-to-my-statcan-cloud-account","text":"Login to https://visualstudio.microsoft.com/subscriptions/ with your organization's email address. For StatCan employees, this will be your canada.ca email address. Add your cloud account as an alternate account. This will allow you to use your licenses for Visual Studio & Azure DevOps in the CAE. For Statistics Canada employees: If you do not have a Visual Studio Subscription, please consult your supervisor. If they decide that a subscription is needed, they can then contact StatCan Software Asset Management by submitting an SRM to request a license on your behalf.","title":"How do I link my Visual Studio Subscription to my StatCan cloud account?"},{"location":"FAQ/#virtual-machines","text":"","title":"Virtual Machines"},{"location":"FAQ/#what-do-i-do-if-i-have-forgotten-the-password-for-my-virtual-machine","text":"If you forget the password of your virtual machine, please contact the support team through the https://cae-eac.slack.com channel to have it reset. You can also delete and recreate your virtual machine. Unfortunately, recreating your virtual machine means you will lose any data and software on your old machine.","title":"What do I do if I have forgotten the password for my virtual machine?"},{"location":"FAQ/#what-do-i-do-if-i-need-to-run-a-long-running-job-on-my-virtual-machine","text":"Machines are shut down every day at 7pm EST in order to reduce costs. If you have a long-running job, it is recommended that you use Databricks or Data Factory. WARNING: Disabling Auto-shutdown is not recommended as it can incur significant costs. To disable Auto-shutdown: Navigate to your virtual machine in the Azure Portal. Disable Auto-shutdown.","title":"What do I do if I need to run a long running job on my virtual machine?"},{"location":"FAQ/#how-do-i-request-changes-to-my-virtual-machine","text":"If the virtual machine you are currently using does not meet your requirements, please contact the support team through the https://cae-eac.slack.com channel.","title":"How do I request changes to my virtual machine?"},{"location":"FAQ/#databricks","text":"","title":"Databricks"},{"location":"FAQ/#why-am-i-unable-to-run-code-from-my-databricks-notebook","text":"You must first start a Databricks cluster that was previously created for you: Click on Clusters. Navigate to your cluster and click on the Start (arrow) button.","title":"Why am I unable to run code from my Databricks notebook?"},{"location":"FAQ/#what-type-of-clusters-are-available-in-databricks","text":"See the following link for the different types of available clusters: Databricks runtime releases Generally, LTS (long term support) clusters are supported and recommended by Databricks. If you need a different version of the databricks cluster, please contact the CAE support team.","title":"What type of clusters are available in databricks?"},{"location":"FAQ/#what-happens-when-clusters-are-upgraded","text":"LTS (long term support) have support for 1-2 years. They will need to be periodically updated to a newer version. When upgraded, all code should be re-run to ensure there are no issues when a cluster is updated","title":"What happens when clusters are upgraded?"},{"location":"FAQ/#how-do-you-read-an-excel-file-with-databrickspython","text":"Here is an example of how to read in an excel file: % python import pandas as pd pd . read_excel ( \"/dbfs/mnt/ccei-ccie-ext/Daily charts.xlsx\" , engine = 'openyxl' )","title":"How do you read an excel file with databricks/python?"},{"location":"FAQ/#dashboard","text":"","title":"Dashboard"},{"location":"FAQ/#how-do-i-change-my-subscription-so-that-i-can-see-my-resources","text":"In the Azure Portal click the Directory + subscription icon. Select the vdl subscription.","title":"How do I change my subscription so that I can see my resources?"},{"location":"FAQ/#caes-sql-server-deployment-options","text":"Azure SQL Database Azure SQL Database - Managed Instance SQL Server on Azure Virtual Machines Yes No No","title":"CAE's SQL Server Deployment Options"},{"location":"FAQ/#sql-server-machine-learning-services","text":"Machine Learning Services is a feature in SQL Server that allows to run Python and R scripts and perform predictive analytics and machine learning with relational data. The scripts are executed in-database without the need to move data outside SQL Server and can use open-source packages and frameworks as well as Microsoft's enterprise packages. This feature is offered only offered in 2 of the 3 Azure Cloud SQL Server deployment options, SQL Server on Azure Virtual Machines and Azure SQL Database - Managed Instance. Presently, CAE has only Azure SQL Database that doesn't offer this feature. However, CAE users can experiment with SQL ML services on their datascience VM Azure SQL server (developer edition).","title":"SQL Server Machine Learning Services"},{"location":"FAQ/#other","text":"","title":"Other"},{"location":"FAQ/#how-do-i-connect-to-the-internal-data-lake-with-power-bi-desktop","text":"Prerequisites: - A VM in the Collaborative Analytics Environment (CAE). - Power BI Desktop (available by default in the Windows Data Science Virtual Machine images.) Steps: 1. Login to your CAE VM. 2. Launch Power BI Desktop. 3. Follow the steps in Analyze data in Azure Data Lake Storage Gen2 by using Power BI - Create a report in Power BI Desktop . Please send a slack message to [Slack]https://cae-eac.slack.com if you don't know the Azure Data Lake Storage Gen2 URL.","title":"How do I connect to the internal data lake with Power BI Desktop?"},{"location":"FAQ/#how-can-statistics-canada-employees-transfer-files-from-their-data-centre","text":"For Statistics Canada Employees, they can refer to this internal documentation: Data Ingestion","title":"How can Statistics Canada employees transfer files from their data centre?"},{"location":"FAQ/#how-do-i-add-a-faq","text":"Please send your suggestion through the Slack channel.","title":"How do I add a FAQ?"},{"location":"GitHubConfiguration/","text":"GitHub.com is an online platform that is used for collaboration as well as tracking changes and versioning for a variety of project types. IMPORTANT: Do not store protected B data on GitHub.com Creating a GitHub Account For information on creating a GitHub account (or using your existing account), click here . Azure Data Factory On the Manage tab, click on Git configuration . Click Configure . Under Repository type , select GitHub , then enter your GitHub account username. Click Continue . A pop-up will appear. Click AuthorizeAzureDataFactory , then enter your GitHub account password. Configure a repository. You can either select a repository that you own, or enter a repository link. Specify additional settings, then click Apply . Set your working branch. You can either create a new branch or use an existing one. Then click Save . To remove GitHub Integration: On the Git configuration screen, click Disconnect . Enter the name of the Data Factory, then click Disconnect again to confirm. Azure Databricks Set up Git Integration Go to User Settings , then click on the Git Integration tab. Under Git provider , select GitHub. Enter your GitHub username. From your GitHub account, follow the instructions to create a personal access token , ensuring that the repo permission is checked. Note that you will have to repeat this process when your token has expired, unless you set the Expiration to never expire. Copy the token, and paste it into Databricks. Click Save . Add a Git Repository On the Repos tab, click Add Repo . With Clone remote Git repo selected, enter your GitHub repository url. The Git provider and Repo name should fill in automatically. Click Create . CAE Virtual Machines VS Code To learn how to use GitHub with VS Code, see the GitHub - Getting Started documentation. R-Studio In the File menu, click New Project... , then select Version Control . Select Git . Enter the URL for the GitHub repository that you want to clone, choose a folder on your VM where the local files will be stored, then click Create Project . Azure Machine Learning Create a compute instance, then open a terminal. In the terminal window, enter the following (replace the example email with your own): ssh-keygen -t rsa -b 4096 -C \"first.last@canada.ca\" Press ENTER until your key is generated. Enter in the terminal: cat ~/.ssh/id_rsa.pub . Select the output and copy it to the clipboard. Go to your GitHub account settings (on GitHub.com), click on SSH and GPG keys , then New SSH key . Paste in the key you just copied, then click Add SSH key . In the terminal window, type: git clone [url] (replace [url] with the SSH url for your GitHub repository, e.g. git@github.com:username/reponame.git ). When prompted, type yes . Microsoft Documentation Git Integration for Azure Machine Learning Azure Synapse On the Manage tab, click on Git configuration . Click Configure . Under Repository type , select GitHub , then enter your GitHub account username. Click Continue . A pop-up will appear. Enter your GitHub account login info, then click AuthorizeAzureSynapse . Configure a repository. You can either select a repository that you own, or enter a repository link. Specify additional settings, then click Apply . Set your working branch. You can either create a new branch or use an existing one. Then click Save . To remove GitHub Integration: On the Git configuration screen, click Disconnect . Enter the workspace name, then click Disconnect again to confirm.","title":"Configuration"},{"location":"GitHubConfiguration/#creating-a-github-account","text":"For information on creating a GitHub account (or using your existing account), click here .","title":"Creating a GitHub Account"},{"location":"GitHubConfiguration/#azure-data-factory","text":"On the Manage tab, click on Git configuration . Click Configure . Under Repository type , select GitHub , then enter your GitHub account username. Click Continue . A pop-up will appear. Click AuthorizeAzureDataFactory , then enter your GitHub account password. Configure a repository. You can either select a repository that you own, or enter a repository link. Specify additional settings, then click Apply . Set your working branch. You can either create a new branch or use an existing one. Then click Save . To remove GitHub Integration: On the Git configuration screen, click Disconnect . Enter the name of the Data Factory, then click Disconnect again to confirm.","title":"Azure Data Factory"},{"location":"GitHubConfiguration/#azure-databricks","text":"","title":"Azure Databricks"},{"location":"GitHubConfiguration/#set-up-git-integration","text":"Go to User Settings , then click on the Git Integration tab. Under Git provider , select GitHub. Enter your GitHub username. From your GitHub account, follow the instructions to create a personal access token , ensuring that the repo permission is checked. Note that you will have to repeat this process when your token has expired, unless you set the Expiration to never expire. Copy the token, and paste it into Databricks. Click Save .","title":"Set up Git Integration"},{"location":"GitHubConfiguration/#add-a-git-repository","text":"On the Repos tab, click Add Repo . With Clone remote Git repo selected, enter your GitHub repository url. The Git provider and Repo name should fill in automatically. Click Create .","title":"Add a Git Repository"},{"location":"GitHubConfiguration/#cae-virtual-machines","text":"","title":"CAE Virtual Machines"},{"location":"GitHubConfiguration/#vs-code","text":"To learn how to use GitHub with VS Code, see the GitHub - Getting Started documentation.","title":"VS Code"},{"location":"GitHubConfiguration/#r-studio","text":"In the File menu, click New Project... , then select Version Control . Select Git . Enter the URL for the GitHub repository that you want to clone, choose a folder on your VM where the local files will be stored, then click Create Project .","title":"R-Studio"},{"location":"GitHubConfiguration/#azure-machine-learning","text":"Create a compute instance, then open a terminal. In the terminal window, enter the following (replace the example email with your own): ssh-keygen -t rsa -b 4096 -C \"first.last@canada.ca\" Press ENTER until your key is generated. Enter in the terminal: cat ~/.ssh/id_rsa.pub . Select the output and copy it to the clipboard. Go to your GitHub account settings (on GitHub.com), click on SSH and GPG keys , then New SSH key . Paste in the key you just copied, then click Add SSH key . In the terminal window, type: git clone [url] (replace [url] with the SSH url for your GitHub repository, e.g. git@github.com:username/reponame.git ). When prompted, type yes .","title":"Azure Machine Learning"},{"location":"GitHubConfiguration/#microsoft-documentation","text":"Git Integration for Azure Machine Learning","title":"Microsoft Documentation"},{"location":"GitHubConfiguration/#azure-synapse","text":"On the Manage tab, click on Git configuration . Click Configure . Under Repository type , select GitHub , then enter your GitHub account username. Click Continue . A pop-up will appear. Enter your GitHub account login info, then click AuthorizeAzureSynapse . Configure a repository. You can either select a repository that you own, or enter a repository link. Specify additional settings, then click Apply . Set your working branch. You can either create a new branch or use an existing one. Then click Save . To remove GitHub Integration: On the Git configuration screen, click Disconnect . Enter the workspace name, then click Disconnect again to confirm.","title":"Azure Synapse"},{"location":"GitHubGettingStarted/","text":"GitHub.com is an online platform that is used for collaboration as well as tracking changes and versioning for a variety of project types. This document shows how to start using git with various Azure services that already have git integration set up. See GitHub - Configuration for instructions on how to set this up. IMPORTANT: Do not store protected B data on GitHub. Creating a GitHub Account For information on creating a GitHub account (or using your existing account), click here . Azure Data Factory If git integration is set up for your Data Factory, whenever you save or publish changes, these changes will automatically sync with the GitHub repository. To change which branch you're working one (the default collaboration branch is main ), click on the down arrow next to the branch name in the top left of the screen. From there, you can select a different branch or create a new one. Databricks Configuring a Personal Access Token Before you can work with GitHub repositories in Databricks, you first need to configure a personal access token (this gives Databricks access to your GitHub account). In Databricks, go to User Settings , then click on the Git Integration tab. Under Git provider , select GitHub. Enter your GitHub username. From your GitHub account, follow the instructions to create a personal access token , ensuring that the repo permission is checked. If you set an expiration date, you will need to repeat this process to create a new token after this date. Copy the token, and paste it into Databricks. Click Save . Creating/Changing Branches It is a best practice to do all of your work on your own branch (not main), then merge your changes with the main branch once you are ready to publish. To create a new branch or change to an existing branch, from the Repos tab, click the folder containing the GitHub repository to open it. Click on the down arrow next to the branch name, then click Git... . Click the down arrow to find an existing branch, or the plus to create a new one. Your branch should include your name. Once your branch is created, find it in the drop-down menu and click on it to switch to it. Click close . All of your work will now be saved to this branch unless you change it again later. Creating, Moving, and Cloning Notebooks To create a new notebook within the repository, from the drop-down menu next to your branch name, hover over Create , then click Notebook . You can also create folders this way. To move or clone an existing notebook from your workspace, navigate to the notebook (in the Workspace tab), click on the down arrown next to the notebook name, then click Clone to create a copy of the notebook in the repo, or Move to move the notebook from the workspace to the repo. Find the repo in the pop-up menu, and navigate to the folder where you want the notebook to be cloned or moved to. Click Clone/Select . Committing and Pushing Changes From the Repos tab, click the folder containing the GitHub repository to open it. Click on the down arrow next to the branch name, then click Git... . Make sure all the changes that you want to commit are checked, type a short summary describing what was changed, then click Commit & Push . Azure Synapse See Azure Data Factory above, and follow the same instructions in Synapse. VS Code How to Clone a Repository Click on the Source Control tab. Then you can either open a folder containing a git repository (if you already have one on your cloud VM), or clone a repository from a URL. To clone a repository, click Clone Repository . Copy the repository URL from GitHub (e.g. https://github.com/username/reponame ), paste it in the textbox, and click Clone from URL . Choose a folder on your cloud VM where the git repository will be stored locally. You may be promted to sign in to your GitHub account. Once the repository is cloned to your machine, you can open the local folder in VS Code. How to Commit Changes Before you can commit changes, you need to configure your user name and email. Open a terminal window (by clicking Terminal > New Terminal in the menu bar). In the terminal, type the following: git config user.name \"First Last\" git config user.email \"first.last@canada.ca\" When you are ready to publish your changes to GitHub, on the Source Control tab, type in a commit message, then click the checkmark button. Click the source control menu button, then click Push . If you get an error message (this will happen if your local copy of the repository is not up to date with the version stored on GitHub), first click Pull, then Push to merge your changes. R Studio Note: Instructions are the same whether you are using the desktop version of R Studio from a cloud VM or the web version through Databricks. Set Up In the File menu, click New Project... , then select Version Control . Select Git . Enter the URL for the GitHub repository that you want to clone, choose a folder where the local files will be stored, then click Create Project . If you are prompted to sign in to your GitHub account, enter your GitHub username and a personal access token as the password. How to Commit Changes When you are ready to publish your changes to GitHub, on the Git tab, click Commit . Click the checkbox for each of changes you want to commit. Enter a commit message briefly describing your changes, then click Commit . A pop-up will appear confirming that your commit was successful. Click Close . Click the Push button to upload your changes to GitHub. A pop-up will appear confirming that the push was successful. If you get an error message, (this will happen if your local copy of the repository is not up to date with the version stored on GitHub), first click Pull, then Push to merge your changes. FAQ I get an error when trying to push changes. What's happening? This could mean that your local copy of the repository is not up to date with the copy stored on GitHub. Try clicking pull first, then push your changes. It is a best practice to always run a pull command before starting to work with a repository to ensure you are working on the latest version, and no more than one person should be editing the same file at the same time. How do I revert back to a previous commit? This must be done from VS Code (accessible through a cloud VM), regardless of where you primarily use git. In VS Code, follow the steps above to clone a repository if you haven't done so already. Open a terminal (by clicking Terminal -> New Terminal ). Within the terminal window, type git revert HEAD~n --no-edit and press enter (replace n with the number of commits you want to revert, starting from 0). Note: you can find your commit history from GitHub by clicking on x commits near the top right of your repository. If there are merge conflicts, click Accept incoming change , then commit and push as normal (see above for detailed instructions on how to do this in VS Code).","title":"Getting Started"},{"location":"GitHubGettingStarted/#creating-a-github-account","text":"For information on creating a GitHub account (or using your existing account), click here .","title":"Creating a GitHub Account"},{"location":"GitHubGettingStarted/#azure-data-factory","text":"If git integration is set up for your Data Factory, whenever you save or publish changes, these changes will automatically sync with the GitHub repository. To change which branch you're working one (the default collaboration branch is main ), click on the down arrow next to the branch name in the top left of the screen. From there, you can select a different branch or create a new one.","title":"Azure Data Factory"},{"location":"GitHubGettingStarted/#databricks","text":"","title":"Databricks"},{"location":"GitHubGettingStarted/#configuring-a-personal-access-token","text":"Before you can work with GitHub repositories in Databricks, you first need to configure a personal access token (this gives Databricks access to your GitHub account). In Databricks, go to User Settings , then click on the Git Integration tab. Under Git provider , select GitHub. Enter your GitHub username. From your GitHub account, follow the instructions to create a personal access token , ensuring that the repo permission is checked. If you set an expiration date, you will need to repeat this process to create a new token after this date. Copy the token, and paste it into Databricks. Click Save .","title":"Configuring a Personal Access Token"},{"location":"GitHubGettingStarted/#creatingchanging-branches","text":"It is a best practice to do all of your work on your own branch (not main), then merge your changes with the main branch once you are ready to publish. To create a new branch or change to an existing branch, from the Repos tab, click the folder containing the GitHub repository to open it. Click on the down arrow next to the branch name, then click Git... . Click the down arrow to find an existing branch, or the plus to create a new one. Your branch should include your name. Once your branch is created, find it in the drop-down menu and click on it to switch to it. Click close . All of your work will now be saved to this branch unless you change it again later.","title":"Creating/Changing Branches"},{"location":"GitHubGettingStarted/#creating-moving-and-cloning-notebooks","text":"To create a new notebook within the repository, from the drop-down menu next to your branch name, hover over Create , then click Notebook . You can also create folders this way. To move or clone an existing notebook from your workspace, navigate to the notebook (in the Workspace tab), click on the down arrown next to the notebook name, then click Clone to create a copy of the notebook in the repo, or Move to move the notebook from the workspace to the repo. Find the repo in the pop-up menu, and navigate to the folder where you want the notebook to be cloned or moved to. Click Clone/Select .","title":"Creating, Moving, and Cloning Notebooks"},{"location":"GitHubGettingStarted/#committing-and-pushing-changes","text":"From the Repos tab, click the folder containing the GitHub repository to open it. Click on the down arrow next to the branch name, then click Git... . Make sure all the changes that you want to commit are checked, type a short summary describing what was changed, then click Commit & Push .","title":"Committing and Pushing Changes"},{"location":"GitHubGettingStarted/#azure-synapse","text":"See Azure Data Factory above, and follow the same instructions in Synapse.","title":"Azure Synapse"},{"location":"GitHubGettingStarted/#vs-code","text":"","title":"VS Code"},{"location":"GitHubGettingStarted/#how-to-clone-a-repository","text":"Click on the Source Control tab. Then you can either open a folder containing a git repository (if you already have one on your cloud VM), or clone a repository from a URL. To clone a repository, click Clone Repository . Copy the repository URL from GitHub (e.g. https://github.com/username/reponame ), paste it in the textbox, and click Clone from URL . Choose a folder on your cloud VM where the git repository will be stored locally. You may be promted to sign in to your GitHub account. Once the repository is cloned to your machine, you can open the local folder in VS Code.","title":"How to Clone a Repository"},{"location":"GitHubGettingStarted/#how-to-commit-changes","text":"Before you can commit changes, you need to configure your user name and email. Open a terminal window (by clicking Terminal > New Terminal in the menu bar). In the terminal, type the following: git config user.name \"First Last\" git config user.email \"first.last@canada.ca\" When you are ready to publish your changes to GitHub, on the Source Control tab, type in a commit message, then click the checkmark button. Click the source control menu button, then click Push . If you get an error message (this will happen if your local copy of the repository is not up to date with the version stored on GitHub), first click Pull, then Push to merge your changes.","title":"How to Commit Changes"},{"location":"GitHubGettingStarted/#r-studio","text":"Note: Instructions are the same whether you are using the desktop version of R Studio from a cloud VM or the web version through Databricks.","title":"R Studio"},{"location":"GitHubGettingStarted/#set-up","text":"In the File menu, click New Project... , then select Version Control . Select Git . Enter the URL for the GitHub repository that you want to clone, choose a folder where the local files will be stored, then click Create Project . If you are prompted to sign in to your GitHub account, enter your GitHub username and a personal access token as the password.","title":"Set Up"},{"location":"GitHubGettingStarted/#how-to-commit-changes_1","text":"When you are ready to publish your changes to GitHub, on the Git tab, click Commit . Click the checkbox for each of changes you want to commit. Enter a commit message briefly describing your changes, then click Commit . A pop-up will appear confirming that your commit was successful. Click Close . Click the Push button to upload your changes to GitHub. A pop-up will appear confirming that the push was successful. If you get an error message, (this will happen if your local copy of the repository is not up to date with the version stored on GitHub), first click Pull, then Push to merge your changes.","title":"How to Commit Changes"},{"location":"GitHubGettingStarted/#faq","text":"","title":"FAQ"},{"location":"GitHubGettingStarted/#i-get-an-error-when-trying-to-push-changes-whats-happening","text":"This could mean that your local copy of the repository is not up to date with the copy stored on GitHub. Try clicking pull first, then push your changes. It is a best practice to always run a pull command before starting to work with a repository to ensure you are working on the latest version, and no more than one person should be editing the same file at the same time.","title":"I get an error when trying to push changes. What's happening?"},{"location":"GitHubGettingStarted/#how-do-i-revert-back-to-a-previous-commit","text":"This must be done from VS Code (accessible through a cloud VM), regardless of where you primarily use git. In VS Code, follow the steps above to clone a repository if you haven't done so already. Open a terminal (by clicking Terminal -> New Terminal ). Within the terminal window, type git revert HEAD~n --no-edit and press enter (replace n with the number of commits you want to revert, starting from 0). Note: you can find your commit history from GitHub by clicking on x commits near the top right of your repository. If there are merge conflicts, click Accept incoming change , then commit and push as normal (see above for detailed instructions on how to do this in VS Code).","title":"How do I revert back to a previous commit?"},{"location":"Language/","text":"This document describes how to change languages in the various service offerings. Azure portal To change the language settings in the Azure portal: Click the Settings menu in the global page header. Click the Language & region tab. Use the drop-downs to choose your preferred language and regional format settings. Click Apply to update your language and regional format settings. Dashboard To select the English dashboard of the Collaborative Analytics Environment (CAE): From the dashboard view, click the arrow next to the dashboard name. Select the Collaborative Analytics Environment dashboard from the displayed list of dashboards. Note: If the dashboard is not listed, select Browse all dashboards to access the complete list. Virtual Machines Windows Server To configure the display language for a Windows virtual machine: Go to Settings . Select Time & Language . Select Language . Under Preferred languages , select Add a language . In the Choose a language to install dialog box, select your preferred language pack and then click Next . In the Install language features dialog box, click Install . The Windows display language box should now include the newly added language. To switch to the new language, select it from the Windows display language box, sign out of the current Windows session, and then sign back in. Ubuntu Server If you are using X2GO to access the GUI of your Ubuntu machine, you might need to manually install additional language packages because the default session is available only in English. Azure Machine Learning To change the language settings in the Microsoft Azure Machine Learning workspace: Click the Settings menu in the global page header. Under Language and formats , use the drop-downs to choose your preferred language and the regional format settings. Click Apply to update your language and regional format settings. Azure Machine Learning - Jupyter Lab Run in Azure ML compute instance terminal: sh pip install jupyterlab==3 Restart compute instance Run in Azure ML compute instance terminal: sh pip install git+https://github.com/StatCan/jupyterlab-language-pack-fr_FR In JupyterLab, switch Settings - Language - French Slack To change the language settings in the Slack application: Click the profile icon in the global page header. Click Preferences . Select the Language & region tab. Under Language , use the drop-down to choose your preferred language. Close the Preferences window. Azure Storage Explorer By default, the application detects your language based on the language preferences on your computer. To change the language settings on Microsoft Azure Storage Explorer: Click Edit . Click Settings . In the Settings page, select Application . Under Regional Settings , use the drop-down to choose your preferred language. To switch to the new language, close and restart the application. Power BI More information is also available in Supported languages and countries/regions for Power BI . Power BI Service By default, the Power BI service detects your language based on the language preferences on your computer. The steps to access and change these preferences may vary depending on your operating system and browser. To switch the menu language in the Power BI service: In the Power BI service, click the Settings icon and select Settings . In the General tab, select Language . Select your language and click Apply . See Languages for the Power BI service for more details. Power BI Desktop By default: - the Application language is based on the Windows language - the Model language is based on the Application language - the Query steps are based on the Application language . It is recommended to set the Model language to English (United States) . The model language applies only when the report is first created and cannot be changed on existing reports. Thus, setting the language model to U.S. English is recommended, unless you have a specific need to use another language for the report model. String comparisons and internal date fields are affected by this setting. To switch the menu language and model language in Power BI Desktop: Open the Options menu. Under GLOBAL , click Regional Settings and set the Application language and Model language to the desired language. NOTE : The Import language is set separately in the CURRENT FILE section Regional Settings . You need to change this only if you import data files that have numbers and dates in a specific locale (e.g., Canada English DD/MM/YYYY, United State English MM/DD/YYYY). Azure Databricks To change the language settings in Databricks: Select the user dropdown at the top right and select User Settings On the page, select Language Settings Click on the dropdown and select the language of your choice. Azure Data Factory To select the language: In Azure Data Factory, go to Settings . Select English . Click on Apply . JupyterLab To change the language settings in JupyterLab: Within JupyterLab, open up a console or terminal. Install the language example of your choice using pip. Example: python pip install jupyterlab-language-pack-zh-CN 3. Under the settings tab, highlight over language and select the language you installed. Click on OK to refresh the page, you will see the language change. For More information about changing languages, click the link below: Jupyerlab change language display Visual Studio Code (VSCode) To change the display language in VSCode: Open VSCode and open the command Pallette (Ctrl+Shift+P). In the command pallette, type in \"display\" and select install additional languages . Note: If you have already installed the language you wanted, you can select the language from the dropdown. On the left side of VSCode, languages will appear which can be installed, select the language of your choice. A pop-up may appear at the bottom right of the screen in which you can change the language and it will restart VSCode. Visual Studio If you have already installed language packages within Visual Studio: On the top bar, select Tools then Options . From the menu, under the Environment tab, select International Settings . From the drop down under Language , select the language of your choice. If you have not installed other language packages within Visual Studio On your computer, open the Visual Studio Installer. In the installer, select the modify button. On the new window, select Language Packs . Select all the languages you want to add and then select modify. From here you can follow the steps on using the installed language packages within Visual Studio. RStudio To set RStudio into a different language: Open up RStudio and open up the console. In the console, type in \"Sys.getenv(LANGUAGE = \"fr\") Note: \"fr\" is for the french language, for a list of other languages that can be used, click here . To test it, you can type \"2+x\" and it should give an error in the language inputted. Web browsers Chrome Safari Edge Firefox Opera","title":"Language"},{"location":"Language/#azure-portal","text":"To change the language settings in the Azure portal: Click the Settings menu in the global page header. Click the Language & region tab. Use the drop-downs to choose your preferred language and regional format settings. Click Apply to update your language and regional format settings.","title":"Azure portal"},{"location":"Language/#dashboard","text":"To select the English dashboard of the Collaborative Analytics Environment (CAE): From the dashboard view, click the arrow next to the dashboard name. Select the Collaborative Analytics Environment dashboard from the displayed list of dashboards. Note: If the dashboard is not listed, select Browse all dashboards to access the complete list.","title":"Dashboard"},{"location":"Language/#virtual-machines","text":"","title":"Virtual Machines"},{"location":"Language/#windows-server","text":"To configure the display language for a Windows virtual machine: Go to Settings . Select Time & Language . Select Language . Under Preferred languages , select Add a language . In the Choose a language to install dialog box, select your preferred language pack and then click Next . In the Install language features dialog box, click Install . The Windows display language box should now include the newly added language. To switch to the new language, select it from the Windows display language box, sign out of the current Windows session, and then sign back in.","title":"Windows Server"},{"location":"Language/#ubuntu-server","text":"If you are using X2GO to access the GUI of your Ubuntu machine, you might need to manually install additional language packages because the default session is available only in English.","title":"Ubuntu Server"},{"location":"Language/#azure-machine-learning","text":"To change the language settings in the Microsoft Azure Machine Learning workspace: Click the Settings menu in the global page header. Under Language and formats , use the drop-downs to choose your preferred language and the regional format settings. Click Apply to update your language and regional format settings.","title":"Azure Machine Learning"},{"location":"Language/#azure-machine-learning-jupyter-lab","text":"Run in Azure ML compute instance terminal: sh pip install jupyterlab==3 Restart compute instance Run in Azure ML compute instance terminal: sh pip install git+https://github.com/StatCan/jupyterlab-language-pack-fr_FR In JupyterLab, switch Settings - Language - French","title":"Azure Machine Learning - Jupyter Lab"},{"location":"Language/#slack","text":"To change the language settings in the Slack application: Click the profile icon in the global page header. Click Preferences . Select the Language & region tab. Under Language , use the drop-down to choose your preferred language. Close the Preferences window.","title":"Slack"},{"location":"Language/#azure-storage-explorer","text":"By default, the application detects your language based on the language preferences on your computer. To change the language settings on Microsoft Azure Storage Explorer: Click Edit . Click Settings . In the Settings page, select Application . Under Regional Settings , use the drop-down to choose your preferred language. To switch to the new language, close and restart the application.","title":"Azure Storage Explorer"},{"location":"Language/#power-bi","text":"More information is also available in Supported languages and countries/regions for Power BI .","title":"Power BI"},{"location":"Language/#power-bi-service","text":"By default, the Power BI service detects your language based on the language preferences on your computer. The steps to access and change these preferences may vary depending on your operating system and browser. To switch the menu language in the Power BI service: In the Power BI service, click the Settings icon and select Settings . In the General tab, select Language . Select your language and click Apply . See Languages for the Power BI service for more details.","title":"Power BI Service"},{"location":"Language/#power-bi-desktop","text":"By default: - the Application language is based on the Windows language - the Model language is based on the Application language - the Query steps are based on the Application language . It is recommended to set the Model language to English (United States) . The model language applies only when the report is first created and cannot be changed on existing reports. Thus, setting the language model to U.S. English is recommended, unless you have a specific need to use another language for the report model. String comparisons and internal date fields are affected by this setting. To switch the menu language and model language in Power BI Desktop: Open the Options menu. Under GLOBAL , click Regional Settings and set the Application language and Model language to the desired language. NOTE : The Import language is set separately in the CURRENT FILE section Regional Settings . You need to change this only if you import data files that have numbers and dates in a specific locale (e.g., Canada English DD/MM/YYYY, United State English MM/DD/YYYY).","title":"Power BI Desktop"},{"location":"Language/#azure-databricks","text":"To change the language settings in Databricks: Select the user dropdown at the top right and select User Settings On the page, select Language Settings Click on the dropdown and select the language of your choice.","title":"Azure Databricks"},{"location":"Language/#azure-data-factory","text":"To select the language: In Azure Data Factory, go to Settings . Select English . Click on Apply .","title":"Azure Data Factory"},{"location":"Language/#jupyterlab","text":"To change the language settings in JupyterLab: Within JupyterLab, open up a console or terminal. Install the language example of your choice using pip. Example: python pip install jupyterlab-language-pack-zh-CN 3. Under the settings tab, highlight over language and select the language you installed. Click on OK to refresh the page, you will see the language change. For More information about changing languages, click the link below: Jupyerlab change language display","title":"JupyterLab"},{"location":"Language/#visual-studio-code-vscode","text":"To change the display language in VSCode: Open VSCode and open the command Pallette (Ctrl+Shift+P). In the command pallette, type in \"display\" and select install additional languages . Note: If you have already installed the language you wanted, you can select the language from the dropdown. On the left side of VSCode, languages will appear which can be installed, select the language of your choice. A pop-up may appear at the bottom right of the screen in which you can change the language and it will restart VSCode.","title":"Visual Studio Code (VSCode)"},{"location":"Language/#visual-studio","text":"If you have already installed language packages within Visual Studio: On the top bar, select Tools then Options . From the menu, under the Environment tab, select International Settings . From the drop down under Language , select the language of your choice.","title":"Visual Studio"},{"location":"Language/#if-you-have-not-installed-other-language-packages-within-visual-studio","text":"On your computer, open the Visual Studio Installer. In the installer, select the modify button. On the new window, select Language Packs . Select all the languages you want to add and then select modify. From here you can follow the steps on using the installed language packages within Visual Studio.","title":"If you have not installed other language packages within Visual Studio"},{"location":"Language/#rstudio","text":"To set RStudio into a different language: Open up RStudio and open up the console. In the console, type in \"Sys.getenv(LANGUAGE = \"fr\") Note: \"fr\" is for the french language, for a list of other languages that can be used, click here . To test it, you can type \"2+x\" and it should give an error in the language inputted.","title":"RStudio"},{"location":"Language/#web-browsers","text":"Chrome Safari Edge Firefox Opera","title":"Web browsers"},{"location":"Login/","text":"Prerequisites An authorized StatCan Cloud account or guest account. Access using StatCan Network accounts is currently only available for the Power BI Service, but may be added in the future for other services in the Collaborative Analytics Environment (CAE). Notes We recommend you use Chrome, Chromium or Edge (not Internet Explorer) to access the Azure Portal, Azure services or the Power BI Service. When connecting to the Azure cloud services, you will either login with your: - StatCan Cloud account (i.e.\u202ffirstname.lastname@cloud.statcan.ca), or - Other government or researcher email credentials (e.g. firstname.lastname@dept-d\u00e9pt.gc.ca or name@gov.prov.ca), or - StatCan Network account (i.e. firstname.lastname@statcan.gc.ca) for StatCan employees using the Power BI Service only. Follow the instructions for your type of account to complete your sign-in. StatCan Cloud Account (firstname.lastname@cloud.statcan.ca) Applicable to all Azure cloud Services (Power BI, Databricks, Data Factory, Virtual Machine, etc.) Using Chrome, Chromium or Edge, open either: The Collaborative Analytics Environment Azure Portal Dashboard The Power BI App URL (if provided) or Power BI Service Login Page When opening a Power BI App URL or the Power BI Service login page, you will be directed to the Microsoft Power BI Sign In landing page, as shown below, saying \u201cAlready have an account?\u201d. Click on SIGN IN . You will then be prompted to either enter or pick an account: Upon your first time signing in, you will receive the Microsoft Sign in prompt, as shown below. Enter your cloud account credentials (firstname.lastname@cloud.statcan.ca), and click on Next . On subsequent sign-ins, you will receive the Microsoft Pick an account prompt, as shown below. Click on your cloud.statcan.ca account. You will then be prompted to enter your cloud account password, as shown below. Once entered, click on Sign in . Lastly, you may receive the More Information required \u2013 Your organization needs more information to keep your account secure prompt from Statistics Canada, as shown below, especially if it\u2019s the first time you log in to a Web portal with your cloud account. Click on Next and ensure to follow the instructions to secure your account by configuring your authentication email and configuring your security questions. Other Government or Researcher Email Credentials Applicable to most Azure cloud Services (Power BI, Databricks, Data Factory, Virtual Machine, etc.) Using Chrome, Chromium or Edge, open either: The Collaborative Analytics Environment Azure Portal Dashboard The Power BI App URL (if provided) or Power BI Service Login Page Azure Databricks Azure Data Factory When opening a Power BI App URL or the Power BI Service login page, you will be directed to the Microsoft Power BI Sign In landing page, as shown below, saying \u201cAlready have an account?\u201d. Click on SIGN IN . You will then be prompted to either enter or pick an account: Upon your first time signing in, you will receive the Microsoft Sign in prompt, as shown below. Enter your official email/O365 credentials or GCCollaboration credentials (e.g., firstname.lastname@dept-d\u00e9pt.gc.ca, firstname.lastname.department@gccollaboration.ca or name@gov.prov.ca), and click on Next . On subsequent sign-ins, you will receive the Microsoft Pick an account prompt, as shown below. Click on your official email/O365 credentials or GCCollaboration credentials (e.g., firstname.lastname@dept-d\u00e9pt.gc.ca, firstname.lastname.department@gccollaboration.ca or name@gov.prov.ca) account. You will then be prompted to enter your cloud account password, as shown below. Once entered, click on Sign in . If your Official Email Credentials do not support this type of login you will automatically be emailed a code to enter in place of a password and then click on Sign in . The Email you receive will look like the following, if you do not receive it double check if it was sent to your Spam or Junk folder. Accept the Review permissions (first time only consent). You will then receive the following message. Wait until completed. Lastly, you may receive the More Information required \u2013 Your organization needs more information to keep your account secure prompt from Statistics Canada, as shown below, especially if it\u2019s the first time you log in to a Web portal with your cloud account. Click on Next and ensure to follow the instructions to secure your account by configuring your authentication email and configuring your security questions. StatCan Network Account (firstname.lastname@statcan.gc.ca) Applicable to the Power BI Service for Statistics Canada employees only. May be added in the future for other services in the Collaborative Analytics Environment (CAE) Please note that step 5 (Internet password) below may not appear in the order specified. From a StatCan laptop or Network B VDI, and using Chrome or Chromium, open either: The Power BI App URL (if provided) or https://powerbi.microsoft.com/en-us/landing/signin/ You will be directed to the Microsoft Power BI Sign In landing page, as shown below, saying \u201cAlready have an account?\u201d. You will then be prompted to either enter or pick an account: On first time sign-in, you will receive the Microsoft Sign in prompt, as shown below. Enter your StatCan Network account (firstname.lastname@statcan.gc.ca), and then, click on Next . On subsequent sign-ins, you will receive the Microsoft Pick an account prompt, as shown below. Click on your statcan.gc.ca account. You will then receive a Microsoft message \u201cTaking you to your organizations sign-in page\u201d. You may then be prompted to enter your Internet username and password as shown below. Once entered, click Sign in . You will then be prompted to sign in using your StatCan Network account (i.e.\u202ffirstname.lastname@statcan.gc.ca) and Network A password. Once entered, click Sign in . Lastly, you may receive the More Information required \u2013 Your organization needs more information to keep your account secure prompt from Statistics Canada, as shown below, especially if it\u2019s the first time you logged in to a Web portal with your StatCan Network account. Click on Next and ensure to follow the instructions to secure your account by configuring your authentication email and configuring your security questions. Microsoft Documentation Azure portal documentation","title":"How to Login"},{"location":"Login/#prerequisites","text":"An authorized StatCan Cloud account or guest account. Access using StatCan Network accounts is currently only available for the Power BI Service, but may be added in the future for other services in the Collaborative Analytics Environment (CAE).","title":"Prerequisites"},{"location":"Login/#notes","text":"We recommend you use Chrome, Chromium or Edge (not Internet Explorer) to access the Azure Portal, Azure services or the Power BI Service. When connecting to the Azure cloud services, you will either login with your: - StatCan Cloud account (i.e.\u202ffirstname.lastname@cloud.statcan.ca), or - Other government or researcher email credentials (e.g. firstname.lastname@dept-d\u00e9pt.gc.ca or name@gov.prov.ca), or - StatCan Network account (i.e. firstname.lastname@statcan.gc.ca) for StatCan employees using the Power BI Service only. Follow the instructions for your type of account to complete your sign-in.","title":"Notes"},{"location":"Login/#statcan-cloud-account-firstnamelastnamecloudstatcanca","text":"Applicable to all Azure cloud Services (Power BI, Databricks, Data Factory, Virtual Machine, etc.) Using Chrome, Chromium or Edge, open either: The Collaborative Analytics Environment Azure Portal Dashboard The Power BI App URL (if provided) or Power BI Service Login Page When opening a Power BI App URL or the Power BI Service login page, you will be directed to the Microsoft Power BI Sign In landing page, as shown below, saying \u201cAlready have an account?\u201d. Click on SIGN IN . You will then be prompted to either enter or pick an account: Upon your first time signing in, you will receive the Microsoft Sign in prompt, as shown below. Enter your cloud account credentials (firstname.lastname@cloud.statcan.ca), and click on Next . On subsequent sign-ins, you will receive the Microsoft Pick an account prompt, as shown below. Click on your cloud.statcan.ca account. You will then be prompted to enter your cloud account password, as shown below. Once entered, click on Sign in . Lastly, you may receive the More Information required \u2013 Your organization needs more information to keep your account secure prompt from Statistics Canada, as shown below, especially if it\u2019s the first time you log in to a Web portal with your cloud account. Click on Next and ensure to follow the instructions to secure your account by configuring your authentication email and configuring your security questions.","title":"StatCan Cloud Account (firstname.lastname@cloud.statcan.ca)"},{"location":"Login/#other-government-or-researcher-email-credentials","text":"Applicable to most Azure cloud Services (Power BI, Databricks, Data Factory, Virtual Machine, etc.) Using Chrome, Chromium or Edge, open either: The Collaborative Analytics Environment Azure Portal Dashboard The Power BI App URL (if provided) or Power BI Service Login Page Azure Databricks Azure Data Factory When opening a Power BI App URL or the Power BI Service login page, you will be directed to the Microsoft Power BI Sign In landing page, as shown below, saying \u201cAlready have an account?\u201d. Click on SIGN IN . You will then be prompted to either enter or pick an account: Upon your first time signing in, you will receive the Microsoft Sign in prompt, as shown below. Enter your official email/O365 credentials or GCCollaboration credentials (e.g., firstname.lastname@dept-d\u00e9pt.gc.ca, firstname.lastname.department@gccollaboration.ca or name@gov.prov.ca), and click on Next . On subsequent sign-ins, you will receive the Microsoft Pick an account prompt, as shown below. Click on your official email/O365 credentials or GCCollaboration credentials (e.g., firstname.lastname@dept-d\u00e9pt.gc.ca, firstname.lastname.department@gccollaboration.ca or name@gov.prov.ca) account. You will then be prompted to enter your cloud account password, as shown below. Once entered, click on Sign in . If your Official Email Credentials do not support this type of login you will automatically be emailed a code to enter in place of a password and then click on Sign in . The Email you receive will look like the following, if you do not receive it double check if it was sent to your Spam or Junk folder. Accept the Review permissions (first time only consent). You will then receive the following message. Wait until completed. Lastly, you may receive the More Information required \u2013 Your organization needs more information to keep your account secure prompt from Statistics Canada, as shown below, especially if it\u2019s the first time you log in to a Web portal with your cloud account. Click on Next and ensure to follow the instructions to secure your account by configuring your authentication email and configuring your security questions.","title":"Other Government or Researcher Email Credentials"},{"location":"Login/#statcan-network-account-firstnamelastnamestatcangcca","text":"Applicable to the Power BI Service for Statistics Canada employees only. May be added in the future for other services in the Collaborative Analytics Environment (CAE) Please note that step 5 (Internet password) below may not appear in the order specified. From a StatCan laptop or Network B VDI, and using Chrome or Chromium, open either: The Power BI App URL (if provided) or https://powerbi.microsoft.com/en-us/landing/signin/ You will be directed to the Microsoft Power BI Sign In landing page, as shown below, saying \u201cAlready have an account?\u201d. You will then be prompted to either enter or pick an account: On first time sign-in, you will receive the Microsoft Sign in prompt, as shown below. Enter your StatCan Network account (firstname.lastname@statcan.gc.ca), and then, click on Next . On subsequent sign-ins, you will receive the Microsoft Pick an account prompt, as shown below. Click on your statcan.gc.ca account. You will then receive a Microsoft message \u201cTaking you to your organizations sign-in page\u201d. You may then be prompted to enter your Internet username and password as shown below. Once entered, click Sign in . You will then be prompted to sign in using your StatCan Network account (i.e.\u202ffirstname.lastname@statcan.gc.ca) and Network A password. Once entered, click Sign in . Lastly, you may receive the More Information required \u2013 Your organization needs more information to keep your account secure prompt from Statistics Canada, as shown below, especially if it\u2019s the first time you logged in to a Web portal with your StatCan Network account. Click on Next and ensure to follow the instructions to secure your account by configuring your authentication email and configuring your security questions.","title":"StatCan Network Account (firstname.lastname@statcan.gc.ca)"},{"location":"Login/#microsoft-documentation","text":"Azure portal documentation","title":"Microsoft Documentation"},{"location":"PostgreSQL/","text":"Fran\u00e7ais Accessing Azure PostgreSQL Database Azure Data Factory A linked service can be setup inside Azure Data Factory. Username is Cloud account username or AD group name if access was granted to a AD group, followed by the server name. Password is an Azure AD access Token as described below in step 6 of pgAdmin section or Please contact the support team through the https://cae-eac.slack.com channel if you need assistance. Virtual Machine You can access an Azure PostgreSQL database from your cloud virtual machine, using various applications including: 1. pgAdmin 2. Azure Data Studio 3. Visual Studio Code Prerequisites A virtual machine in the Collaborative Analytics Environment (CAE). See the VM page for more information. pgAdmin or Azure Data Studio and Visual Studio Code . Those two are available by default in the Windows Data Science Virtual Machine images. pgAdmin This is one of the common tool for PostgreSQL database administration. In your cloud VM install pgAdmin from https://www.pgadmin.org/download/ Connect to your cloud VM and launch pgAdmin . Add the server you need to connect to by right clicking on Servers in the top left corner In the General tab, enter a name for your server. You can write the real name of the server In the Connection tab, enter the full Server name and add your Cloud Account as Username followed by the server name or the active directory group you belong to followed by the server name if access to the server was granted to that active directory group. You can now see in the list of server the newly added server. Click on it to connect and you will be asked to enter a password The password you need to enter is a Azure AD access Token that will be generated for an authenticated Azure AD user. From PowerShell, you can generate this access token by entering the following command az account get-access-token --resource https://ossrdbms-aad.database.windows.net The output is as follow, where you will need to copy the value for accessToken and use it as Password in pgAdmin Azure Data Studio Connectez-vous \u00e0 votre machine virtuelle dans l'EAC et lancez Azure Data Studio . The connexion details should be as follow Where the user name can be the cloud account username followed by the server name: firstname.lastname\\ @cloud.statcan.ca@servername.postgres.database.azure.com or firstname.lastname@cloud.statcan.ca@servername if the access to the server was granted to an active directory group the user belong to AD-Group@servername.postgres.database.azure.com or AD-Group@servername","title":"Azure Postgres Database"},{"location":"PostgreSQL/#accessing-azure-postgresql-database","text":"","title":"Accessing Azure PostgreSQL Database"},{"location":"PostgreSQL/#azure-data-factory","text":"A linked service can be setup inside Azure Data Factory. Username is Cloud account username or AD group name if access was granted to a AD group, followed by the server name. Password is an Azure AD access Token as described below in step 6 of pgAdmin section or Please contact the support team through the https://cae-eac.slack.com channel if you need assistance.","title":"Azure Data Factory"},{"location":"PostgreSQL/#virtual-machine","text":"You can access an Azure PostgreSQL database from your cloud virtual machine, using various applications including: 1. pgAdmin 2. Azure Data Studio 3. Visual Studio Code","title":"Virtual Machine"},{"location":"PostgreSQL/#prerequisites","text":"A virtual machine in the Collaborative Analytics Environment (CAE). See the VM page for more information. pgAdmin or Azure Data Studio and Visual Studio Code . Those two are available by default in the Windows Data Science Virtual Machine images.","title":"Prerequisites"},{"location":"PostgreSQL/#pgadmin","text":"This is one of the common tool for PostgreSQL database administration. In your cloud VM install pgAdmin from https://www.pgadmin.org/download/ Connect to your cloud VM and launch pgAdmin . Add the server you need to connect to by right clicking on Servers in the top left corner In the General tab, enter a name for your server. You can write the real name of the server In the Connection tab, enter the full Server name and add your Cloud Account as Username followed by the server name or the active directory group you belong to followed by the server name if access to the server was granted to that active directory group. You can now see in the list of server the newly added server. Click on it to connect and you will be asked to enter a password The password you need to enter is a Azure AD access Token that will be generated for an authenticated Azure AD user. From PowerShell, you can generate this access token by entering the following command az account get-access-token --resource https://ossrdbms-aad.database.windows.net The output is as follow, where you will need to copy the value for accessToken and use it as Password in pgAdmin","title":"pgAdmin"},{"location":"PostgreSQL/#azure-data-studio","text":"Connectez-vous \u00e0 votre machine virtuelle dans l'EAC et lancez Azure Data Studio . The connexion details should be as follow Where the user name can be the cloud account username followed by the server name: firstname.lastname\\ @cloud.statcan.ca@servername.postgres.database.azure.com or firstname.lastname@cloud.statcan.ca@servername if the access to the server was granted to an active directory group the user belong to AD-Group@servername.postgres.database.azure.com or AD-Group@servername","title":"Azure Data Studio"},{"location":"R-Shiny/","text":"This document describes how to access the R-Shiny package from RStudio. Getting Started To use R-Shiny, please send a slack message to the CEA team to enable RStudio on your Databricks cluster. Warning : R-Shiny clusters are shut down every day at 7pm . To save on costs, please stop your R-Shiny clusters when you are not using them. Accessing R-Shiny from Databricks From the Azure portal, launch the Databricks workspace that was created for you. From the Databricks workspace, click on Clusters . From the list of available clusters, select the cluster with RStudio installed. Note: You must have the cluster running before you can access RStudio. See the Databricks section for information on how to start a cluster. Select the Apps tab. Click on Set up RStudio . A one-time password is generated for you, click on show to display and copy it. Click on Open RStudio . A new tab opens, enter the username and password provided (step 6) in the login form and sign in to RStudio. From the RStudio UI, enter the library(shiny) command in the console to import the Shiny package. RShiny App Example You can use use the Hello Shiny example to explore the structure of a Shiny app. Launch the app from your R session by running: library(shiny) runExample(\"01_hello\") Your app should match the image below. Accessing files from the datalake By default, the working directory in RStudio will be on the driver node of the Databricks cluster. To persist your work, you'll need to use DBFS. To access DBFS in the File Explorer, click on the ... to the right and enter /dbfs/mnt/ at the prompt. The data lake will be available and you will be able to access and store your files. When your cluster is terminated at the end of your session, your work will be there for you when you return. NOTE: Here are some code samples to access your files from the datalake. library(SparkR) sparkR.session() testData = as.data.frame(read.df(\"/mnt/the file path\", source = \"the file extension\", header=\"true\", inferSchema = \"true\")) str(testData) setwd(\"/dbfs/mnt/the file path\") testData = read.csv(\"the filename\") str(testData)","title":"R-Shiny"},{"location":"R-Shiny/#getting-started","text":"To use R-Shiny, please send a slack message to the CEA team to enable RStudio on your Databricks cluster. Warning : R-Shiny clusters are shut down every day at 7pm . To save on costs, please stop your R-Shiny clusters when you are not using them.","title":"Getting Started"},{"location":"R-Shiny/#accessing-r-shiny-from-databricks","text":"From the Azure portal, launch the Databricks workspace that was created for you. From the Databricks workspace, click on Clusters . From the list of available clusters, select the cluster with RStudio installed. Note: You must have the cluster running before you can access RStudio. See the Databricks section for information on how to start a cluster. Select the Apps tab. Click on Set up RStudio . A one-time password is generated for you, click on show to display and copy it. Click on Open RStudio . A new tab opens, enter the username and password provided (step 6) in the login form and sign in to RStudio. From the RStudio UI, enter the library(shiny) command in the console to import the Shiny package.","title":"Accessing R-Shiny from Databricks"},{"location":"R-Shiny/#rshiny-app-example","text":"You can use use the Hello Shiny example to explore the structure of a Shiny app. Launch the app from your R session by running: library(shiny) runExample(\"01_hello\") Your app should match the image below.","title":"RShiny App Example"},{"location":"R-Shiny/#accessing-files-from-the-datalake","text":"By default, the working directory in RStudio will be on the driver node of the Databricks cluster. To persist your work, you'll need to use DBFS. To access DBFS in the File Explorer, click on the ... to the right and enter /dbfs/mnt/ at the prompt. The data lake will be available and you will be able to access and store your files. When your cluster is terminated at the end of your session, your work will be there for you when you return. NOTE: Here are some code samples to access your files from the datalake. library(SparkR) sparkR.session() testData = as.data.frame(read.df(\"/mnt/the file path\", source = \"the file extension\", header=\"true\", inferSchema = \"true\")) str(testData) setwd(\"/dbfs/mnt/the file path\") testData = read.csv(\"the filename\") str(testData)","title":"Accessing files from the datalake"},{"location":"RegisterProject/","text":"Register here to be part of the Data Analytics as a Service (DAaaS) Early Adopter Community: https://forms.office.com/r/ErV7YkMPBF If you have already been given access to the platform, see How to login for more information.","title":"Register your Project"},{"location":"StartHere/","text":"Collaborative Analytics Environment Data Storage Scenario Best Choice Other Choices Relational data from SQL Server or mysql Total size less than 4 TB with largest fact table < 60 millions Azure SQL Database Synapse SQL Pools or Data Lake (Parquet) Binary Files (images or similar) Azure Data Lake Relational Data with lots of GeoSpatial queries Requires builtin GeoSpatial functions Less than 4TB and largest table < 60 million records Azure Database for PostgreSQL Tabular files (CSV, Parquet...) to be used in ML training Azure Data Lake Azure SQL Database SQL Data warehouse >10TB Storage with fact tables > 100 million records Synapse SQL Pools Notebooks Scenario Best Choice Other Choices Data manipulation using python with Pandas Migrating from individual-machine-experience Dataset < 10 GB (per dataframe) Azure Machine Learning Must use Jupyter notebooks or Jupyter lab Need access to the terminal of the VM Azure Machine Learning Large dataset Using Pyspark Performance is the biggest concern No dependancy on SQL Pools in Synapse Azure Databricks Azure Synapse Spark Use R on Spark Azure Databricks Need to use Spark on single machine cluster Azure Databricks Use .NET for Spark Azure Synapse Spark Project that has SQL Pools, Pipelines and Spark Prefer one UI Azure Synapse Spark Data movement Scenario Best Choice Other Choices Prefer no code or low code. Use relational database Data Factory Synapse Pipelines Prefer no code or low code. Use data lake Synapse Pipelines Data Factory Prefer Python, R Azure Databricks Prefer SQL Synapse Serverlerss SQL Pools UI apps Scenario Best Choice Other Choices Desktop software (SQL Server Management Studio, VSCode, SAS Desktop) Virtual Machines / Azure Virtual Desktop R-Shiny Apps Azure Machine Learning compute + App service Azure Databricks","title":"Collaborative Analytics Environment"},{"location":"StartHere/#collaborative-analytics-environment","text":"","title":"Collaborative Analytics Environment"},{"location":"StartHere/#data-storage","text":"Scenario Best Choice Other Choices Relational data from SQL Server or mysql Total size less than 4 TB with largest fact table < 60 millions Azure SQL Database Synapse SQL Pools or Data Lake (Parquet) Binary Files (images or similar) Azure Data Lake Relational Data with lots of GeoSpatial queries Requires builtin GeoSpatial functions Less than 4TB and largest table < 60 million records Azure Database for PostgreSQL Tabular files (CSV, Parquet...) to be used in ML training Azure Data Lake Azure SQL Database SQL Data warehouse >10TB Storage with fact tables > 100 million records Synapse SQL Pools","title":"Data Storage"},{"location":"StartHere/#notebooks","text":"Scenario Best Choice Other Choices Data manipulation using python with Pandas Migrating from individual-machine-experience Dataset < 10 GB (per dataframe) Azure Machine Learning Must use Jupyter notebooks or Jupyter lab Need access to the terminal of the VM Azure Machine Learning Large dataset Using Pyspark Performance is the biggest concern No dependancy on SQL Pools in Synapse Azure Databricks Azure Synapse Spark Use R on Spark Azure Databricks Need to use Spark on single machine cluster Azure Databricks Use .NET for Spark Azure Synapse Spark Project that has SQL Pools, Pipelines and Spark Prefer one UI Azure Synapse Spark","title":"Notebooks"},{"location":"StartHere/#data-movement","text":"Scenario Best Choice Other Choices Prefer no code or low code. Use relational database Data Factory Synapse Pipelines Prefer no code or low code. Use data lake Synapse Pipelines Data Factory Prefer Python, R Azure Databricks Prefer SQL Synapse Serverlerss SQL Pools","title":"Data movement"},{"location":"StartHere/#ui-apps","text":"Scenario Best Choice Other Choices Desktop software (SQL Server Management Studio, VSCode, SAS Desktop) Virtual Machines / Azure Virtual Desktop R-Shiny Apps Azure Machine Learning compute + App service Azure Databricks","title":"UI apps"},{"location":"TestVideo/","text":"Test Video How to Login to the Azure Portal How to Login to Azure Portal - Transcript Go to portal.azure.com in your web browser. Enter your credential and password. For internal Statcan users, use your cloud account. For external users, use your external account. The first time you login you will be prompted to enter your security questions. For multifactor authentication, you can use the Microsoft Authenticator app on your cell phone.","title":"Test Video"},{"location":"TestVideo/#test-video","text":"","title":"Test Video"},{"location":"TestVideo/#how-to-login-to-the-azure-portal","text":"How to Login to Azure Portal - Transcript Go to portal.azure.com in your web browser. Enter your credential and password. For internal Statcan users, use your cloud account. For external users, use your external account. The first time you login you will be prompted to enter your security questions. For multifactor authentication, you can use the Microsoft Authenticator app on your cell phone.","title":"How to Login to the Azure Portal"},{"location":"VirtualMachines/","text":"Find Your DevTest Lab In your project's custom Dashboard in the Azure Portal, click on the DevTest Lab. Select the DevTest Lab that was assigned. Create Your Virtual Machine Note: In some instances a Virtual Machine will be pre-created for you and you will not have permission to create a virtual machine. See the FAQ if you need to make changes to your virtual machine. From the DevTest Lab Overview page, click on the + Add button. Choose an appropriate base for your VM (e.g., Data Science Virtual Machine - Windows Server 2019). For more details on the software included with the Data Science Virtual Machines, please click here . Enter a name for your VM and a User name and password that you will use to login to the VM. Be sure to deselect the Use a saved secret and Save as default password checkboxes. You may click the Change Size Link to change your VM size if you wish to do so. Leave the rest as defaults and click on the Create button. Find Your Virtual Machine From the DevTest Lab Overview page, scroll down until you see your VM under My virtual machines . Click on your VM to access its Overview page. Start Your Virtual Machine From the Overview page for your VM, click on the Start button. It takes a few minutes for your VM to start up. Monitor its startup progress by selecting the Notifications icon at the top right of the window. Connect To Your Virtual Machine From the Overview page for your VM, click on the Browser connect button (if you do not see a Browser connect button you might have to click on the Connect button and then choose Bastion from the dropdown menu). Ensure the Open in new window checkbox is selected, enter the Username and Password that you used when you created your VM, and click on the Connect button. Your VM should open in a new browser tab. Note : By default, the Ubuntu virtual machine opens in Terminal mode. You can access the GUI of your Ubuntu machine from a Windows machine using X2Go . Note : After attempting to login for the first time, an error may appear that a popup blocker is preventing a new window to open. To disable it, an icon will pop up on the browser's search bar, select the button and click always allow . Stop Your Virtual Machine Virtual machines only incur costs while they are running. You should shut down your virtual machine when not in use to prevent unneccessary charges. From the Overview page for your VM, click on the Stop button.","title":"Virtual Machines"},{"location":"VirtualMachines/#find-your-devtest-lab","text":"In your project's custom Dashboard in the Azure Portal, click on the DevTest Lab. Select the DevTest Lab that was assigned.","title":"Find Your DevTest Lab"},{"location":"VirtualMachines/#create-your-virtual-machine","text":"Note: In some instances a Virtual Machine will be pre-created for you and you will not have permission to create a virtual machine. See the FAQ if you need to make changes to your virtual machine. From the DevTest Lab Overview page, click on the + Add button. Choose an appropriate base for your VM (e.g., Data Science Virtual Machine - Windows Server 2019). For more details on the software included with the Data Science Virtual Machines, please click here . Enter a name for your VM and a User name and password that you will use to login to the VM. Be sure to deselect the Use a saved secret and Save as default password checkboxes. You may click the Change Size Link to change your VM size if you wish to do so. Leave the rest as defaults and click on the Create button.","title":"Create Your Virtual Machine"},{"location":"VirtualMachines/#find-your-virtual-machine","text":"From the DevTest Lab Overview page, scroll down until you see your VM under My virtual machines . Click on your VM to access its Overview page.","title":"Find Your Virtual Machine"},{"location":"VirtualMachines/#start-your-virtual-machine","text":"From the Overview page for your VM, click on the Start button. It takes a few minutes for your VM to start up. Monitor its startup progress by selecting the Notifications icon at the top right of the window.","title":"Start Your Virtual Machine"},{"location":"VirtualMachines/#connect-to-your-virtual-machine","text":"From the Overview page for your VM, click on the Browser connect button (if you do not see a Browser connect button you might have to click on the Connect button and then choose Bastion from the dropdown menu). Ensure the Open in new window checkbox is selected, enter the Username and Password that you used when you created your VM, and click on the Connect button. Your VM should open in a new browser tab. Note : By default, the Ubuntu virtual machine opens in Terminal mode. You can access the GUI of your Ubuntu machine from a Windows machine using X2Go . Note : After attempting to login for the first time, an error may appear that a popup blocker is preventing a new window to open. To disable it, an icon will pop up on the browser's search bar, select the button and click always allow .","title":"Connect To Your Virtual Machine"},{"location":"VirtualMachines/#stop-your-virtual-machine","text":"Virtual machines only incur costs while they are running. You should shut down your virtual machine when not in use to prevent unneccessary charges. From the Overview page for your VM, click on the Stop button.","title":"Stop Your Virtual Machine"},{"location":"Archive/VirtualMachineAccess/","text":"Find Your Virtual Machine From your project's DevTest Lab's Overview page scroll down until you see your VM under My virtual machines . Click this to access the Overview page for your VM. Start Your Virtual Machine From the Overview page for your VM, click the Start button. It takes a few minutes for your VM to start up, monitor its startup progress by selecting the Notifications icon at the top right of the window. Connect To Your Virtual Machine From your project's DevTest Lab's Overview page scroll down until you see your VM under My virtual machines . Click this and then from the Overview page for your VM click the Browser connect button (if you do not see a Browser connect button you might have to hit the Connect button and then choose Bastion from the dropdown menu). Ensure the Open in new window checkbox is selected, type in the Username and Password that you used when you created your VM, and click the Connect button. Your VM should open in a new browser tab. Stop Your Virtual Machine From the Overview page for your VM, click the Stop button.","title":"Find Your Virtual Machine"},{"location":"Archive/VirtualMachineAccess/#find-your-virtual-machine","text":"From your project's DevTest Lab's Overview page scroll down until you see your VM under My virtual machines . Click this to access the Overview page for your VM.","title":"Find Your Virtual Machine"},{"location":"Archive/VirtualMachineAccess/#start-your-virtual-machine","text":"From the Overview page for your VM, click the Start button. It takes a few minutes for your VM to start up, monitor its startup progress by selecting the Notifications icon at the top right of the window.","title":"Start Your Virtual Machine"},{"location":"Archive/VirtualMachineAccess/#connect-to-your-virtual-machine","text":"From your project's DevTest Lab's Overview page scroll down until you see your VM under My virtual machines . Click this and then from the Overview page for your VM click the Browser connect button (if you do not see a Browser connect button you might have to hit the Connect button and then choose Bastion from the dropdown menu). Ensure the Open in new window checkbox is selected, type in the Username and Password that you used when you created your VM, and click the Connect button. Your VM should open in a new browser tab.","title":"Connect To Your Virtual Machine"},{"location":"Archive/VirtualMachineAccess/#stop-your-virtual-machine","text":"From the Overview page for your VM, click the Stop button.","title":"Stop Your Virtual Machine"},{"location":"Archive/VirtualMachineCreate/","text":"Create Your Virtual Machine From your project's DevTest Lab's Overview page press the + Add button. Choose an appropriate base for your VM (eg. Data Science Virtual Machine - Windows Server 2019). For more details on software included with the Data Science Virtual Machines please click here . Enter a name for your VM and a User name and password that you wish to use to log into it. Be sure to deselect Save as default password checkbox. Leave the rest as defaults and click the Create button","title":"Create Your Virtual Machine"},{"location":"Archive/VirtualMachineCreate/#create-your-virtual-machine","text":"From your project's DevTest Lab's Overview page press the + Add button. Choose an appropriate base for your VM (eg. Data Science Virtual Machine - Windows Server 2019). For more details on software included with the Data Science Virtual Machines please click here . Enter a name for your VM and a User name and password that you wish to use to log into it. Be sure to deselect Save as default password checkbox. Leave the rest as defaults and click the Create button","title":"Create Your Virtual Machine"},{"location":"Archive/VirtualMachineDevTestLab/","text":"Find Your DevTest Lab Go to your project's DevTest Lab by clicking it from within your project's custom Dashboard.","title":"Find Your DevTest Lab"},{"location":"Archive/VirtualMachineDevTestLab/#find-your-devtest-lab","text":"Go to your project's DevTest Lab by clicking it from within your project's custom Dashboard.","title":"Find Your DevTest Lab"}]}